[
  {
    "objectID": "blog/2021-05-09-stanford-mlsys-seminars.en/index.html",
    "href": "blog/2021-05-09-stanford-mlsys-seminars.en/index.html",
    "title": "Stanford MLSys Seminar Series",
    "section": "",
    "text": "I will progressively summarize talks I find illuminating from the Stanford MLSys Seminar Series here."
  },
  {
    "objectID": "blog/2021-05-09-stanford-mlsys-seminars.en/index.html#chip-floor-planning-problem",
    "href": "blog/2021-05-09-stanford-mlsys-seminars.en/index.html#chip-floor-planning-problem",
    "title": "Stanford MLSys Seminar Series",
    "section": "Chip Floor Planning Problem",
    "text": "Chip Floor Planning Problem\n\nTask of designing the physical layout of a computer chip\nSolution is a form of graph resource optimization\nPlace the chip components such as macros(memory components) and standard cells(logic gates such as NAND and NOR) on a canvas to minimize the latency of computation, power consumption etc. while adhering to constraints such as congestion, cell utilization, heat profile etc.\nNo of states: \\[ 10^{9000} \\]\nPrior approaches include partitioning based methods(e.g. MinCut), stochastic methods (e.g. simulated annealing) and analytic solvers (e.g. RePlAce)"
  },
  {
    "objectID": "blog/2021-05-09-stanford-mlsys-seminars.en/index.html#chip-floor-planning-with-rl",
    "href": "blog/2021-05-09-stanford-mlsys-seminars.en/index.html#chip-floor-planning-with-rl",
    "title": "Stanford MLSys Seminar Series",
    "section": "Chip Floor Planning with RL",
    "text": "Chip Floor Planning with RL\n\nTrain an agent to place the components on the canvas. Reward signal is used to update the parameters of the RL Policy\nAlgorithm used: PPO\n\n\n\nState: Graph embedding of a chip netlist or the graph, embedding of the node being placed next, and the canvas on which the graph is placed.\nAction: Where to place the node on the canvas\nReward: After placing every node, take the Negative Weighted average of total wire length, density and congestion\n\n\nObjective Function\n\\[ J(\\theta,G) = \\frac{1}{K} \\sum\\limits_{g \\sim G} E_{g,p \\sim \\pi_{\\theta}} [R_{p,g}]  \\]\nwhere:\n\\(G\\) : Set of training graphs  \\(K\\) : Size of training set  \\(\\pi*{*\\theta}\\) : RL policy parameterized by \\(\\theta\\) \\(R{p,g}\\): Reward corresponding to placement of node p on graph g\n\\[ R_{p,g} = -Wirelength(p,g) - \\lambda Congestion(p,g) - \\gamma Denisty(p,g) \\]\n\n\nResults\n\nSmoother , rounder palcements\nReduces total wirelength by 2.9% and takes only 24 hrs vs 6-8 weeks for humans\nUsing a pre-trained policy, no of iterations required to place a chip on a canvas was reduced from 10,000’s to 100’s\nSupervised learning was used to learn generalizable representations of the problem\nValue network was doing poorly in predicting the quality of placements when trained on placements generated by a single policy. Problem was decomposed by training models capable of accurately predicting reward from off-policy data.\n\nThis model was trained by extracting samples from various stages of the RL training process. In the earlier stages, placements are poor and later stages, placements are really good. Wirelengths and costs were also estimated.\n\nThis model was then used as an encoder for the RL Agent"
  },
  {
    "objectID": "blog/2021-05-09-stanford-mlsys-seminars.en/index.html#reward-model-architecture",
    "href": "blog/2021-05-09-stanford-mlsys-seminars.en/index.html#reward-model-architecture",
    "title": "Stanford MLSys Seminar Series",
    "section": "Reward Model Architecture",
    "text": "Reward Model Architecture\nEdge property focused Graph CNNs were used.\n\nNode features include x and y co-ordinates, width and heights, type of macros etc.\nTwo fully connected layers used, one the predicts wire length and another that predicts congestion.\nKey metrics like wire_length are edge properties,so edge focused CNN makes more sense\n\n\nEdge Based Convolutions\n\nGet node embeddings by passing node properties through a fully connected layer\n\n\n\nGet edge features by concatenating the embeddings of the nodes of the edge.\n\n\n\nGet edge embeddings by passing the edge feature through a fully connected layer\n\n\n\nPropagate: Get new representation of the node by taking the mean of the edge embeddings the node participates in\n\n\n\nRinse and repeat Steps 2 - 4.7 iterations give good results. Each node is influenced by its 7-hop neighborhood\nTake a mean over all edges in the graph to get a representation of the entire graph."
  },
  {
    "objectID": "blog/2021-05-09-stanford-mlsys-seminars.en/index.html#policyvalue-model-architecture",
    "href": "blog/2021-05-09-stanford-mlsys-seminars.en/index.html#policyvalue-model-architecture",
    "title": "Stanford MLSys Seminar Series",
    "section": "Policy/Value Model Architecture",
    "text": "Policy/Value Model Architecture\n - Embeddings are passed into the policy network that predicts the probability distribution for placement of nodes on the canvas - Valuenet predicts quality of placements so far. - A “Mask” is used to prevent invalid placements"
  },
  {
    "objectID": "blog/2021-05-09-stanford-mlsys-seminars.en/index.html#other-points",
    "href": "blog/2021-05-09-stanford-mlsys-seminars.en/index.html#other-points",
    "title": "Stanford MLSys Seminar Series",
    "section": "Other Points",
    "text": "Other Points\n\nNo incremental rewards were used, only a final reward at the end.\nReduced chip placement times from weeks to 24 hours, allowing faster experimentation. SIll takes 72 hours to fabricate and test the chip and retrieve relevant metrics.\n\nPaper: https://arxiv.org/pdf/2004.10746.pdf"
  },
  {
    "objectID": "blog/2021-05-09-stanford-mlsys-seminars.en/index.html#workflow-and-setup",
    "href": "blog/2021-05-09-stanford-mlsys-seminars.en/index.html#workflow-and-setup",
    "title": "Stanford MLSys Seminar Series",
    "section": "Workflow and SetUp",
    "text": "Workflow and SetUp\nGoal is to train an ML model across multiple remote devices.\n\n\nObjective is to minimize sum of losses across k devices\nAt every training iteration, the central server sends current version of the model to a subset of the devices, the devices perform local training and return model updates to central server.\nChallenges\n\nExpensive communications\n\nmassive, slow networks across thousands of devices e.g. cell phones\n\nPrivacy concerns - user privacy constraints\nstatistical heterogeneity - unbalanced, non -IID data\nsystems heterogeneity - variable hardware, connectivity etc"
  },
  {
    "objectID": "blog/2021-05-09-stanford-mlsys-seminars.en/index.html#heterogeneity-considerations",
    "href": "blog/2021-05-09-stanford-mlsys-seminars.en/index.html#heterogeneity-considerations",
    "title": "Stanford MLSys Seminar Series",
    "section": "Heterogeneity Considerations",
    "text": "Heterogeneity Considerations\n\nImpact of heterogeneity on federated optimization methods\nModel might perform well on subset of devices and poorly on other devices. How to equalize performance across diverse networks ?\nPersonalizing models for specific devices"
  },
  {
    "objectID": "blog/2021-05-09-stanford-mlsys-seminars.en/index.html#federated-averaging",
    "href": "blog/2021-05-09-stanford-mlsys-seminars.en/index.html#federated-averaging",
    "title": "Stanford MLSys Seminar Series",
    "section": "Federated Averaging",
    "text": "Federated Averaging\n\nTrain models on a device, share trained model with central server which averages the models ,sends it back to devices for additional local training.\n\nFederated averaging can diverge in heterogeneous settings.\nAs the amount of local work increases, performance can deteriorate\n\n\n\nX axis: No of communication rounds\nIn the presence of statistical heterogeneity, As the amount of local work increases, performance can deteriorate\nBehavior can be even worse in the presence of systems heterogeneity e.g. some devices are stragglers and cannot complete training in time. Such stragglers may have to be dropped from the training update which exacerbates convergence issues"
  },
  {
    "objectID": "blog/2021-05-09-stanford-mlsys-seminars.en/index.html#fedprox",
    "href": "blog/2021-05-09-stanford-mlsys-seminars.en/index.html#fedprox",
    "title": "Stanford MLSys Seminar Series",
    "section": "FedProx",
    "text": "FedProx\n\nAlgorithm for heterogeneous optimization.\nModifies the local sub problem for device k.\n\n\\[ \\min\\limits_{w_k} F_k(w_k) + \\frac{\\mu}{2} \\Vert w_k - w^t \\Vert^2 \\] The first term is a loss function on the local weights on device k.\nThe second term is the proximal term. It limits the impact of local heterogeneous updates by ensuring that local update is not too different from global weights.\nThis approach also incorporates partial work completed by stragglers.\nThis approach converges despite non-IID data, local updating and partial participation\nKey result: Fed prox with the proximal term leads to a 22% test accuracy improvement on average."
  },
  {
    "objectID": "blog/2021-05-09-stanford-mlsys-seminars.en/index.html#fairness",
    "href": "blog/2021-05-09-stanford-mlsys-seminars.en/index.html#fairness",
    "title": "Stanford MLSys Seminar Series",
    "section": "Fairness",
    "text": "Fairness\n\nMost approaches use some form of empirical risk minimization approach that attempts to reduce the weighted loss across all devices, this does not ensure uniformly good performance across devices.\nSolution inspired by fair resource allocation problems e.g. fairly allocating bandwidth: \\(\\alpha\\) fairness\n\nA modified objective (qFFL) is given below\n\\[qFFL = \\min\\limits_{w} \\frac{1}{q+1} \\big( p_1 F_1^{q+1} + p_2 F_2^{q+1} + ... + p_N F_N^{q+1} \\big)  \\] If \\(q \\rightarrow 0\\), you get the traditional risk minimization objective.\nIf \\(q \\rightarrow \\infty\\), you get minimax fairness.\nIncreasing \\(q\\) reduces variance of accuracy distribution and increases fairness. This approach can cut variance in half while maintaining the overall average accuracy."
  },
  {
    "objectID": "blog/2021-05-09-stanford-mlsys-seminars.en/index.html#personalization",
    "href": "blog/2021-05-09-stanford-mlsys-seminars.en/index.html#personalization",
    "title": "Stanford MLSys Seminar Series",
    "section": "Personalization",
    "text": "Personalization\n\nIn the absence of data for a specific, might want to learn from peers.\nLearning one model across the network means we have non-personalized models.\nCan you deliver personalized models that learn from peers ?\nUse the multi task learning framework.\n\nLearn separate model for each device.\nLearn relationship that exists between the devices\n\n\n\n\n\\(w_t\\) represents models on device \\(t\\)\n\\(W\\) represents a relationship matrix between devices and \\(\\Omega\\) represents a task relationship matrix\n\\(W\\) and \\(\\Omega\\) can be learn various possible relationships between devices and tasks ranging from a scenario where all tasks are related across devices to a scenario where 1 device asymmetrically impacts all other devices\nNot scalable to very deep neural nets\n\nDevice metadata can be included in the regularization factor\n\n\nBenchmark dataset for federated learning: LEAF"
  },
  {
    "objectID": "blog/2021-05-09-stanford-mlsys-seminars.en/index.html#current-state",
    "href": "blog/2021-05-09-stanford-mlsys-seminars.en/index.html#current-state",
    "title": "Stanford MLSys Seminar Series",
    "section": "Current State",
    "text": "Current State\n\nPipelines can be used to process data inside a model as in sklearn or TF\nTypically there are ETL or ELT pipelines that populate a table\nModels have to see data twice - during training and inference\n\nThese needs to be consistent\n\nTypically pipelines are developed by data scientists for model training and then re-developed and maintained by data engineers for model serving"
  },
  {
    "objectID": "blog/2021-05-09-stanford-mlsys-seminars.en/index.html#ml-data-challenges",
    "href": "blog/2021-05-09-stanford-mlsys-seminars.en/index.html#ml-data-challenges",
    "title": "Stanford MLSys Seminar Series",
    "section": "ML Data Challenges",
    "text": "ML Data Challenges\n\nBuilding feature pipelines\n\nFeature engineering is time consuming\nRequires different technologies for different production requirements (distributed compute , stream processing , low latency transformation)\nReliable computation and backfilling of features requires a large investment\n\nConsistent data access\n\nRedevelopment of pipelines leads to inconsistencies in data\nTraining-serving skew degrades model performance\nModels needs point-in-time correct view of data to avoid label leakage (especially for time series)\n\nDuplication of effort - Siloed development - No means of collaboration or sharing feature pipelines - Lack of governance and standardization\n\nEnsuring data quality\n\nIs the model receiving right data and still operating correctly ?\nAre features fresh ?\nHas there been drift in data over time ?"
  },
  {
    "objectID": "blog/2021-05-09-stanford-mlsys-seminars.en/index.html#solution-with-feature-stores",
    "href": "blog/2021-05-09-stanford-mlsys-seminars.en/index.html#solution-with-feature-stores",
    "title": "Stanford MLSys Seminar Series",
    "section": "Solution with Feature Stores",
    "text": "Solution with Feature Stores\n\nEasy pipeline creation\n\nWrite feature definitions in SQL\nRegister in feature store specifying online or offline computation\nFeature is computed and populated at required schedule\n\nConsistent data acess\n\nCommon serving API to access data for both training and serving\n\n\nCataloging and Discovery\n\nCan browse through library of features , how many teams use it , documentation etc.\n\nData quality monitoring\n\nCan produce statistics of data over time\nIntegrates with packages like great expectations\nSupports Feature as code, including version control and CI/CD integration."
  },
  {
    "objectID": "blog/2021-05-09-stanford-mlsys-seminars.en/index.html#deployment-patterns",
    "href": "blog/2021-05-09-stanford-mlsys-seminars.en/index.html#deployment-patterns",
    "title": "Stanford MLSys Seminar Series",
    "section": "Deployment Patterns",
    "text": "Deployment Patterns\n\nOffline feature serving\n\n\nSuitable for use cases like Pricing, Risk, Churn prediction where jobs are run periodically or in an ad-hoc fashion\n\nOnline feature serving\n\n\nSuitable for low latency use cases like recommendations and personalization\n\nOnline feature computation\n\n\nReal time / on demand feature transformations are supported\nModel is triggered when a transaction event occurs\nThe metadata within the transaction is often required to derive features synchronously rather than fetch something that has been pre-computed.\nModels serving layer will send trxn metadata to the feature store . Feature store will use pre-computed streaming data , batch data , trxn metadata; call vendor apis; combine these, produce new features and return to mode serving layer.\nCall vendor API from feature store, so all this data can be logged."
  },
  {
    "objectID": "blog/2021-05-09-stanford-mlsys-seminars.en/index.html#feature-stores-in-a-modern-data-stack",
    "href": "blog/2021-05-09-stanford-mlsys-seminars.en/index.html#feature-stores-in-a-modern-data-stack",
    "title": "Stanford MLSys Seminar Series",
    "section": "Feature Stores in A Modern Data Stack",
    "text": "Feature Stores in A Modern Data Stack\n\nGreater abstraction meaning we don’t need a separate modules for an online store, offline store and data processing\n\n\ndbt allows you to write ELT type queries\n\nML engineers create basic model and Data scientists can optimize the model further. This is opposed to the traditional approach of DS developing models locally and ML engineers rewriting and deploying them."
  },
  {
    "objectID": "blog/2021-05-09-stanford-mlsys-seminars.en/index.html#other-points-1",
    "href": "blog/2021-05-09-stanford-mlsys-seminars.en/index.html#other-points-1",
    "title": "Stanford MLSys Seminar Series",
    "section": "Other Points",
    "text": "Other Points\n\nFeature stores work better with structured rather than unstructured data\nThe performance of the code you use to write transformations will affect whether you can use a feature in both batch and real time use cases. Writing a transformation in pandas means it could be slower than writing it as a JVM process."
  },
  {
    "objectID": "blog/2021-05-09-stanford-mlsys-seminars.en/index.html#active-learning",
    "href": "blog/2021-05-09-stanford-mlsys-seminars.en/index.html#active-learning",
    "title": "Stanford MLSys Seminar Series",
    "section": "Active Learning",
    "text": "Active Learning\n\nTrain model on available subset of data.\nApply this trained model to all available unlabeled data.\nSelect highest value data points(highest uncertainty ?) , label them add to training data set.\nRepeat until budget is exhausted.\nSmaller deep learning models that can trained much faster are good approximations of larger models.\nEarly epochs make biggest difference in model performance\n\n\n\nTrain a smaller proxy model to help with data selection and use final model only to train the full dataset for the final downstream task"
  },
  {
    "objectID": "blog/2021-05-09-stanford-mlsys-seminars.en/index.html#core-set-selection",
    "href": "blog/2021-05-09-stanford-mlsys-seminars.en/index.html#core-set-selection",
    "title": "Stanford MLSys Seminar Series",
    "section": "Core Set Selection",
    "text": "Core Set Selection\n\nWhen large amounts of labelled data are available, select a small subset of data that accurately approximates the full dataset.\nTrain a proxy model that can be used to identify most useful subset and train full model on this data."
  },
  {
    "objectID": "blog/2021-05-09-stanford-mlsys-seminars.en/index.html#active-search",
    "href": "blog/2021-05-09-stanford-mlsys-seminars.en/index.html#active-search",
    "title": "Stanford MLSys Seminar Series",
    "section": "Active Search",
    "text": "Active Search\n\nhttps://arxiv.org/abs/2007.00077 #Paper\nVariation on Active Learning: Goal is to select as many positive examples as possible from billions of unlabeled examples.\nPre trained deep learning models tightly cluster unseen concepts forming well connected components\nLook at local neighborhood of positive examples rather than the entire unlabelled dataset.\n\n\n\nSimilarity Search for Efficient Active Learning and Search (SEALS)\n\nOnly take examples that lie in the neighborhood of known positive examples\n\n\n\nIn each iteration, take the newly labeled examples and find their neighbors.\n\n\n\nSelection Criteria for Samples\n\nMost Likely Positive\nMax Entropy : Select points with highest entropy based on predicted class probabilities\nInformation density: Select points in regions of high density and high uncertainty"
  },
  {
    "objectID": "blog/2020-08-01-crossing-the-chasm.en/index.html",
    "href": "blog/2020-08-01-crossing-the-chasm.en/index.html",
    "title": "Crossing the Chasm",
    "section": "",
    "text": "As someone who is learning the ropes of B2B Product Management and Product Marketing, this book has been deeply educational for me. Below are my notes from the book. Needless to say, If a summary of the book is a 30 minute read, it is a book worth studying not just reading."
  },
  {
    "objectID": "blog/2020-08-01-crossing-the-chasm.en/index.html#chapter-1",
    "href": "blog/2020-08-01-crossing-the-chasm.en/index.html#chapter-1",
    "title": "Crossing the Chasm",
    "section": "Chapter 1",
    "text": "Chapter 1\nDiscontinuous or Disruptive Innovation: Products that require us to change our current mode of behavior or to modify other products and services we rely on\nContinuous or Sustaining Innovation: Normal upgrading of products that do not require us to change our current behavior\nAll new products lie on this continuum between discontinuous and continuous.\n\nInnovators: Pursue new technology products aggressively out of curiosity and interest\nEarly Adopters: Adopts new technology in order to obtain a competitive advantage. Do not need references and hence are key to your business becoming viable\nEarly majority: Pragmatic. Looking to adopt only proven technology with good references. Constitutes 1/3 of market and fundamental to profits and growth\nLate majority: More averse to using technology then the early majority. Comfortable buying only once the technology becomes a standard and there is plenty of support services. Can be a profitable segment as selling costs decreases and cost of R&D has been amortized\nLaggards: Don’t want to buy new technology. Interested in buying only is they don’t have to know anything about it. E.g. breaking system in a car\n\nIn reality, there exists a gap between any of these psychographic groups. The gap represents the disconnect between groups i.e. the group on the right may not be receptive to a product presented in the same as the group to the left.\nThe first gap between Innovators and Early adopters exist when a new technology cannot be translated into a major new benefit.\nA similar gap exists between early and late majority. This is due to the demands on the user to be technologically competent. The product needs to be exceedingly easy to adopt and use to cross this gap.\nThe CHASM is the gap between the early adopters and early majority. These two segments are very similar in the size of the order or sale but they buy for very different reasons. The early adopters want to adopt the technology to leap ahead of the competition and recognize that the technology is at an early stage and are willing to put up with bugs and glitches. They are looking for exponential improvement.\nOn the other hands, the early majority is looking at the technology to boost productivity of existing operations. They are looking for linear improvement ;evolution not revolution. Hence, they are unwilling to put up with bugs and glitches. They want want the new technology to integrate easily with the existing base. They need references from other members in the early majority cohort to adopt the new technology,not references from other early adopters.\nThe chasm exists because a company transitioning to the early majority is often trying to win a market for which no references exist and for whom the necessary support system has not been built to ensure a smooth transition to the new technology."
  },
  {
    "objectID": "blog/2020-08-01-crossing-the-chasm.en/index.html#chapter-2",
    "href": "blog/2020-08-01-crossing-the-chasm.en/index.html#chapter-2",
    "title": "Crossing the Chasm",
    "section": "Chapter 2",
    "text": "Chapter 2\nMarketing strategy must change in response to the three important stages: early market, chasm and the mainstream market.\n\nMarketing means taking actions to create, grow, maintain or defend markets.\n\nFor high tech, a market is:\n\na set of actual or potential customers\nfor a given set of products or services\nwho have a common set of needs or wants and\nwho references each other when making a buying decision\n\nIf two buyers have no way to reference each other, they are not in the same market.\nIt is to be noted however that the internet has the potential to consolidate fragmented markets. A buyer can easily go to an online forum and reference the experience of a buyer in a different country or a different vertical when in the pre-internet era, they would have no way to influence each other.\nThe tablet is a category, however the tablet as a technology has multiple markets or market segments. a market for professionals, a market for college students and a market for young children.\n\nInnovators: The Technology Enthusiasts\n\nExpects the truth about the capabilities of a product i.e. no vaporware\nRequires high quality technical support\nWant early access to the latest technology\nExpects cheap prices\n\nInnovators often are housed in the advanced technology divisions in a company, they are empowered to buy one of any product.\nYou can work with innovators under non-disclosure agreements to get early feedback on a product. They can champion your product within the organization or in the larger market.\nReach this segment through online forums, targeted e-mails, demos and webinars.\n\n\nEarly Adopters: The Visionaries\nEarly adopters want to leverage the technology being offered to significantly improve business outcomes. They can match the technology to a strategic opportunity, and motivate their organization to take on a high impact, high risk project. They tend to be ambitious, early career rather than seasoned executives looking for growth and personal recognition. Example: Reed Hastings moving Netflix’s tech stack to AWS.\nThey are aware the initiative is high risk high return, have larger budgets and are willing to invest up front to kick start a project in the hopes of out sized returns down the road.\nYou have to work with visionaries in projects, typically starting with a pilot and building more capabilities in a project set up with milestones and goals . Visionaries want to be closely attuned to the product development process so that they know their goals will be met. Vendors need to be able to delineate the project into productizable chunks for the larger market.\nGiven visionaries are pursuing a small window of opportunity to make a big market impact, they can exert a lot of pressure with hard deadlines. Therefore good account management and executive restraint are necessary.\nPlan each phase of the project such that\n\nThey are realistic and attainable\nProvides the vendor with a marketable product\n\n3)Provides the customer with a concrete return that can be celebrated as a major step forward\n\nThe most important principle is the emphasis on management of expectations. Because controlling expectations is so crucial, the only practical way to do business with visionaries is through a small, top-level direct sales force\n\nSales force should:\n\nunderstand visionary’s goals and inspire confidence\nBe flexible and adapt to visionary’s agenda\nNegotiate so that you don’t commit to do the impossible\n\nYou can reach this segment only through technology enthusiasts or by leveraging existing relationships.\n\n\nEarly Majority: The Pragmatists\nPragmatists care about the company they are buying from (they typically buy only from established companies), quality of the product , infrastructure of supporting products and system interfaces and reliability of service they are getting. They are invested for the long term and control the bulk of the dollars in the market place.\nThey try to limit touch points with any distribution channel. Relationships with a specific sales person can be valuable. For a start up, the best way to reach this market is by entering into partnerships with accepted vendors. Value Added Resellers (VAR) who provide turnkey solutions can thrive in this market.\nPragmatists typically buy from market leaders because there will be third parties offering supporting products and services for a market leading product.\nPragmatists are reasonably price sensitive, they are willing to pay a premium for top quality or special services but otherwise want the best deal.\nYou reach pragmatists by being knowledgeable about issues in their industry, by attending industry conferences and trade shows, by publishing blogs and articles they read. You need referenceable customers and need to have partnerships and alliances with other vendors.\n\nYou need to have a reputation for quality and service and make yourself into the obvious supplier of choice\n\n\n\nLate Majority: The Conservatives\nConservatives want to buy pre-assembled packages, with everything bundled at highly discounted prices.\n\nThe packaged solution should offer a ‘whole product’ offering every element of the solution required to address the end user’s market needs without need to offer after sales support as margins are low\n\n2)Needs a Low overhead distribution channel to reach the market. e.g. “As a service” model on the web\n\nFocus on convenience rather than performance, user experience rather than feature sets\n\n\n\nDynamics of Mainstream Markets\nTo maintain market leadership in the mainstream market\n\nKeep pace with competition - being a technology leader or having the best product is not essential. Have to respond to a major breakthrough by a competitor\nConservatives are service oriented while Tech enthusiasts are product oriented. The importance of product and service vary on a spectrum between these two.\n\n\nCompanies stumble during transition from visionaries to pragmatists as they fail to change the sales pitch. A company may be trumpeting it’s early success in pilots when the pragmatists wants to hear about up and running production installations. The company says ‘state of the art’ when the customer wants to hear ‘industry standard’"
  },
  {
    "objectID": "blog/2020-08-01-crossing-the-chasm.en/index.html#chapter-3",
    "href": "blog/2020-08-01-crossing-the-chasm.en/index.html#chapter-3",
    "title": "Crossing the Chasm",
    "section": "Chapter 3",
    "text": "Chapter 3\nTo enter a mainstream market being controlled by a competitor, you need to find a niche or a beachhead as in the Invasion of Normandy and invest all your efforts into winning that niche. Once you have won this niche, you can expand into adjacent markets.\nBy providing an overabundance of support to win this niche, you can develop a solid base of references, collateral and internal procedures and documentation.\n\nIgnore the size of the larger market opportunity and be lazer focused on winning the niche you have targeted\n\nGiven this early majority is looking for a guaranteed boost to business outcomes, it is necessary that your deliver the whole product\n\nThe whole product is the complete set of products and services needed to deliver the benefits promised to the buyer.\n\nA whole product commitment requires you to allocate your most valuable resources to meet a customer’s requirements. It should be strategic so that you can leverage this investment into winning more customers. This is possible only if you are focused an a specific niche and a limited set of use cases.\nWord of mouth marketing is essential to winning this segment of the market. By focusing on a niche and winning over 4 or 5 customers in the niche, you can create self-reinforcing word of mouth effects. Winning one or two customers in multiple markets through a sales driven approach, means your word of mouth message will peter out.\nPragmatists also prefer to buy only from market leaders. To be a market leader, you will need anywhere between 35% to 50% of the market. So the overall market or niche you are targeting should be no more than twice your expected sales.\nGiven the dynamics of the mainstream market, even providers of horizontally oriented offerings such as infrastructure services should focus on a vertical niche, when crossing the chasm.\nWhile picking a target niche, ensure that\n\nThe economic value of the problem you are solving is high. Higher this value, faster the target niche will pull you out of the chasm\nLine up other market segments you can move into by leveraging your initial win\nthe target segment is big enough to matter\nit is small enough to win\ngood fit for your unique strengths"
  },
  {
    "objectID": "blog/2020-08-01-crossing-the-chasm.en/index.html#chapter-4",
    "href": "blog/2020-08-01-crossing-the-chasm.en/index.html#chapter-4",
    "title": "Crossing the Chasm",
    "section": "Chapter 4",
    "text": "Chapter 4\nAcknowledge that you have to pick your target market niche without very much data to go on. Informed intuition rather than analytic reasoning is your best decision making tool\nIn a low data environment, getting more data improves confidence but not the quality of your decision.\nInstead of trying to identify a target market segment off the bat, identify a unique user scenarios with a clear use case or application of your product. These user scenarios can then be consolidated and prioritized into a list of target market segment opportunities.\nSuggested Template for a user scenario:\n\nHEADER INFORMATION:\nThumbnail information about end user, technical buyer and economic buyer. For business markets capture industry, geography, department and job title. For consumer markets, capture age, sex,economic status and social group.\nIf your selling SAS Enterprise Guide, the end user will be a data scientist, the technical buyer will be the IT team who installs and maintains the application, the economic buyer will be the CIO who sponsors the deal.\nA DAY IN THE LIFE BEFORE:\nCapture the situation in which the user is stuck with significant consequences for the economic buyer\n\nScene or situation: Focus on the moment of frustration. What is going on? What is the user about to attempt?\nDesired Outcome: What is the user trying to accomplish? Why is this important?\nAttempted Approach: Without the new product, how does the user go about the task?\nInterfering factors: What foes wrong? How and why does it go wrong?\nEconomic consequences: What is the impact of the user failing to accomplish the task productively?\n\n\n3 . A DAY IN THE LIFE AFTER:\nReplay the scenario with the new product in place.\n\nNew approach: With the new product how does the end user go about the task?\nEnabling factors: What is it about the new approach that allows the user to get unstuck and be productive?\nEconomic rewards: What are the costs avoided and benefits gained?\n\nOnce the user scenarios are developed, evaluate them against the following factors which are critical to building a go-to-market plan.\nRate each user scenario first against the four ‘showstopper’ issues:\n1. Target Customer : Is there a single identifiable economic buyer fir the product, readily accessible to the sales channel we use.\n2.Compelling Reason to Buy: Are the economic consequences sufficient to make any reasonable economic buyer anxious to fix the problem called out in the scenario?\n3. Whole Product: Can the company find the partners and allies to build a complete solution that addresses the target customer’s compelling reason to buy? You want to be able to enter the market in three months and dominate it by the end of 12 months.\n4. Competition: Has the problem already been addressed by another company such that they have crossed the chasm and occupied the space we would be targeting?\nScore each scenario against these factors assigning a score from 1 -5. Rank order them and select the best scenarios for the next round. When in doubt favor scenarios where the compelling reason to buy is highest.\nNow score the shortlisted set of scenarios against the following issues and rank them:\n5. Partners and Allies: Do we already have relationships begun with other companies needed to fulfill the whole product?\n6. Distribution: Do we have a sales channel that can call on the target customer and fulfill the whole product requirements put on distribution?\n7. Pricing: Is the price of the whole product consistent with the target customer’s budget and with the value gained by fixing the broken process?\n8. Positioning: Is the company credible as a provider of products and services to the target niche?\n9. Next target customer: If we are successful in dominating this niche, does it facilitate entry into adjacent niches?"
  },
  {
    "objectID": "blog/2020-08-01-crossing-the-chasm.en/index.html#chapter-5",
    "href": "blog/2020-08-01-crossing-the-chasm.en/index.html#chapter-5",
    "title": "Crossing the Chasm",
    "section": "Chapter 5",
    "text": "Chapter 5\nThere exits a gap between the marketing promise made to the customer and the ability of the shipped product to fulfill the promise. This gap should be be overcome by augmenting the product by a variety of services and ancillary products to make it a whole product.\n\nGeneric Product: What is shipped in the box and what is covered by the purchasing contract\nExpected Product: The minimum configuration of products and services necessary to have any chance of achieving the buying objective. e.g. a WiFi or cellular connection for an iPad\nAugmented Product: The product fleshed out to provide the maximum chance of achieving the buying objective E.g An iPad with email, browser , an app store etc.\nPotential Product: This represents the product’s room for growth as more and more ancillary products come on the market and as customer specific enhancements are made ****e.g. The apps in the App Store\nAt the beginning of a disruptive innovation, the marketing battle happens at the level of the generic product but as the market matures becoming mainstream, the offerings on the inside become similar between vendors and the battle shifts to the outer layers.\nInnovators take it upon themselves to make the product whole as they have the technical chops while the visionaries rely on system integrators to make the product whole. The segments to the rich of the chasm expect to get the whole product and not do any extra work to make it whole.\n\nPragmatists evaluate and buy whole products\n\nOnce there are more than one or two comparable products in the marketplace, then investing in R&D at the generic level has a decreasing return, while there is an increasing return from investing at the other levels.\nWhole product planning is the centerpiece for developing a market domination strategy as pragmatists will commit to the candidate offering the most coherent path to a whole product . By squeezing out other alternatives, pragmatists bring about the standardization necessary to ensure good whole product development in the market place.\nTo cross the chasm, you need to identify the minimum commitment to whole product necessary to cross the chasm\n\nOnce these minimum commitments are identified, the ‘whole’ product manager should identify those parts the vendor will fulfill and find the right partners and allies to deliver on the rest. The goal of this partnership should be to accelerate the formation of a whole product infrastructure within a specific target segment in support of a segment -specific compelling reason to buy. The partnership should commit to co-delivering a whole product and market it cooperatively.These partnerships are tactical alliances growing out of whole product needs.\nThings to Keep in mind while building these partnerships:\n\nReview the whole product from each participant’s point of view. Make sure each vendor wins, and that no vendor gets an unfair share of the pie. Inequities here, particularly when they favor you, will instantly defeat the whole product effort—companies are naturally suspicious of each other anyway, and given any encouragement, will interpret your entire scheme as a rip-off.\nDevelop the whole product relationships slowly, working from existing instances of cooperation toward a more formalized program. Do not try to institutionalize cooperation in advance of credible examples that everyone can benefit from it—not the least of whom should be the customers. Also, do not recruit directly competing partners to serve the same need in the same segment—this will only discourage them from making a full commitment to your program.\nWith large partners, try to work from the bottom up; with small ones, from the top down. The goal in either case is to work as close as possible to where decisions that affect the customer actually get made.\nOnce formalized relationships are in place, use them as openings for communication only. Do not count on them to drive cooperation. Partnerships ultimately work only when specific individuals from the different companies involved choose to trust each other.\nIf you are working with very large partners, focus your energy on establishing relationships at the district sales office level and watch out for wasting time and effort with large corporate staffs. Conversely, if you are working with small partners, be sensitive to their limited resources and do everything you can to leverage your company to work to their advantage.\nFinally, do not be surprised to discover that the most difficult partner to manage is your own company. If the partnership really is equitable, you can count on someone inside your company insisting on taking a bigger share of the benefit pie. In fighting back, look to your customers to be your truest and most powerful allies."
  },
  {
    "objectID": "blog/2020-08-01-crossing-the-chasm.en/index.html#chapter-6",
    "href": "blog/2020-08-01-crossing-the-chasm.en/index.html#chapter-6",
    "title": "Crossing the Chasm",
    "section": "Chapter 6",
    "text": "Chapter 6\nPragmatist buyers do not take a product category seriously until there is established competition and an established leader. This is a signal that the market is sufficiently mature to support a reasonable whole product infrastructure. In other words, pragmatists loathe to buy until they can compare.\nOften in the early markets, a product has no clear competitors or alternatives. To cross the chasm one has to create the competition.\n\nCreating the competition is the single most important marketing communication decision made in the battle to enter the mainstream\n\nYou have to position your product within a buying category that already has some established credibility with the pragmatist buyer. The buying category should have other reasonable buying choices the pragmatists are familiar with. Within this category, you have to position your product as the indisputably correct buying choice.\nTo create the competition, you need to identify two competitors that can be references\n1) The Market Alternative: This is the vendor the target customer has been buying for years. We will seek to address the problem they address and seek to steal away the budget currently allocated to them. We of course need to solve a problem that the current solution does not address.\n2) The Product Alternative: The is a company harnessing the same or similar technology that we are and is positioning itself as a technology leader. This gives credibility and provides parallels to our product. which has a different segment-specific focus.\nE.g. Box positioned Microsoft Sharepoint as the market alternative and Dropbox as the product alternative.\n\nCompetitive Positioning Compass\n\nThere are four domains of value in high-tech marketing: technology, product, market, and company. As products move through the Technology Adoption Life Cycle, the domain of greatest value to the customer changes. In the early market, where decisions are dominated by technology enthusiasts and visionaries, the key value domains are technology and product. In the mainstream, where decisions are dominated by pragmatists and conservatives, the key domains are market and company. Crossing the chasm, in this context, represents a transition from product-based to market-based values.\nLet us now explore how a product evolves through it’s technology adoption life cycle and how marketing efforts also need to evolve and change accordingly.\nEarly Market\n\nThe Technology Enthusiasts are specialists who have deep expertise in and attach the highest value to the core technology. They are the gatekeepers to the early market and tend to be skeptical about any product but have a passion for new technology. The marketing message at this stage has to be focused on the technology component of your product - how original and advantageous it is. Once they understand and appreciate the technology, they will endorse the product. This segment can open the door to the Visionaries\nThe Visionaries care about how the product can allow them to leap ahead of the competition.The marketing message here has to be focused on the value proposition of the product. Once the visionaries are convinced of this value proposition, they tend to champion your products and company.\n\nMainstream Market\n\nThe Pragmatists are the gatekeepers to the mainstream market. Winning this market means that your messaging which was focused on product and on like minded specialists(Visionaries) has to change significantly - hence the chasm - to resonate with market oriented,wary generalists . The messaging here has to emphasize how your product helps the firm to tap into an unmet and significant market opportunity. This segment cares less about your specific product but can appreciate the market opportunity. Once they understand the market opportunity you create for them, they will champion your company.\nThe Conservatives care about buying stable products from market leaders. The marketing message here has to focus on how your product is the industry standard and how it offers a ‘whole product’ solution.\n\nAt the initial stages of both the early and mainstream market, communications on product or company strengths is a mistake.\nTo cross the chasm, your messaging has to shift from a product-centric one (e.g.: cool product, easy to use, elegant architecture, product price, unique functionality) to a market-centric one(e.g. most complete whole product, solid user experience, compatibility with standards, whole product price, situational value, fit for purpose).\n\n\nPositioning\nKey Principles:\n1.Positioning, first and foremost, is a noun, not a verb. That is, it is best understood as an attribute associated with a company or a product, and not as the marketing contortions that people go through to set up that association.\n2.Positioning is the single largest influence on the buying decision. It serves as a kind of buyers’ shorthand, shaping not only their final choice but even the way they evaluate alternatives leading up to that choice. In other words, evaluations are often simply rationalizations of preestablished positioning.\n3.Positioning exists in people’s heads, not in your words. If you want to talk intelligently about positioning, you must frame a position in words that are likely to actually exist in other people’s heads, and not in words that come straight out of hot advertising copy.\n4.People are highly conservative about entertaining changes in positioning. This is just another way of saying that people do not like you messing with the stuff that is inside their heads. In general, the most effective positioning strategies are the ones that demand the least amount of change.\n\nThe goal of positioning should be to make products easier to buy not easier to sell.\n\nPositioning should should create a space inside the customer’s head called ‘best buy for this type of situation’, and attain sole, undisputed occupancy of this space. Creating this space consists of the following stages\n\nName it and Frame it : Specify what problem your product solves and what category it falls under and provide a technically accurate description of the disruptive innovation. This is for the innovators.\nWho for and what for: Describe the value proposition of your product and who will benefit from it. This is for the visionaries\nCompetition and Differentiation : Place the product in the appropriate comparative context using a market alternative and a product alternative. This is for the pragmatist\nFinancials and future: Provide confidence to the buyer that your company has staying power and will continue to invest and support the product. This is for the conservatives.\n\nTo create a compelling positioning message :\n\nIt needs to make a claim of undisputed market leadership within a given target segment\nIt should provide evidence to make any disputation unreasonable\nIt should identify and address the right audience in the right sequence with the right version of the message\nIt should be updated and revised when competitors attack it\n\nThe evidence to support a positioning claim should be adjusted based on the target segment as shown below. The claims that resonate with each segment is different.\n\nPresent your position in the following template to ensure it passes the elevator test.\nFor (target customers—beachhead segment only)\n-Who are dissatisfied with (the current market alternative)\n-Our product is a (product category)\n-That provides (compelling reason to buy).\n-Unlike (the product alternative),\n-We have assembled (key whole product features for your specific application).\nE.g.\nHANA\n• For online retailers and others\n• Who want to better assist their customer agents to up sell and cross-sell consumers during their purchasing transactions,\n• HANA is a database for online transaction processing\n• That supports applying analytics in real time to determine the very best offer to make.\n• Unlike database solutions from Oracle, the market leader,\n• HANA does not require melding and maintaining two separate environments for transaction processing and analytics.\nWhole Product Launches\nThe message while launching a whole product for the mainstream market should be ‘Look at this hot new market’. This message typically consists of a description of the emerging new market, anchored by a new approach to a problem stubbornly resistant to conventional solutions, fed by an emerging set of partners and allies, each supplying a part of the whole product puzzle, to the satisfaction of an increasingly visible and growing set of customers. The lure embedded in this story is that we are seeing a new trend in the making, and everyone who has a seat on this bandwagon is going to be in on the Big Win\nTwo vehicles can be use for communicating this ‘whole product’ message:\n\nBusiness press : Emphasize the market opportunity being created by the whole product and technology. Bring along as many of the other players in the market as possible including customers, analysts,partners,distributors etc.\nVertical media i.e. Media specifically dedicated to an industry or profession."
  },
  {
    "objectID": "blog/2020-08-01-crossing-the-chasm.en/index.html#chapter-7",
    "href": "blog/2020-08-01-crossing-the-chasm.en/index.html#chapter-7",
    "title": "Crossing the Chasm",
    "section": "Chapter 7",
    "text": "Chapter 7\nEnsure that we pick the right distribution channel for the type of customer being targeted. When crossing the chasm to the main stream market the number one concern of pricing should be to motivate the distribution channel\nThe type of customer being targeted is typically one of the following\n1. Enterprise executives making big-ticket purchasing decisions focused on complex systems to be adopted broadly across their companies,\n2. End users making relatively low-cost purchasing decisions focused on personal or workgroup technologies to be adopted locally and individually,\n3. Department heads making medium-cost purchasing decisions for use-case-specific solutions that will be adopted within their own organization,\n4. Engineers making design decisions for products and services to be sold to their company’s customers, and\n5. Small business owner-operators making modest purchase decisions that are nonetheless highly material to them, given limited capital to spend and a strong need to get value back.\n\nDirect Sales and Enterprise Buyer\n\nDeals are worth hundreds of thousands or millions\nSales is consultative to identify customer key needs so as to custom-fit the solution\nCalls for relationship marketing. Hosting thought leadership events where vendor can interact with senior executives,make personal contacts that lead to referrals down into the organization.\nSales approach is solution selling, focused on the whole product and tailoring it to the specific customer\nIn the early market, provocation-based selling may be required to convince a customer to re-allocate existing budget to realize a hitherto unidentified opportunity\nDelivery is focused on delivering the whole product and often calls for professional services teams and third party systems integrators\n\nE.g Salesforce, Workday, Oracle\n\n\nWeb Based Self-Service and End-User Buyer\n\nDeals with end users worth hundreds of dollars or tens of dollars a month often preceded by a free trial\nSales is mostly through self service over a website\nCalls for promotional marketing through targeted e-mail campaigns or digital advertising\nOnce a customer clicks on an ad or a promotional e-mail, sales is direct response, allowing the user to evaluate the product for free through a freemium model before committing\nSupport is through a FAQ page or chat\n\n\n\nSales 2.0 and the Department Manager Buyer\n\nThese customers typically need products that meet enterprise standards but have much smaller budgets and have been reliant on local value added re-sellers\nSales 2.0 is the sales approach best suited for this segment with direct touch marketing,sales and service all delivered over the web.\nUnlike web based self service, an expression of interest triggers a sales person to directly interact with the customer through a call or e-mail\nWhite papers, webinars, live demos are also effective tools\nDelivery through SaaS models\nSupport can be direct or through a community\n\ne.g. Rackspace, Box\n\n\nTraditional Two-Tier Distribution and the Design Engineer\n\nEngineers have a big say in purchase decisions without controlling the budget, they dislike traditional sales initiatives\nMarketing can be done primarily through the web to provide a factual perspective that engineers demand\nTwo tier distribution channel with the second tier being customer facing representatives and the first being a vendor facing organization - the distributor\nOnce the engineer approves a vendor,the vendor negotiates with the customer’s purchase department to set price, conditions for future purchase etc.\nSupport is provided by sophisticated engineers from the vendor\n\nE.g. Broadcom, NVIDIA\n\n\nValue Added Reseller and the Small Business Owner\n\nSMB’s are like end consumers who needs do not fit neatly into consumer buckets\nThey often need to find custom solutions for themselves and are best supported by local value added re-sellers(VAR) who are tech-savvy SMBs themselves\nThese VAR do not have marketing or sales expertise that are handled solely by the vendor, with post sales support being handled by the VAR\nMarketing is mostly through the web, and lead flow is often actively shared with VARs\n\ne.g. Intuit, Bill.com"
  },
  {
    "objectID": "blog/2020-08-01-crossing-the-chasm.en/index.html#distribution-oriented-pricing",
    "href": "blog/2020-08-01-crossing-the-chasm.en/index.html#distribution-oriented-pricing",
    "title": "Crossing the Chasm",
    "section": "Distribution Oriented Pricing",
    "text": "Distribution Oriented Pricing\n\nCustomer -Oriented Pricing\n\nSet a higher price point for visionaries in lieu of the higher ROI and special service they are expecting and greater price insensitivity. → Value Based Pricing\nSet lower price points for conservatives as they buy late to minimize costs → Cost based pricing\nSet price points higher (say 30%) than competition for a pragmatists IF you are a market leader given pragmatists have a preference for market leaders. Discount accordingly if you are not a market leader. → Competition based pricing\nAlso consider price of product and market alternative that you used to crate the competitive set and mark up accordingly based on the superiority of your technology and the value of your segment specific solution.\n\n\n\nVendor-Oriented Pricing\n\nSet prices as a function of cost of goods, cost of sales, cost of overhead,cost of capital, promised rate of risk adjusted return etc.\nLeast sound basis for pricing decisions when crossing the chasm\n\n\n\nDistribution Oriented Pricing\nPricing should address two questions\n\nIs it priced to sell?\nIs it worthwhile to see?\n\nPrices set for visionaries may be too high for pragmatists given the competitive set. If priced too low, there may not be enough margin to reward the channel for introducing a disruptive innovation into their already established relationship with an established customer.\n\nSet pricing at the market leader price point, reinforcing your claims to market leadership and build a disproportionately high reward for the channel into the price margin, a reward that will be phased out as the product becomes established in the mainstream."
  },
  {
    "objectID": "blog/2020-08-01-crossing-the-chasm.en/index.html#conclusions-leaving-the-chasm-behind",
    "href": "blog/2020-08-01-crossing-the-chasm.en/index.html#conclusions-leaving-the-chasm-behind",
    "title": "Crossing the Chasm",
    "section": "Conclusions: Leaving the Chasm Behind",
    "text": "Conclusions: Leaving the Chasm Behind\n\nPost chasm enterprises are bound by the commitments made by the pre-chasm enterprise. Avoid making the wrong kind of commitments by looking ahead and understanding where the company needs to be in the post chasm period in terms of financial,organizational and product developmental issues.\n\n\nFinance\n\nIn the pre-chasm period, your goals is to find product market fit and prove that your product and use cases resonate with your target customer, you are not focused on profitability. Only in the post-chasm period does a company become disciplined about profitability.\nBe aware that your growth curve is more likely to be a stair case rather than a hockey stick. Early stage entrepreneurs make commitments to VC’s about hockey stick growth and end up diluting their equity when this fails to materialize\nA company’s future is not secure until break-even cash flow is achieved. In slow developing markets with low capitalization requirements,it is advisable to embrace profitability goals from day one. In fast growing markets which are capital intensive and where scale matters, relying on venture capital to ‘blitzscale’ is the best option.\nYou can hold off on investing in advertising , developing partnerships or building channel relationships until you have established early market leadership. Only once this has been established, should you invest in these and an effective marketing communications program.\n\n\n\nOrganization\n\nCrossing the chasm means going from being pioneers to being settlers\nPeople who thrive in the pre-chasm phase are technologists who are interested only in innovation. They may dislike processes, documentation, standards, common interfaces etc. As the enterprise shifts from the early-market product centric world to the mainstream market-centric one, these people have to adapt or be reassigned.\nPioneer salesmen also may not be a good fit for a post chasm enterprise. These sales people excel at making ‘visionary’ sales that are predicated on delivering a customized whole product implementation. This distracts from R&D efforts to build a standardized whole product solutions for the entire target segment.\n\nWhile crossing the chasm , institute two transitional roles.\n\nTarget Market Segment Manager (who becomes an industry marketing manager)\n\n\nThe TMS manager’s mandate is to transform a visionary customer relationship into a potential beachhead for entry into the mainstream vertical market that the particular customer participated in\nHe should work very closely with the customer to learn the ins and outs of their systems, business and industry\nSupervise and oversee the visionary’s project, break it down into achievable phases, manage the roll out of early phases, get feedback and buy in from end users and work with in-house staff to create localized implementations that deliver immediate value and impact.\nIdentify parts of the visionary projects that are to be account specific modifications and are going to be part of the whole product\n\nHe/she is also expected to:\n\nExpedite the implementation of the first installation of the system. This is essential to creating a reference base\nReplace himself with a true account manager who will serve and manage the client relationship in the future\nCreate product extensions or add-ons that solve some industry wide problems elegantly. Absorb these into the product line or distribute them informally through a users’ group.\n\n\nWhole Product Manager (who becomes a Product Marketing Manager)\n\nThis person serves the role of a product manager and an externally focused product marketing manager. The Whole product manager replaces the early stage pioneer ‘Product manager’.\nThis is essential because the early stage PM is focused on personal commitments to early customers while crossing the chasm requires prioritization of mainstream,pragmatist customer satisfaction.\nThis accomplished the transition from early stage product focus to market focus.\nCompensating Sales People\n\nEarly stage sales is account penetration. This is harder and winning the account itself is a significant achievement. Compensation should be bonus-based and event driven but should ensure the pioneer salesman does not over commit without regard to long term consequences . It should also be time bound so that a pioneer salesman does not end up overstaying their welcome.\n\n\n\nProduct Development\nTransition from product-centric R&D in the early market to whole product R&D in the mainstream market.\nTechnology Driven R&D → Market place drive R&D\nCreative technology → Creative market segmentation\nInvent new technology → Reuse existing technology"
  },
  {
    "objectID": "blog/2019-03-17-r-studio-conference-2019/index.html",
    "href": "blog/2019-03-17-r-studio-conference-2019/index.html",
    "title": "R Studio Conference 2019",
    "section": "",
    "text": "This is a notebook summarizing what I learned at the R Studio Conference 2019 Links to all the talks and slide decks,including talks on how to use R in production at scale are available at https://github.com/kbroman/RStudioConf2019Slides\nMake sure the libraries being used in each section are installed."
  },
  {
    "objectID": "blog/2019-03-17-r-studio-conference-2019/index.html#reproducible-examples-with-reprex",
    "href": "blog/2019-03-17-r-studio-conference-2019/index.html#reproducible-examples-with-reprex",
    "title": "R Studio Conference 2019",
    "section": "Reproducible Examples with reprex",
    "text": "Reproducible Examples with reprex\nThe reprex package allows you to create a minimal reproducible example that you can share if you are reporting an issue on Github or asking a question on stack overflow. Running the code chunk below after uncommenting will create a new web page with the code and the results that can be shared with others.\n# reprex({\n#   x &lt;- 1:4\n#   y &lt;- 2:5\n#   x + y\n# })"
  },
  {
    "objectID": "blog/2019-03-17-r-studio-conference-2019/index.html#categorical-data-in-r",
    "href": "blog/2019-03-17-r-studio-conference-2019/index.html#categorical-data-in-r",
    "title": "R Studio Conference 2019",
    "section": "Categorical data in R",
    "text": "Categorical data in R\nFactor variables in R can be idiosyncratic\nx &lt;- c(20,20,10,40,20)\ncat(x)\n## 20 20 10 40 20\nConverting this to a factor variable produces the following\nxf &lt;- factor(x)\nxf\n## [1] 20 20 10 40 20\n## Levels: 10 20 40\nConverting this factor back to numeric produces the following odd result\nx &lt;- as.numeric(xf)\nx\n## [1] 2 2 1 3 2\nThe right way to carry out this conversion is as follows\nx &lt;- as.numeric(as.character(xf))\nx\n## [1] 20 20 10 40 20\nOne also needs to be careful while re-ordering the levels of a factor\nratings1&lt;-  as.factor(c(rep('High',30),rep('Low',10),rep('Medium',20)))\nratings2 &lt;- ratings1\nratings3 &lt;- ratings1\ntable(ratings1)\n## ratings1\n##   High    Low Medium \n##     30     10     20\ncat('Levels: \\n')\n## Levels:\ncat(levels(ratings1))\n## High Low Medium\nWe might want to change the levels of this variable to be Low,Medium,High\nThe next couple of approaches do not yield expected results.\nlevels(ratings1) &lt;- levels(ratings1)[c(2,1,3)]\ntable(ratings1)\n## ratings1\n##    Low   High Medium \n##     30     10     20\nlevels(ratings2) &lt;- c('Low','Medium','High')\ntable(ratings2)\n## ratings2\n##    Low Medium   High \n##     30     10     20\nThe correct approach here would be as follows\nratings3 &lt;- factor(ratings3,levels=c('Low','Medium','High'))\ntable(ratings3)\n## ratings3\n##    Low Medium   High \n##     10     20     30\nThe forcats package by Hadley Wickham makes working with factors much more straightforward.\nlibrary(forcats)\nratings1&lt;-  as.factor(c(rep('High',30),rep('Low',10),rep('Medium',20)))\nratings1 &lt;- fct_relevel(ratings1,c('Low','Medium','High'))\ntable(ratings1)\n## ratings1\n##    Low Medium   High \n##     10     20     30\nRecoding the levels of the factors are also straightforward.\nratings1 &lt;- fct_recode(ratings1,Poor='Low',Fair ='Medium',Good = 'High')\ntable(ratings1)\n## ratings1\n## Poor Fair Good \n##   10   20   30\nPlenty of other useful functions are available in the forcats package."
  },
  {
    "objectID": "blog/2019-03-17-r-studio-conference-2019/index.html#defensive-coding",
    "href": "blog/2019-03-17-r-studio-conference-2019/index.html#defensive-coding",
    "title": "R Studio Conference 2019",
    "section": "Defensive Coding",
    "text": "Defensive Coding\nThe testthat and assertthat packages allows us to code defensively. This is typically best practice in software engineering and makes debugging much easier.\nFor instance if a function is designed to accept a scalar (i.e. a vector of length 1), you may want the function to throw an error if a vector is passed into the function, so that you can adjust the function appropriately.\nlibrary(assertthat)\n\nis_odd &lt;- function(x) {\n  assert_that(is.numeric(x), length(x) == 1)\n  x %% 2 == 1\n}\n\n\nprint(is_odd(3))\n## [1] TRUE\nprint(is_odd(2))\n## [1] FALSE\n#is_odd(c(1,4)) ##Throws an error##\nThe see_if function returns a logical value and an error message as an attribute that allows execution to continue.\nx &lt;- c(1,2)\ny &lt;- 'a'\nsee_if(is.numeric(x), length(x) == 1)\n## [1] FALSE\n## attr(,\"msg\")\n## [1] \"length(x) not equal to 1\"\nsee_if(is.numeric(y), length(y) == 1)\n## [1] FALSE\n## attr(,\"msg\")\n## [1] \"y is not a numeric or integer vector\"\nThe testthat package provides functionality that is essential to software testing. This is useful if you are building packages or writing code that is meant to be used in production\nlibrary(testthat)\n\n#Check for equality within numerical tolerance\nexpect_that(10,equals(10+1e-7))\n###Check for exact equality - Throws an error\n#expect_that(2*5, is_identical_to(10 + 1e-7))\nmodel &lt;- lm(mpg~wt,data=mtcars)\nexpect_that(model,is_a(\"lm\"))\n###Throws an error\n#expect_that(model,is_a(\"glm\"))\nIf you are making a function you can use this to ensure that it produces warnings when expected.\n#  Two functions below pass\nexpect_that(log(-1), gives_warning())\nexpect_that(log(-1),\ngives_warning(\"NaNs produced\"))\n\n# This one fails if run\n#expect_that(log(0), gives_warning())"
  },
  {
    "objectID": "blog/2019-03-17-r-studio-conference-2019/index.html#new-features-in-r-studio-1.2",
    "href": "blog/2019-03-17-r-studio-conference-2019/index.html#new-features-in-r-studio-1.2",
    "title": "R Studio Conference 2019",
    "section": "New Features in R Studio 1.2",
    "text": "New Features in R Studio 1.2\nR Studio 1.2 allows incorporation of SQL, Python , RCPP, Stan etc seamlessly into your workflow. The reticulate package allows you to use Python within R Studio.\n\nPython\nNote the bit below requires R Studio 1.2 and python’s Anaconda installation. Also you might encounter the issue described here and will have to fix by adding the appropriate path to the environment variable\nlibrary(reticulate)\nuse_condaenv(\"r-reticulate\")\nimport os\nos.environ['QT_QPA_PLATFORM_PLUGIN_PATH'] = 'C:/Users/learningmachine/AppData/Local/Continuum/anaconda3/Library/plugins/platforms'\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nx = np.arange(0.0,2,0.01)\ny = np.sin(2* np.pi*x)\nplt.plot(x,y)\nplt.grid(True)\nplt.show()\n\ndata = pd.DataFrame({'x':x,'y':y})\nA pandas dataframe can also be used within R.\nlibrary(ggplot2)\nggplot(py$data,aes(x,y))+ geom_line(col='blue')\n SImilarly you can embed SQL or Stan code chunks in your markdown document."
  },
  {
    "objectID": "blog/2019-03-17-r-studio-conference-2019/index.html#parsnip---a-tidy-model-interface",
    "href": "blog/2019-03-17-r-studio-conference-2019/index.html#parsnip---a-tidy-model-interface",
    "title": "R Studio Conference 2019",
    "section": "Parsnip - A tidy model interface",
    "text": "Parsnip - A tidy model interface\nlibrary(parsnip)\nlibrary(tidymodels)\nlibrary(glmnet)\nlibrary(randomForest)\nParsnip is tidy version of the popular caret package being developed by Max Kuhn (who also developed caret).This package creates a unified interface to models, organizes them by model type (e.g. Logistic regression, splines etc)and generalizes how to fit them.\nFirst create a model specification\n## Linear Regression Model Specification (regression)\n## \n## Main Arguments:\n##   penalty = 0.01\nThe computation engine can be one of lm, glmnet,stan,spark or keras. Parsnip can translate this general syntax to the model’s argument.\nreg_model %&gt;%\n  set_engine(\"glmnet\")%&gt;%\n    translate()\n## Linear Regression Model Specification (regression)\n## \n## Main Arguments:\n##   penalty = 0.01\n## \n## Computational engine: glmnet \n## \n## Model fit template:\n## glmnet::glmnet(x = missing_arg(), y = missing_arg(), weights = missing_arg(), \n##     lambda = 0.01, family = \"gaussian\")\nNote that data is not required for the model specification. Further although glmnet requires the predictors and target to be provided as X,y matrices , parsnip allows you to use a formula.\nreg_model %&gt;%\n  set_engine(\"glmnet\")%&gt;%\n    fit(mpg ~ .,data = mtcars)\n## parsnip model object\n## \n## \n## Call:  glmnet::glmnet(x = as.matrix(x), y = y, family = \"gaussian\",      lambda = ~0.01) \n## \n##      Df   %Dev Lambda\n## [1,] 10 0.8687   0.01\nThe whole sequence of steps from specification to prediction can be done as follows\nlinear_reg(penalty =0.01) %&gt;%\n  set_engine(\"glmnet\") %&gt;%\n    fit(mpg~.,dat =mtcars %&gt;% slice(1:29)) %&gt;% # train set\n      predict(new_data = mtcars %&gt;% slice(30:32)) #test set\n## # A tibble: 3 x 1\n##   .pred\n##   &lt;dbl&gt;\n## 1  17.9\n## 2  10.9\n## 3  25.6\nIf we want to fit the model using multiple penalty values:\npreds &lt;- \n  linear_reg() %&gt;%\n    set_engine(\"glmnet\")%&gt;%\n      fit(mpg ~. , data = mtcars %&gt;% slice(1:29))%&gt;%\n        multi_predict(new_data = mtcars %&gt;% slice(30:32))\n\n\nprint(preds)\n## # A tibble: 3 x 1\n##   .pred            \n##   &lt;list&gt;           \n## 1 &lt;tibble [80 x 2]&gt;\n## 2 &lt;tibble [80 x 2]&gt;\n## 3 &lt;tibble [80 x 2]&gt;\nThis produces a list of dataframes where each list contains predictions for all penalty values considered. To obtain the first 5 predictions for the second data point:\npreds %&gt;% pull(.pred) %&gt;% pluck(2) %&gt;% slice(1:5)\n## # A tibble: 5 x 2\n##   penalty .pred\n##     &lt;dbl&gt; &lt;dbl&gt;\n## 1 0.00346  10.9\n## 2 0.00380  10.9\n## 3 0.00417  10.9\n## 4 0.00457  10.9\n## 5 0.00502  10.9\nIn the cases of models like random forest where certain data dependent parameters like mtry need to be specified,the model can be specified as follows:\nmod &lt;- rand_forest(trees= 1000, mtry =floor(.preds() * 0.75)) %&gt;%\n        set_engine(\"randomForest\")\n\nmod %&gt;% translate()\n## Random Forest Model Specification (unknown)\n## \n## Main Arguments:\n##   mtry = floor(.preds() * 0.75)\n##   trees = 1000\n## \n## Computational engine: randomForest \n## \n## Model fit template:\n## randomForest::randomForest(x = missing_arg(), y = missing_arg(), \n##     mtry = floor(.preds() * 0.75), ntree = 1000)\nFit the model.\nmod %&gt;% fit(mpg ~ ., data = mtcars)\n## parsnip model object\n## \n## \n## Call:\n##  randomForest(x = as.data.frame(x), y = y, ntree = ~1000, mtry = ~floor(.preds() *      0.75)) \n##                Type of random forest: regression\n##                      Number of trees: 1000\n## No. of variables tried at each split: 7\n## \n##           Mean of squared residuals: 5.352952\n##                     % Var explained: 84.79\nAs you can see, the number of predictors used is 75 % of the number of predictors in the data.\nfloor((ncol(mtcars) - 1) *0.75)\n## [1] 7"
  },
  {
    "objectID": "blog/2019-03-17-r-studio-conference-2019/index.html#time-series-in-tidyverse",
    "href": "blog/2019-03-17-r-studio-conference-2019/index.html#time-series-in-tidyverse",
    "title": "R Studio Conference 2019",
    "section": "Time Series in Tidyverse",
    "text": "Time Series in Tidyverse\nTidy and Transform the data using the tsibble package and perform modelling/forecasting using fable. tsibble is a time series version of a tibble/dataframe.\nlibrary(tidyr)\nlibrary(tsibble)\nlibrary(lubridate)\nWe can use the EuStockMarkets data available in base R which is a ts object.\ndata(\"EuStockMarkets\")\nas_tibble can easily convert this into a tsibble in a long format. Here the index represents time and key identifies the variable that defines the series. Here each stock index ha a unique time stamp.\neu_ts &lt;- as_tsibble(EuStockMarkets)\nprint(eu_ts)\n## # A tsibble: 7,440 x 3 [1s] &lt;UTC&gt;\n## # Key:       key [4]\n##    index               key   value\n##    &lt;dttm&gt;              &lt;chr&gt; &lt;dbl&gt;\n##  1 1991-07-01 02:18:28 DAX   1629.\n##  2 1991-07-02 12:00:00 DAX   1614.\n##  3 1991-07-03 21:41:32 DAX   1607.\n##  4 1991-07-05 07:23:05 DAX   1621.\n##  5 1991-07-06 17:04:37 DAX   1618.\n##  6 1991-07-08 02:46:09 DAX   1611.\n##  7 1991-07-09 12:27:42 DAX   1631.\n##  8 1991-07-10 22:09:14 DAX   1640.\n##  9 1991-07-12 07:50:46 DAX   1635.\n## 10 1991-07-13 17:32:18 DAX   1646.\n## # ... with 7,430 more rows\nGiven the index in seconds, the tsibble expects an entry for each second, so the index should be converted to day.\neu_ts2 &lt;-  eu_ts %&gt;%\n            mutate(index = as.Date(index))\nprint(eu_ts2)\n## # A tsibble: 7,440 x 3 [1s]\n## # Key:       key [4]\n##    index      key   value\n##    &lt;date&gt;     &lt;chr&gt; &lt;dbl&gt;\n##  1 1991-07-01 DAX   1629.\n##  2 1991-07-02 DAX   1614.\n##  3 1991-07-03 DAX   1607.\n##  4 1991-07-05 DAX   1621.\n##  5 1991-07-06 DAX   1618.\n##  6 1991-07-08 DAX   1611.\n##  7 1991-07-09 DAX   1631.\n##  8 1991-07-10 DAX   1640.\n##  9 1991-07-12 DAX   1635.\n## 10 1991-07-13 DAX   1646.\n## # ... with 7,430 more rows\nOne can also calculate the average price of a week or month as follows.\neu_ts %&gt;%\n    index_by(Month = floor_date(index,'month'))%&gt;%\n        group_by(key)%&gt;%\n            summarize(Price =  mean(value))%&gt;% head()\n## # A tsibble: 6 x 3 [?] &lt;UTC&gt;\n## # Key:       key [1]\n##   key   Month               Price\n##   &lt;chr&gt; &lt;dttm&gt;              &lt;dbl&gt;\n## 1 CAC   1991-07-01 00:00:00 1753.\n## 2 CAC   1991-08-01 00:00:00 1800.\n## 3 CAC   1991-09-01 00:00:00 1871.\n## 4 CAC   1991-10-01 00:00:00 1851.\n## 5 CAC   1991-11-01 00:00:00 1814.\n## 6 CAC   1991-12-01 00:00:00 1693.\nWe can also check if there are gaps in the time series as follows.\nhas_gaps(eu_ts2)\n## # A tibble: 4 x 2\n##   key   .gaps\n##   &lt;chr&gt; &lt;lgl&gt;\n## 1 CAC   TRUE \n## 2 DAX   TRUE \n## 3 FTSE  TRUE \n## 4 SMI   TRUE\nYou can get a 30 day rolling average of the process as follows. Note that the first 29 observations of the rolling mean will be NA.\neu_ts3 &lt;-  eu_ts2 %&gt;%\n            group_by(key) %&gt;%\n              mutate(avg_30day = slide_dbl(value,~mean(.,na.rm = TRUE),.size=30))\n\nprint(eu_ts3 %&gt;% filter_index(\"1991-08-01\"~\"1991-09-01\") )\n## # A tsibble: 88 x 4 [1D]\n## # Key:       key [4]\n## # Groups:    key [4]\n##    index      key   value avg_30day\n##    &lt;date&gt;     &lt;chr&gt; &lt;dbl&gt;     &lt;dbl&gt;\n##  1 1991-08-02 DAX   1620.       NA \n##  2 1991-08-03 DAX   1620.       NA \n##  3 1991-08-05 DAX   1623.       NA \n##  4 1991-08-06 DAX   1614.       NA \n##  5 1991-08-08 DAX   1632.       NA \n##  6 1991-08-09 DAX   1630.       NA \n##  7 1991-08-10 DAX   1633.     1624.\n##  8 1991-08-12 DAX   1627.     1624.\n##  9 1991-08-13 DAX   1650.     1625.\n## 10 1991-08-15 DAX   1650.     1627.\n## # ... with 78 more rows\nTsibble works well with ggplot2.\nlibrary(ggplot2)\nggplot(eu_ts2,aes(x=index,y=value)) + geom_line()+\n  facet_wrap(.~key,ncol=1)\n\nForecasting can be done using the fable package which is a tidy implementation of the classic forecast package by Rob Hyndman. This package is being developed by Hyndman’s student Earo Wang and is currently not available on CRAN"
  },
  {
    "objectID": "blog/2019-03-17-r-studio-conference-2019/index.html#model-representation-using-broom",
    "href": "blog/2019-03-17-r-studio-conference-2019/index.html#model-representation-using-broom",
    "title": "R Studio Conference 2019",
    "section": "Model representation using Broom",
    "text": "Model representation using Broom\nUse tidy() to summarize information about models Use glance() to report goodness of fit metrics Use augment() to add information about observations\nProduces outputs in a simple tibble\nThe code below shows what we typically get when building a model\nattach(mtcars)\nlibrary(broom)\nols_fit &lt;- lm(hp~ mpg + cyl, mtcars)\nsummary(ols_fit)\n## \n## Call:\n## lm(formula = hp ~ mpg + cyl, data = mtcars)\n## \n## Residuals:\n##    Min     1Q Median     3Q    Max \n## -53.72 -22.18 -10.13  14.47 130.73 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(&gt;|t|)   \n## (Intercept)   54.067     86.093   0.628  0.53492   \n## mpg           -2.775      2.177  -1.275  0.21253   \n## cyl           23.979      7.346   3.264  0.00281 **\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 38.22 on 29 degrees of freedom\n## Multiple R-squared:  0.7093, Adjusted R-squared:  0.6892 \n## F-statistic: 35.37 on 2 and 29 DF,  p-value: 1.663e-08\nUsing tidy from the broom package we get:\ntidy(ols_fit)\n## # A tibble: 3 x 5\n##   term        estimate std.error statistic p.value\n##   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n## 1 (Intercept)    54.1      86.1      0.628 0.535  \n## 2 mpg            -2.77      2.18    -1.27  0.213  \n## 3 cyl            24.0       7.35     3.26  0.00281\nUsing glance\nglance(ols_fit)\n## # A tibble: 1 x 11\n##   r.squared adj.r.squared sigma statistic p.value    df logLik   AIC   BIC\n##       &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n## 1     0.709         0.689  38.2      35.4 1.66e-8     3  -160.  329.  335.\n## # ... with 2 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;\nUsing augment gives information about each observation in the training data\naugment(ols_fit)\n## # A tibble: 32 x 11\n##    .rownames    hp   mpg   cyl .fitted .se.fit .resid   .hat .sigma .cooksd\n##    &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;\n##  1 Mazda RX4   110  21       6   140.     6.84 -29.7  0.0320   38.5 0.00687\n##  2 Mazda RX~   110  21       6   140.     6.84 -29.7  0.0320   38.5 0.00687\n##  3 Datsun 7~    93  22.8     4    86.7   13.3    6.28 0.121    38.9 0.00141\n##  4 Hornet 4~   110  21.4     6   139.     7.00 -28.6  0.0335   38.5 0.00668\n##  5 Hornet S~   175  18.7     8   194.    12.8  -19.0  0.112    38.7 0.0117 \n##  6 Valiant     105  18.1     6   148.     8.75 -42.7  0.0524   38.0 0.0243 \n##  7 Duster 3~   245  14.3     8   206.     9.79  38.8  0.0656   38.2 0.0258 \n##  8 Merc 240D    62  24.4     4    82.3   11.6  -20.3  0.0924   38.7 0.0105 \n##  9 Merc 230     95  22.8     4    86.7   13.3    8.28 0.121    38.9 0.00246\n## 10 Merc 280    123  19.2     6   145.     7.47 -21.7  0.0382   38.7 0.00443\n## # ... with 22 more rows, and 1 more variable: .std.resid &lt;dbl&gt;\nTo compare multiple models on goodness of fit:\nlibrary(purrr)\n\nfits &lt;- list( fit1 = lm(hp~cyl,mtcars),\n              fit2 = lm(hp ~ cyl +mpg, mtcars),\n              fit3 = lm(hp~. , mtcars))\n\nmap_df(fits,glance,.id = \"model\") %&gt;% arrange(AIC)\n## # A tibble: 3 x 12\n##   model r.squared adj.r.squared sigma statistic p.value    df logLik   AIC   BIC\n##   &lt;chr&gt;     &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n## 1 fit3      0.903         0.857  26.0      19.5 1.90e-8    11  -143.  310.  327.\n## 2 fit1      0.693         0.683  38.6      67.7 3.48e-9     2  -161.  329.  333.\n## 3 fit2      0.709         0.689  38.2      35.4 1.66e-8     3  -160.  329.  335.\n## # ... with 2 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;\nA more realistic application for broom are cases where you want to bootstrap the parameter estimates of a model\nConsider the model \\[ mpg = \\frac{k}{wt} + b + \\epsilon, \\sim Normal(0,\\sigma^2) \\]\nTo estimate the bootstrapped estimates, you can do the following\nlibrary(rsample)\nboots &lt;- bootstraps(mtcars,times=100)\nFunction to fit non linear least squares\nfit_nls_on_bootstrap &lt;- function(split) {\n  nls(\n    mpg ~ k/wt + b,\n    analysis(split),\n    start = list(k=1,b=0)\n  )\n}\nBootstrap as follows:\nboot_fits &lt;- boots %&gt;%\n              mutate(fit = map(splits,fit_nls_on_bootstrap),\n                     coef_info = map(fit,tidy))\n\nhead(boot_fits)\n## # A tibble: 6 x 4\n##   splits          id           fit       coef_info       \n## * &lt;list&gt;          &lt;chr&gt;        &lt;list&gt;    &lt;list&gt;          \n## 1 &lt;split [32/13]&gt; Bootstrap001 &lt;S3: nls&gt; &lt;tibble [2 x 5]&gt;\n## 2 &lt;split [32/12]&gt; Bootstrap002 &lt;S3: nls&gt; &lt;tibble [2 x 5]&gt;\n## 3 &lt;split [32/11]&gt; Bootstrap003 &lt;S3: nls&gt; &lt;tibble [2 x 5]&gt;\n## 4 &lt;split [32/13]&gt; Bootstrap004 &lt;S3: nls&gt; &lt;tibble [2 x 5]&gt;\n## 5 &lt;split [32/11]&gt; Bootstrap005 &lt;S3: nls&gt; &lt;tibble [2 x 5]&gt;\n## 6 &lt;split [32/11]&gt; Bootstrap006 &lt;S3: nls&gt; &lt;tibble [2 x 5]&gt;\nGet bootstrapped coefficients\nlibrary(resample)\n## \n## Attaching package: 'resample'\n## The following object is masked from 'package:broom':\n## \n##     bootstrap\nlibrary(tidyr)\n\ndata(\"mtcars\")\nmtcars_bs &lt;- bootstraps(mtcars,times=20)\nboot_coefs &lt;- boot_fits %&gt;%\n                unnest(coef_info)\n\nhead(boot_coefs)\n## # A tibble: 6 x 6\n##   id           term  estimate std.error statistic  p.value\n##   &lt;chr&gt;        &lt;chr&gt;    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n## 1 Bootstrap001 k        41.2       3.38     12.2  3.86e-13\n## 2 Bootstrap001 b         5.56      1.23      4.53 8.86e- 5\n## 3 Bootstrap002 k        38.8       2.82     13.8  1.66e-14\n## 4 Bootstrap002 b         6.52      1.03      6.36 5.15e- 7\n## 5 Bootstrap003 k        51.7       4.80     10.8  8.12e-12\n## 6 Bootstrap003 b         2.61      1.70      1.53 1.36e- 1\nPlot bootstrapped estimates as follows\np &lt;- ggplot(boot_coefs,aes(estimate))+\n      geom_histogram(binwidth = 2) + facet_wrap(~ term,scales='free')+\n        labs(title = \"Sampling distribution of k and b\",\n             y = \"Count\",\n             x = \"Value\" )\n\n\n\np\n\nWe can also plot the bootstrapped predictions as follows\nboot_aug &lt;- boot_fits %&gt;%\n              mutate(augmented = map(fit,augment))%&gt;%\n                unnest(augmented)\n\n\nggplot(boot_aug,aes(wt,mpg))+\n  geom_point()+\n    geom_line(aes(y = .fitted,group =id),alpha =0.2)"
  },
  {
    "objectID": "blog/2019-03-17-r-studio-conference-2019/index.html#gganimate",
    "href": "blog/2019-03-17-r-studio-conference-2019/index.html#gganimate",
    "title": "R Studio Conference 2019",
    "section": "gganimate",
    "text": "gganimate\nPackage for animated graphics which extends ggplot2. Additional dependencies will have to be installed.\nlibrary(gapminder)\nlibrary(gganimate)\n\n\nggplot(gapminder, aes(gdpPercap, lifeExp, size = pop, colour = country)) +\n  geom_point(alpha = 0.7, show.legend = FALSE) +\n  scale_colour_manual(values = country_colors) +\n  scale_size(range = c(2, 12)) +\n  scale_x_log10() +\n  # Here comes the gganimate specific bits\n  labs(title = 'Year: {frame_time}', x = 'GDP per capita', y = 'life expectancy') +\n  transition_time(year) +\n  ease_aes('linear')"
  },
  {
    "objectID": "blog/2019-03-17-r-studio-conference-2019/index.html#hypothetical-outcome-plots",
    "href": "blog/2019-03-17-r-studio-conference-2019/index.html#hypothetical-outcome-plots",
    "title": "R Studio Conference 2019",
    "section": "Hypothetical Outcome Plots",
    "text": "Hypothetical Outcome Plots\nHypothetical outcome plots provide an alternative way to visualize uncertainty by animating a finite set of individual draws rather than producing a static representation of distributions.\nLoad required packages.\nlibrary(rsample)\nlibrary(purrr)\nlibrary(ggplot2)\nlibrary(gganimate)\nlibrary(transformr)\nThe plot below uses bootstrapped samples to visualize uncertainty in the trend line.\nattach(mtcars)\n\n\n#Draw bootstrap samples\n\nmtcars_bs &lt;- bootstraps(mtcars,times=20)\n\nmtcars_bs_df &lt;- map_df(mtcars_bs$splits, function(x){\n                        return(as.data.frame(x))\n                        })\n\nmtcars_bs_df$id &lt;- rep(c(1:20),each=nrow(mtcars))\n\n\nmtcars %&gt;% \n  ggplot(aes(disp,mpg))+\n    geom_point()+ \n      geom_smooth(\n        data = mtcars_bs_df,\n        aes(group =id),\n        se = FALSE,\n        alpha = 0.6\n      )\n\nThe same can be represented using a hypothetical outcome plot as follows\nmtcars_bs_df %&gt;%\n  ggplot(aes(disp,mpg))+\n    geom_smooth(aes(group =id),se=FALSE,alpha=0.6)+\n      geom_point(data=mtcars)+\n        transition_states(id)"
  },
  {
    "objectID": "blog/2019-03-17-r-studio-conference-2019/index.html#datapasta",
    "href": "blog/2019-03-17-r-studio-conference-2019/index.html#datapasta",
    "title": "R Studio Conference 2019",
    "section": "Datapasta",
    "text": "Datapasta\nIf you want to quickly copy some sample data from excel or SQL into R, you often have to convert it into csv or excel file and then import it or type it manually. Datapasta is an R Studio add in that provides a much easier alternative.\nAfter installing the package ‘datapasta’ , simply copy the table from an excel file, go to add ins and select the appropriate option.\nHere, I simply copied a table from excel using ‘Ctrl+C’ and then selected ‘Paste as data.frame’ from the add ins dropdown.\ndata.frame(stringsAsFactors=FALSE,\n        Name = c(\"A\", \"B\", \"C\", \"D\", \"E\"),\n       Marks = c(87L, 35L, 76L, 34L, 86L)\n)\n##   Name Marks\n## 1    A    87\n## 2    B    35\n## 3    C    76\n## 4    D    34\n## 5    E    86"
  },
  {
    "objectID": "blog/2019-03-17-r-studio-conference-2019/index.html#vctrs",
    "href": "blog/2019-03-17-r-studio-conference-2019/index.html#vctrs",
    "title": "R Studio Conference 2019",
    "section": "vctrs",
    "text": "vctrs\nThis is another package from Hadley Wickham designed to make sure that outputs from functions are predictable.\nSometimes, R does not behave predictable. For instance when combining two factors..\nc(factor(\"a\"),factor(\"b\"))\n## [1] 1 1\nAlso..\ntoday &lt;- as.Date('2019-02-23')\ntomorrow &lt;- as.POSIXct(\"2019-02-23 12:00\",tz = \"America/Chicago\")\n\nc(today,tomorrow)\n## [1] \"2019-02-23\"    \"4248312-08-21\"\nAlso..\nc(NULL,tomorrow)\n## [1] 1550944800\nThe vctrs package formalizes the relationships between various datatypes and requires the user to be more explicit when combining different datatypes.\nTo summarize: Logical –&gt; Integer –&gt; Double Date –&gt; Date-time Factor –&gt;Character Ordered –&gt; Character\nlibrary(vctrs)\n## \n## Attaching package: 'vctrs'\n## The following object is masked from 'package:lubridate':\n## \n##     new_duration\nWhen coercing to character , it is stricter than base R. vec_c below would throw an error\nc(1.5,\"a\")\n## [1] \"1.5\" \"a\"\n#vec_c(1.5,\"a\")\nInstead you have to be explicit..\nvec_c(1.5,\"x\",.ptype = character())\n## [1] \"1.5\" \"x\"\nUnlike in base R, the following works..\nvec_c(factor(\"a\"),factor(\"b\"))\n## [1] a b\n## Levels: a b\nNote the order affects the levels..\nvec_c(factor(\"b\"),factor(\"a\"))\n## [1] b a\n## Levels: b a\nSimilarly when concatenating a factor and an ordered factor…\nc(factor(\"a\"),ordered(\"b\"))\n## [1] 1 1\nThe first call below would throw an error..\n#vec_c(factor(\"a\"),ordered(\"b\"))\nvec_c(factor(\"a\"),ordered(\"b\"),.ptype = character())\n## [1] \"a\" \"b\"\nThe date problem does not occur here..\nvec_c(today,tomorrow)\n## [1] \"2019-02-23 00:00:00 CST\" \"2019-02-23 12:00:00 CST\"\nDisparate datatypes can be combined into a list\nvec_c(today,factor(\"a\"),.ptype=list())\n## [[1]]\n## [1] \"2019-02-23\"\n## \n## [[2]]\n## [1] a\n## Levels: a"
  },
  {
    "objectID": "blog/2019-03-17-r-studio-conference-2019/index.html#tidy-evaluation",
    "href": "blog/2019-03-17-r-studio-conference-2019/index.html#tidy-evaluation",
    "title": "R Studio Conference 2019",
    "section": "Tidy Evaluation",
    "text": "Tidy Evaluation\nA tool for metaprogramming. This becomes important when you do not know the names of an object in advance and you want to build a tidyverse pipeline which operates on such objects. This is often the case when you have to operate on user inputs, say in a shiny application where you have to make indirect references with column names stored in variables or passed as function arguments.\nIt is tidy evaluation that really allows you to use column names below without using quotes.\nattach(starwars)\n\nstarwars %&gt;%\n  filter(homeworld == \"Tatooine\")%&gt;%\n    arrange(height)%&gt;%\n      select(name,ends_with(\"color\"))\n## # A tibble: 10 x 4\n##    name               hair_color  skin_color eye_color\n##    &lt;chr&gt;              &lt;chr&gt;       &lt;chr&gt;      &lt;chr&gt;    \n##  1 R5-D4              &lt;NA&gt;        white, red red      \n##  2 Shmi Skywalker     black       fair       brown    \n##  3 Beru Whitesun lars brown       light      blue     \n##  4 C-3PO              &lt;NA&gt;        gold       yellow   \n##  5 Luke Skywalker     blond       fair       blue     \n##  6 Owen Lars          brown, grey light      blue     \n##  7 Biggs Darklighter  black       light      brown    \n##  8 Cliegg Lars        brown       fair       blue     \n##  9 Anakin Skywalker   blond       fair       blue     \n## 10 Darth Vader        none        white      yellow\nTo evaluate an expression, the interpreter searches the environment for name-value bindings. In tidy eval, the expression is modified or the chain of searched environments is modified. This is what allows you to use unquoted variable names and work inside a dataframe. This is also referred to as non-standard evaluation.\nTidy eval can be implemented in the following ways..\n\nUsing ‘…’\n\nattach(mtcars)\n\nmeasure_hp &lt;- function(df,...){\n  df %&gt;%\n    group_by(...)%&gt;%\n      summarize(avg_hp = mean(hp,na.rm=TRUE))}\n\nmeasure_hp(mtcars,gear)\n## # A tibble: 3 x 2\n##    gear avg_hp\n##   &lt;dbl&gt;  &lt;dbl&gt;\n## 1     3  176. \n## 2     4   89.5\n## 3     5  196.\n\nUsing enquo and !!(called ‘bang!bang!’ operator)\n\nmeasure&lt;- function(df,group_var,summary_var){\n  group_var &lt;- enquo(group_var) #Effectively quotes the variable\n  summary_var &lt;- enquo(summary_var)\n  \n  df %&gt;%\n    group_by(!!group_var)%&gt;%\n      summarize(avg_hp = mean(!!summary_var,na.rm=TRUE))}\n\nmeasure(mtcars,gear,hp)\n## # A tibble: 3 x 2\n##    gear avg_hp\n##   &lt;dbl&gt;  &lt;dbl&gt;\n## 1     3  176. \n## 2     4   89.5\n## 3     5  196.\nIf you need to manipulate user input that is passed as character strings,you can do the following..\nlibrary(rlang)\ngroup_var &lt;- 'gear'\nsummary_var &lt;- 'hp'\n\nmeasure(mtcars,!!rlang::sym(group_var),!!rlang::sym(summary_var))\n## # A tibble: 3 x 2\n##    gear avg_hp\n##   &lt;dbl&gt;  &lt;dbl&gt;\n## 1     3  176. \n## 2     4   89.5\n## 3     5  196.\n\nAction and Selection Verbs\ndplyr verbs comes in two flavors - action and selection. select is a selection verb while verbs like mutate,group_by and transmute are action verbs.\nAction verbs created new vectors and modify the dataframe.Selection verbs look up the position of the columns in the dataframe and re-organizes the dataframe.\nSelection verbs are context aware and know about current variables.\nstarwars %&gt;% select(c(1:height))\n## # A tibble: 87 x 2\n##    name               height\n##    &lt;chr&gt;               &lt;int&gt;\n##  1 Luke Skywalker        172\n##  2 C-3PO                 167\n##  3 R2-D2                  96\n##  4 Darth Vader           202\n##  5 Leia Organa           150\n##  6 Owen Lars             178\n##  7 Beru Whitesun lars    165\n##  8 R5-D4                  97\n##  9 Biggs Darklighter     183\n## 10 Obi-Wan Kenobi        182\n## # ... with 77 more rows\nstarwars %&gt;% select(1:height)\n## # A tibble: 87 x 2\n##    name               height\n##    &lt;chr&gt;               &lt;int&gt;\n##  1 Luke Skywalker        172\n##  2 C-3PO                 167\n##  3 R2-D2                  96\n##  4 Darth Vader           202\n##  5 Leia Organa           150\n##  6 Owen Lars             178\n##  7 Beru Whitesun lars    165\n##  8 R5-D4                  97\n##  9 Biggs Darklighter     183\n## 10 Obi-Wan Kenobi        182\n## # ... with 77 more rows\nstarwars %&gt;% select(-1,-height)\n## # A tibble: 87 x 11\n##     mass hair_color skin_color eye_color birth_year gender homeworld species\n##    &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;      &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;     &lt;chr&gt;  \n##  1    77 blond      fair       blue            19   male   Tatooine  Human  \n##  2    75 &lt;NA&gt;       gold       yellow         112   &lt;NA&gt;   Tatooine  Droid  \n##  3    32 &lt;NA&gt;       white, bl~ red             33   &lt;NA&gt;   Naboo     Droid  \n##  4   136 none       white      yellow          41.9 male   Tatooine  Human  \n##  5    49 brown      light      brown           19   female Alderaan  Human  \n##  6   120 brown, gr~ light      blue            52   male   Tatooine  Human  \n##  7    75 brown      light      blue            47   female Tatooine  Human  \n##  8    32 &lt;NA&gt;       white, red red             NA   &lt;NA&gt;   Tatooine  Droid  \n##  9    84 black      light      brown           24   male   Tatooine  Human  \n## 10    77 auburn, w~ fair       blue-gray       57   male   Stewjon   Human  \n## # ... with 77 more rows, and 3 more variables: films &lt;list&gt;, vehicles &lt;list&gt;,\n## #   starships &lt;list&gt;\nYou can also use selection helpers as follows..\nstarwars %&gt;% select(ends_with(\"color\")) %&gt;% head()\n## # A tibble: 6 x 3\n##   hair_color  skin_color  eye_color\n##   &lt;chr&gt;       &lt;chr&gt;       &lt;chr&gt;    \n## 1 blond       fair        blue     \n## 2 &lt;NA&gt;        gold        yellow   \n## 3 &lt;NA&gt;        white, blue red      \n## 4 none        white       yellow   \n## 5 brown       light       brown    \n## 6 brown, grey light       blue\nstarwars %&gt;% select(dplyr::matches(\"^[nm]a\")) %&gt;% head()\n## # A tibble: 6 x 2\n##   name            mass\n##   &lt;chr&gt;          &lt;dbl&gt;\n## 1 Luke Skywalker    77\n## 2 C-3PO             75\n## 3 R2-D2             32\n## 4 Darth Vader      136\n## 5 Leia Organa       49\n## 6 Owen Lars        120\nstarwars %&gt;% select(10,everything()) %&gt;% head()\n## # A tibble: 6 x 13\n##   species name  height  mass hair_color skin_color eye_color birth_year gender\n##   &lt;chr&gt;   &lt;chr&gt;  &lt;int&gt; &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;      &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt; \n## 1 Human   Luke~    172    77 blond      fair       blue            19   male  \n## 2 Droid   C-3PO    167    75 &lt;NA&gt;       gold       yellow         112   &lt;NA&gt;  \n## 3 Droid   R2-D2     96    32 &lt;NA&gt;       white, bl~ red             33   &lt;NA&gt;  \n## 4 Human   Dart~    202   136 none       white      yellow          41.9 male  \n## 5 Human   Leia~    150    49 brown      light      brown           19   female\n## 6 Human   Owen~    178   120 brown, gr~ light      blue            52   male  \n## # ... with 4 more variables: homeworld &lt;chr&gt;, films &lt;list&gt;, vehicles &lt;list&gt;,\n## #   starships &lt;list&gt;\ngroup_by is an action verb, it does not work with selection helpers. The code below does not work.\n#starwars %&gt;% group_by(ends_with\"color\")\nInstead you can use..\nstarwars %&gt;% group_by_at(vars(ends_with(\"color\"))) %&gt;% summarise(mean_height=mean(height))\n## # A tibble: 67 x 4\n## # Groups:   hair_color, skin_color [50]\n##    hair_color    skin_color       eye_color mean_height\n##    &lt;chr&gt;         &lt;chr&gt;            &lt;chr&gt;           &lt;dbl&gt;\n##  1 &lt;NA&gt;          gold             yellow            167\n##  2 &lt;NA&gt;          green            black             173\n##  3 &lt;NA&gt;          green-tan, brown orange            175\n##  4 &lt;NA&gt;          white, blue      red                96\n##  5 &lt;NA&gt;          white, red       red                97\n##  6 auburn        fair             blue              150\n##  7 auburn, grey  fair             blue              180\n##  8 auburn, white fair             blue-gray         182\n##  9 black         blue, grey       yellow            137\n## 10 black         brown            brown             171\n## # ... with 57 more rows"
  },
  {
    "objectID": "blog/2019-03-17-r-studio-conference-2019/index.html#profviz",
    "href": "blog/2019-03-17-r-studio-conference-2019/index.html#profviz",
    "title": "R Studio Conference 2019",
    "section": "profviz",
    "text": "profviz\nprofviz can be used to profile your code, identify modules that slow down the program and take corrective action.\nlibrary(profvis)\n#profvis::profvis(mtcars %&gt;% lm(mpg ~ wt +cyl,.))"
  },
  {
    "objectID": "blog/2019-03-17-r-studio-conference-2019/index.html#using-spark-to-scale-out-your-jobs",
    "href": "blog/2019-03-17-r-studio-conference-2019/index.html#using-spark-to-scale-out-your-jobs",
    "title": "R Studio Conference 2019",
    "section": "Using Spark to scale out your jobs",
    "text": "Using Spark to scale out your jobs\nSpark allows you to leverage compute from mutliple machines by distributing computations across multiple machines. The sparklyr package allows you to leverage the power of Spark through R.\nAlthough we connect to spark running on the local machine below, we can easily connect to a cluster provided by a cloud provider. Make sure Java is installed in a directory which does not have spaces or special characters. This link may also be useful\nlibrary(DBI)\nlibrary(dplyr)\nlibrary(sparklyr)                                   #R Interface to Apache Spark\n## \n## Attaching package: 'sparklyr'\n## The following object is masked from 'package:rlang':\n## \n##     invoke\n## The following object is masked from 'package:purrr':\n## \n##     invoke\n#spark_install()                                    #Install Apache Spark\n\n#Add path to java to environment variables\nSys.setenv(JAVA_HOME = 'C:/Users/learningmachine/Java/jre1.8.0_201')\n\n\nsc &lt;- spark_connect(master = 'local')               #Connect to local cluster\n##Make sure you have the mtcars.csv file in your working directory\ncars_tbl &lt;- spark_read_csv(sc,\"cars\",\"mtcars.csv\")     #Read data into spark\n\nsummarize(cars_tbl, n =n())                         #Count records with dplyr\n## # Source: spark&lt;?&gt; [?? x 1]\n##       n\n##   &lt;dbl&gt;\n## 1    32\ndbGetQuery(sc,\"SELECT count(*) FROM cars\")          # COunt records with DBI\n##   count(1)\n## 1       32\nPerform linear regression\nml_linear_regression(cars_tbl,mpg~wt+cyl)\n## Formula: mpg ~ wt + cyl\n## \n## Coefficients:\n## (Intercept)          wt         cyl \n##   39.686261   -3.190972   -1.507795\nYou can also define a pipeline and run it on a spark cluster and save it to disk.\npipeline &lt;- ml_pipeline(sc) %&gt;%                #Define Spark pipleine\n              ft_r_formula(mpg~wt + cyl)%&gt;%    #Add formula translation\n                ml_linear_regression()         #Add model to pipeline\n\nfitted &lt;- ml_fit(pipeline,cars_tbl)\nfitted\n## PipelineModel (Transformer) with 2 stages\n## &lt;pipeline_d2021587785&gt; \n##   Stages \n##   |--1 RFormulaModel (Transformer)\n##   |    &lt;r_formula_d201fa95296&gt; \n##   |     (Parameters -- Column Names)\n##   |      features_col: features\n##   |      label_col: label\n##   |     (Transformer Info)\n##   |      formula:  chr \"mpg ~ wt + cyl\" \n##   |--2 LinearRegressionModel (Transformer)\n##   |    &lt;linear_regression_d201098f3a&gt; \n##   |     (Parameters -- Column Names)\n##   |      features_col: features\n##   |      label_col: label\n##   |      prediction_col: prediction\n##   |     (Transformer Info)\n##   |      coefficients:  num [1:2] -3.19 -1.51 \n##   |      intercept:  num 39.7 \n##   |      num_features:  int 2 \n##   |      scale:  num 1\n#ml_save(fitted,\"mtcars_model\",overwrite = TRUE)\nThe sparklyr output of a saved Spark ML Pipeline object is in Scala code, which means that the code can be added to the scheduled Spark ML jobs, and without any dependencies in R."
  },
  {
    "objectID": "blog/2019-03-17-r-studio-conference-2019/index.html#shiny",
    "href": "blog/2019-03-17-r-studio-conference-2019/index.html#shiny",
    "title": "R Studio Conference 2019",
    "section": "Shiny",
    "text": "Shiny\nSome resources worth exploring on shiny and included in the link shared at the beginning of the document are\n\nModules: How to compartmentalize your shiny application into modules for easier maintenance and concurrent development\nReactlog 2.0: Library built in Javascript that allows for easier debugging by visualizing reactive dependencies in the app. Th goto tool for reactivity debugging"
  },
  {
    "objectID": "blog/2019-03-17-r-studio-conference-2019/index.html#miscellaneous",
    "href": "blog/2019-03-17-r-studio-conference-2019/index.html#miscellaneous",
    "title": "R Studio Conference 2019",
    "section": "Miscellaneous",
    "text": "Miscellaneous\n\nSunburst Charts\nPretty cool visualization for representing hierarchical data using concentric circles.\nlibrary(sunburstR)\nsequences &lt;- read.csv(\n  system.file(\"examples/visit-sequences.csv\",package=\"sunburstR\")\n  ,header=F\n  ,stringsAsFactors = FALSE\n)\n\nsunburst(sequences)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLegend\n\n\n\n\n\n\n\n\n\n\nrayshader\nThis is a really cool package. According to the documentation:\nrayshader uses a combination of raytracing, spherical texture mapping, lambertian reflectance, and ambient occlusion to produce hillshades of elevation matrices. Includes water detection and layering functions, programmable color palette generation, several built-in textures, 2D and 3D plotting options, and the ability to export 3D maps to a 3D printable format.\nTry running the code below to see some really cool 3d visualizations.\nlibrary(rayshader)\n\nmontereybay %&gt;%\nsphere_shade(texture=\"imhof2\") %&gt;%\nplot_3d(montereybay, zscale=50, water = TRUE, watercolor=\"imhof2\",\nwaterlinecolor=\"white\", waterlinealpha=0.5)"
  },
  {
    "objectID": "blog/2019-03-17-r-studio-conference-2019/index.html#other-usefulinteresting-resources",
    "href": "blog/2019-03-17-r-studio-conference-2019/index.html#other-usefulinteresting-resources",
    "title": "R Studio Conference 2019",
    "section": "Other useful/interesting resources",
    "text": "Other useful/interesting resources\n\nBest practices for naming files: link\nmlflow for tracking ML experiments,sharing and deploying : link\nAll the risk calculators at http://riskcalc.org/ were built by Cleveland Clinic using Shiny"
  },
  {
    "objectID": "blog/2020-05-16-putting-ml-into-aml/index.html",
    "href": "blog/2020-05-16-putting-ml-into-aml/index.html",
    "title": "Putting ML into AML",
    "section": "",
    "text": "Machine learning has swept through and transformed several industries in the past decade. Financial Crime and Compliance has bucked this trend so far with incremental change the far more likely outcome than any radical transformation. Although several vendors have provided a preliminary set of ML capabilities for early adopters, the future road map for these products should address the specific challenges that prevent the wider adoption and use of these tools as well as introduce capabilities to address specific problems business users face.\nIn this blog post, I outline some of the challenges that militate against wider adoption of ML and how these can be potentially addressed."
  },
  {
    "objectID": "blog/2020-05-16-putting-ml-into-aml/index.html#challenges",
    "href": "blog/2020-05-16-putting-ml-into-aml/index.html#challenges",
    "title": "Putting ML into AML",
    "section": "Challenges",
    "text": "Challenges\n\n1) Data Limitations\nModels are only as good as the data you feed it. This is particularly true for supervised machine learning models that are the most widely used class of models in the industry. The major obstacle that financial institutions face is that they simply don’t have reliable high quality data to train the models. The problem has two facets\na)Labels are not clean and reliable\nHuman experts often provide labels at a level that does not align with the requirements of specific models. For instance, most banks decision cases rather than alerts. This means the data is not fit for use for models that are to be built at an alert level. Inferring labels at an alert level from a case level introduces a lot of noise into the labels.\nFurther all ‘good’ alerts or cases are not equally good.The incidence of money laundering is not as clear cut as say fraud, where there is clear evidence of fraudulent behavior due to monetary loss. In AML,analysts or investigators are more likely to err on the side of caution due to the fact that the cost of false negatives is much higher than that of false positives.Therefore binary labels may not be appropriate for building models in AML.\nb)Unrepresentative Training Data\nMost banks haven’t been actively collecting data to support ML initiatives. The existing labelled data at banks are typically the result of rule based systems that focus on only certain regions of the population. Models trained on such limited data are typically not applicable to the entire population.\nThis leads to a kind of Chicken and Egg problem. Without evidence that ML models can deliver value, most banks are reluctant to embrace these approaches or invest in collecting data. However, without good data, it will be challenging to build good models.\n\n\n2) Model Risk and Utility\nLimitations in labels and data means that models developed using such data will likely have limitations in terms of meeting performance and model risk requirements. Given AML models are Tier 1 models in most institutions subject to the highest level of scrutiny, banks struggle with the overheads that come with deploying such a model\nThe requirements for deploying a model includes a thorough understanding of the weaknesses of the model and putting adequate controls in place to mitigate any risks. Documentation and continual ongoing monitoring are also challenges that banks need assistance with. Many banks are left wondering whether the benefits from deploying ML are worth the costs. It will be imperative to either reduces the costs of doing ML or engage the right levers to increase returns from ML.\n\n\n3) Low appetite for a transition to ML\nMost AML and Compliance teams are risk averse. To transition away from a stable system that has received regulatory approval can be a challenge. There might also be an element of the sunk cost fallacy at play here given the significant resources banks have invested to set up and monitor rule based systems.\nA clear well supported path to transition from a rule based regime to an ML driven regime may be necessary to convince banks to begin this journey."
  },
  {
    "objectID": "blog/2020-05-16-putting-ml-into-aml/index.html#solutions",
    "href": "blog/2020-05-16-putting-ml-into-aml/index.html#solutions",
    "title": "Putting ML into AML",
    "section": "Solutions",
    "text": "Solutions\nTo facilitate the adoption of ML, we need to intervene at one or more point in the cycle above. Although gathering more and higher quality data is the easy and obvious solution that would immediately increase the quality of models and the returns from ML, this is not something that can be accomplished overnight. Most banks do not have the appetite for ML to do this today. Even for banks that are willing to show initiative, efficient systems to gather data are not available.\n\n1) Limited Models can be useful\n\nAll models are wrong but some are useful - George EP Box\n\nGiven the limitations in the data, it is to be acknowledged that building a perfect model is not possible. However, vendors can innovate to allow banks to build models that are useful and help them navigate challenges around model risk.\nBelow are some of the ways vendors can help banks can do this:\n\nLearning with Noisy Labels\n\nThere has been extensive research on how to learn with noisy labels . Vendors need to enable models that are designed to learn from noisy/inaccurate labels.\nb)Make a model’s limitations explicit\nOCC’s supervisory guidance on model risk management says:\n\nAn understanding of model uncertainty and inaccuracy and a demonstration that the bank is accounting for them appropriately are important outcomes of effective model development, implementation, and use. Because they are by definition imperfect representations of reality, all models have some degree of uncertainty and inaccuracy. These can sometimes be quantified, for example, by an assessment of the potential impact of factors that are unobservable or not fully incorporated in the model, or by the confidence interval around a statistical model’s point estimate. Indeed, using a range of outputs, rather than a simple point estimate, can be a useful way to signal model uncertainty and avoid spurious precision.\n\nIt is essential to understand where a model can be used with confidence. Providing the tools for a user to understand this ensures that the model is not misused - the primary concern of Model Risk and Regulators.\nVendor solutions should include models that make quantification of uncertainty explicit. This can range from something simple such as making confidence intervals and predictions intervals available for linear models or something more ambitious such as introducing Bayesian models that provide very intuitive estimates of model and prediction uncertainty.Bayesian models also mitigate the limitation of using unrepresentative data by providing wider credible intervals in regions where data is limited.\n\n\n2) Enable gathering of better data and support better reporting\nOnce banks have the guard rails to get their ML journey started and they begin to see value in these projects, there will be a willingness to invest additional resources to gather additional data that is required to create high performing models. There has been plenty of research in how to efficiently gather label data to improve ML model performance. Some of these are summarized below.\n\nVendors can add value by providing solutions that allow banks to leverage one or more of these techniques to efficiently collect and label data. This will be a major driver in improving model performance.\nAllowing analysts to indicate the confidence they have in a decision when they review alerts or cases is another simple functionality that can improve the quality of data.\nVendors can greatly simplify the adoption of ML models by providing accurate and comprehensive documentation that preempt any concerns or questions from Model Risk . Tooling that simplifies ongoing monitoring and detection of concept drift will help address concerns of most Model Risk departments.\nRisk reporting tools that clearly tie outcomes of ML models to business KPIs can convince business stakeholders of the value of adopting ML.\n\n\n3) A pathway to ML\nAn enterprising vendor will provide a clear well supported path for banks to make the transition from rule based systems to ML based systems. This might include partnerships where banks and vendors can leverage their respective strengths in data,domain expertise and technology to build solutions. A well defined transition plan where ML models initially coexist with rules and eventually replace rules is likely to find favor with regulators.\nVendors should collaborate with multiple banks to create a rich set of engineered, validated features. Money laundering is a challenge facing all financial institutions, it is fairly unique opportunity for banks to closely collaborate with each other and openly share their learning so as to the manage the risk associated with such a transition."
  },
  {
    "objectID": "blog/2020-05-16-putting-ml-into-aml/index.html#conclusion",
    "href": "blog/2020-05-16-putting-ml-into-aml/index.html#conclusion",
    "title": "Putting ML into AML",
    "section": "Conclusion",
    "text": "Conclusion\nThe journey to a strong ML driven compliance program is an iterative one requiring multiple rounds of experimentation, learning and incremental improvement. Vendors need to understand the several challenges banks face and support them at every step of this journey. Vendors who are bold enough to invest in partnerships with financial institutions and value long term success over short term profits have an opportunity to truly transform this industry."
  },
  {
    "objectID": "blog/2020-05-16-putting-ml-into-aml/index.html#references",
    "href": "blog/2020-05-16-putting-ml-into-aml/index.html#references",
    "title": "Putting ML into AML",
    "section": "References",
    "text": "References\n\nhttp://ai.stanford.edu/blog/weak-supervision/"
  },
  {
    "objectID": "blog/2020-09-05-the-labeled-data-problem-in-aml.en/index.html",
    "href": "blog/2020-09-05-the-labeled-data-problem-in-aml.en/index.html",
    "title": "The Labeled Data Problem in AML",
    "section": "",
    "text": "Introduction\nSupervised ML models have been successfully used to detect fraud in the financial industry for several years. Using similar ML models to detect money laundering almost seems inevitable but is still not widely used in industry. The application of supervised models in AML may be subject to some limitations compared to fraud as described below.\n\nUnlike AML, an incidence of fraud can be confirmed with 100% certainty as there is often a financial loss or evidence of an account take over when a customer calls in to report fraud. This means the labels are accurate. In AML however, it is very rare that a case is truly confirmed as money Laundering. Given law enforcement never confirms that reported cases were determine to be Money laundering, the labels in AML are not 100% certain. Given the lower quality of labels, the performance of supervised models in AML may always fall shy of those in Fraud\nIn fraud, the kind of fraud that occurs in Credit Card, Debit Card or check deposits are distinct. Given each product is associated with a different kind of fraud, it is easier to build models targeting specific types of fraud. In AML however such obvious distinctions do not exist. There is a dependency on a bank to create an appropriate taxonomy for different kinds of AML cases and on AML analysts to label cases accordingly. Given this is not standard practice today, building target supervised models in the AML domain may be harder\nAs explained in this article by Andreesen Horowitz, AML is characterized by ‘local long tailed’ distributions of data.\n\n\nIf different typologies of suspicious behavior are very different and they vary from institution to institution, the cost of building an accurate model will be much higher per customer.\nDiseconomies of scale: For long tailed problems, you need exponentially more data to get a linear improvement in results.\n\nThese are real problems that need to be addressed in partnership with financial institutions.\nA fourth problem that is cited as one that stymies the use of supervised models in AML is the ‘Lack of Below The Line data’. I describe this problem below and argue why it is an incorrect description of the problem and how framing of the problem in this manner leads to inappropriate solutions.\n\n\nBTL Data\nBelow the line data is the region of alerts below the current thresholds. Consider a scenario:\n\nIf High Risk Credit Amount &gt;= 5000 and High Risk Debit Amount &gt;= 5000 : Trigger an Alert\n\nThe below the line region is represented by the green dots in the figure below. They are ‘non alerts’ that would have been alerts had the thresholds been lower. All the labeled data that is available for training a model is ATL data (the red and blue). Clearly if we train the model just on the ATL data, the model will not be applicable to the BTL data.\n\nHowever, the supervised model is not being trained with just the two parameters used in the scenario above, in fact the supervised model can use engineered features that are derived from the two features above or any number of additional attributes. For instance, one feature can be the Perc_Credit_HR_Amt which captures what fraction of the Total Credit Amount is High risk and the Perc_Debit_HR_amt which captures what fraction of the Total Debit Amount is High Risk. The same data in this transformed feature space is shown below.\n\nIn this transformed feature space, there is no clear line demarcating ATL from BTL. Given we are training the model in this transformed feature space, the argument that the model can be used only in a distinct well defined portion of the feature space or ‘ATL’ does not hold.\nThe solution proposed to address this problem is to gather more BTL data as part of the BTL testing process.\nBTL testing is the process of sampling the population of non-alerts below the existing thresholds and reviewing them manually to evaluate whether the current thresholds are appropriate, whether they can be retained or have to be changed\nHowever using BTL sampling as a vehicle to gather labelled data for model training has several issues.\n\nBTL samples sizes are calculated using the hyper-geometric sampling calculator. The hyper-geometric sample size is calculated such that finding at least one effective alert in the sample indicates that the effectiveness of the population is greater than the acceptable effectiveness assumed to calculate the sample size. This means that banks typically do not review the entire sample but stop as soon as they find the first effective alert in the sample.\nThey are just samples of the BTL region when the entire population of the ATL region is available. Given the large imbalance between ATL and BTL data available to the model, any model trained on this data will be biased towards the ATL data. this is further compounded by the fact that the incidence of effective alerts is likely to be much lower in the BTL population with the result that the model is far less likely to accurately detect suspicious activity BTL\nThe samples are random and hence are likely to be redundant. If the BTL samples are from a region in the transformed feature space where sufficient data is already available, this additional BTL data will not make any difference to the model.\n\n\n\nBuilding Supervised Models for AML\nAlthough I have argued that the lack of BTL data is an incorrect framing of the problem, the lack of representative data of the entire population is still a problem. To make supervised ML viable in the AML domain, there are potentially two solutions to this problem, the first will be a two-pronged strategy utilizing the following\n\nIncorporate uncertainty intervals into predictions:\n\nThe model should make explicit the confidence it has in a prediction. This will ensure that the model is used appropriately only in instances where it has a higher degree of confidence. In regions where the model has a lower degree of confidence, simpler rules can be utilized. This will mitigate any model risk arising from using the model inappropriately.\n\nGather more data using Active Learning:\n\nWhile a) will allow Financial institutions to start using ML solutions today, this will allow the FI to efficiently collect data that can enhance the performance of the model over time.\nActive learning is a more deliberate and efficient way to collect data rather than randomly sampling below the line. it ensures the data gathered is not redundant and ensures an improvement in the model’s confidence and performance in every iteration.\nThe second solution will leverage semi-supervised learning in some form.\nSemi Supervised Learning, typically involves using a small amount of labelled data to train a teacher model model and using this model to generate predictions on unlabeled data. Now you pre-train a student model on this larger data set and fine tune on the original labelled data. the concept of fine-tuning is applicable mostly to neural networks, so this needs to be translated to classical ML models.\nA successful application of semi supervised ML was presented by Visa Research at the 2019 O’Reilly AI Conference to detect merchant breaches. Their approach is to train a CNN on very limited merchant data breaches and use it to label the much larger unlabeled data set. A second model is then trained on the labelled data and used to detect these breaches. The solution led to detection of breaches 48 days earlier than the incumbent approach and fraud reduction to the tune of $ 7M.\nAs per Andreesen Horrowitz, companies addressing long tailed problems have also found the following approaches useful.\n• If there is consistency across customers, serve customers with a global model built on pooled data with customization for large customers. This has worked really well in fraud detection but it is unclear whether it will work in AML given the problem is ‘local long tail’\n• Meta Models, Transfer Learning and Trunk Models: These are probably relevant only when using deep learning that is uncommon in AML\n• Building data sets in the long tail covering edge cases necessary for success of ML\n\n\nConclusion\nTo pave the way for the application of supervised ML to solve problems in AML, Financial Institutions need to be able to get value out of ML with the data available today while mitigating any model risk from doing so. The directive to collect large amounts of labelled BTL data before they effectively start using ML will not be well received by most FIs.\nML and Data science initiatives succeed only through an iterative process, vendors should build products that allows banks to builds useful ML models with data available today while creating a framework that will allow them to iterate quickly by collecting more labelled data and refining their models.\nGiven the data challenges in AML, financial institutions might also be reluctant to invest heavily upfront in ML knowing that really good models will be available only after several iterations. To help spread costs across lines of business and to see an earlier return on investment, financial institutions will increasingly look to address adjacent problems such as client insights, marketing or even fraud using the same ML tooling. Machine Learning vendors in the AML space will have to find a balance between creating targeted solutions for AML, yet making the underlying methodologies and algorithms generalizable to some of these adjacent problem spaces."
  },
  {
    "objectID": "blog/2022-01-15-when-success-breeds-failure-the-cautionary-tale-of-manchester-united.en/index.html",
    "href": "blog/2022-01-15-when-success-breeds-failure-the-cautionary-tale-of-manchester-united.en/index.html",
    "title": "When Success breeds Failure: The cautionary tale of Manchester United",
    "section": "",
    "text": "1986 -2013. That is how long Sir Alex Ferguson managed my beloved Manchester United. It was truly a dynasty. He built, tore-down and rebuilt arguably four different, successful teams over that period – and boy! The trophies rolled in.\n13 Premier League titles  5 FA Cups  2 European Cups\nIt was the best of times.\nIn the 9 years since his retirement, Manchester United’s record reads\n0 Premier League titles  1 FA Cup  0 European Cups.\nWhat explains this stunning fall from grace? An easy answer is the colossal ineptitude and incompetence of the Manchester United board and owners. That indeed is a good answer but failure is often the consequence of a series of poor decisions, unconscious biases and mistakes. I think a careful analysis will reveal more nuanced lessons that businesses of all sizes and teams at all levels can learn from.\nManchester United is not the only example of an organization that thrived under one leader and then stumbled under the stewardship of another. GE after Jack Welch, Exxon Mobile after Lee Raymond and Microsoft after Bill Gates are some other high-profile examples.\nYou will see that there are common themes across all these examples which suggest measures to prevent this from happening at our firms and teams."
  },
  {
    "objectID": "blog/2022-01-15-when-success-breeds-failure-the-cautionary-tale-of-manchester-united.en/index.html#perils-of-centralization",
    "href": "blog/2022-01-15-when-success-breeds-failure-the-cautionary-tale-of-manchester-united.en/index.html#perils-of-centralization",
    "title": "When Success breeds Failure: The cautionary tale of Manchester United",
    "section": "Perils of Centralization",
    "text": "Perils of Centralization\nWhat one-man giveth, one-man taketh away\nWhat unfolded at Manchester United over the last 35 years is a valuable lesson on the upside and pitfalls of centralization. When decision making authority is concentrated in the hands of a single individual, the institution can thrive provided the individual is highly competent.\nSir Alex reigned supreme at Manchester United for 26 years. His influence pervaded everything at Manchester United, from the boardroom to the boot room. The club thrived as long as one of the greatest managers ever was at the helm.\nHowever, just as an individual gets institutionalized and becomes dependent on an institution, the institution can become overly dependent on an individual. When this individual steps away, it is as if the institution’s central nervous system has been yanked away. The entire system works as long as this critical piece is in place, but the system now has a single point of failure. When Sir Alex stepped away, the system collapsed.\nExxon Mobile under Lee Raymond is another example of a company that thrived under centralized decision making but lost their mojo under new leadership.\nThe only way to overcome this overreliance on an individual is to create a strong, decentralized culture with empowered, autonomous teams. Netflix and Amazon are examples of companies that have successfully done this.\nIn his book “No Rules Rules”, Reed Hastings writes\n\nA loosely coupled organization should resemble a tree rather than a pyramid. The boss is at the roots, holding up the trunk of senior managers who support the outer branches where decision are made\n\nAt Amazon, software engineers are organized into autonomous “two-pizza” teams of no more than 10 people. These teams avoid dependencies and communication overheads by publishing well documented APIs1 that teams can use to consume each other’s products.\nTeams, not just organizations, can fall prey to centralization. Teams very often come to over rely on certain individuals for critical tasks. Google’s engineering team tracks the “bus-factor” for their teams – The number of people that need to get “hit by a bus”2 before the project is completely doomed.\nIt is essential that we track dependencies, encourage knowledge sharing and cross-training within our teams if they are to stay resilient over time."
  },
  {
    "objectID": "blog/2022-01-15-when-success-breeds-failure-the-cautionary-tale-of-manchester-united.en/index.html#obsolete-beliefs",
    "href": "blog/2022-01-15-when-success-breeds-failure-the-cautionary-tale-of-manchester-united.en/index.html#obsolete-beliefs",
    "title": "When Success breeds Failure: The cautionary tale of Manchester United",
    "section": "Obsolete Beliefs",
    "text": "Obsolete Beliefs\nThe world changes and Change is hard to predict. Sometimes change is slow and you realize it too late much like the proverbial frog being boiled to death.\nIt is human nature that our confidence in our beliefs rises if it leads to success. In a static world, it is rational to have monotonically increasing confidence in these beliefs3.\nManchester United had been successful when Sir Alex controlled all aspects of club management; from first team coaching to recruitment to the academy. The club believed the recipe for success was to find a good manager and vest him with authority to run all football operations.\nThe club failed to see that in the modern game these demands would be too onerous for a single person to handle and that other clubs had adopted a more decentralized approach to football management – with a sporting director handling football operations and a head coach focused on coaching the first team.\nAt Exxon4, in the 80’s and 90’s, Lee Raymond focused on profitability by selling off high-cost operations and focusing on low-cost assets and operations. This was appropriate when oil costs were low, but as the business environment changed and oil prices began to rise, Exxon continued to hold on to these obsolete beliefs. Consequently, they were too late to invest in new oil fields and assets that mattered for the subsequent 20 years.\nAt Microsoft, Steve Ballmer’s Windows and Office Centric view of the world were obsolete in a world where computing moved from desktops to smart phones. He continued to see non-Windows mobile platforms like IOS and Android as competitors much like he saw non-Windows desktop platforms as a competitor.\nIn order to escape this Obsolete Belief trap, organizations need a Re-founder5.Re-founders with a fresh perspective, a different set of beliefs and a willingness to let go of the past can realign and refocus the organization, reset its culture and allow it to adapt to a changing world.\nSatya Nadella is the epitome of this re-founder. He made a clean break from the past, transforming Microsoft into a collaboration focused, open source friendly, platform agnostic, cloud services provider6.\nWithin our teams, it is critical that we stay open minded and actively look for evidence to disconfirm our beliefs. We should not become wedded to processes and must constantly re-evaluate whether they serve the team’s goals."
  },
  {
    "objectID": "blog/2022-01-15-when-success-breeds-failure-the-cautionary-tale-of-manchester-united.en/index.html#misattribution",
    "href": "blog/2022-01-15-when-success-breeds-failure-the-cautionary-tale-of-manchester-united.en/index.html#misattribution",
    "title": "When Success breeds Failure: The cautionary tale of Manchester United",
    "section": "Misattribution",
    "text": "Misattribution\nOne of the consequences of success is that you seldom pause to reflect on why you succeeded. When you fail, you have good reason to analyze and identify the root cause of your failure. When you succeed, you attribute your success to everything you did.\nIf you are not careful, you can mistake correlation for causation and internalize the wrong lessons. This is what happened at Manchester United. The club seems to have internalized two lessons from the Ferguson years:\n\nSir Alex took six years to win a trophy so any manager should be given time to succeed.\nIt was the stability offered by having a single manager at the helm for so long that resulted in success.\n\nThe key lessons the club seems to have forgotten are\n\nSir Alex had proven himself in Scotland and in Europe before taking the reins at United. So, it is more important that the right manager be given the time and resources to succeed.\nStability does not breed success. Only progress breeds stability which in turn breeds success. Unless there are clear measurable signs of progress, there cannot be stability and hence no success.\n\nBy failing to identify the causative factors for its previous success, Manchester United could not reproduce its success.\nAs Charlie Munger recommends7\n● Carefully examine each past success, looking for accidental and non-causative factors associated with such success that will tend to mislead as one appraises odds implicit in a proposed new undertaking  ● Look for dangerous aspects of a new undertaking that were not present when past successes occurred.\nWithin our teams, we need to do detailed post-mortems even after successful projects, not just failed ones. It is said Failure is life’s greatest teacher but let us not turn a deaf ear to the lessons Success can teach us."
  },
  {
    "objectID": "blog/2022-01-15-when-success-breeds-failure-the-cautionary-tale-of-manchester-united.en/index.html#vanity-metrics",
    "href": "blog/2022-01-15-when-success-breeds-failure-the-cautionary-tale-of-manchester-united.en/index.html#vanity-metrics",
    "title": "When Success breeds Failure: The cautionary tale of Manchester United",
    "section": "Vanity Metrics",
    "text": "Vanity Metrics\nAs I asserted earlier, only progress breeds stability, which in turn breeds success. No sports team wins the championship and no business wins a market overnight – there has to be a period of steady progress and it is critical that we have actionable metrics to measure this progress and course correct as needed.\nManchester United fell into the trap of using the league position as a metric for progress. United finished 2nd in 2018 and then 3rd and 2nd in 2020 and 2021 respectively. This metric on its own, devoid of context is highly misleading – a vanity metric.\n\nVanity metrics are metrics that make you look good to others but do not help you understand your own performance in a way that informs future strategies8\n\nFor instance, in the three seasons below, placing second means very different things. The points differential with the first placed team says more about the performance and potential of the team than the position in the table.\n\n\n\nFigure 1: Man Utd finished 12 points behind the champions in 2021, currently United sit in 4th place 19 points behind the leaders\n\n\n\n\n\nFigure 2: Liverpool finished 1 point behind the champions in 2019, they won the title the next year\n\n\n\n\n\nFigure 3: Man Utd finished 19 points behind the champions in 2018, they finished 6th 32 points behind the champions in 2019.\n\n\nTo measure progress, Manchester United needed to track metrics that accurately capture real progress. Pairing league position with a suitable counter metric such as points differential or expected goal differential would have been a start.\nCompanies and teams should carefully design and track actionable input metrics, output metrics and outcome metrics9.They should also select a north star metric (one specific outcome metric) which the whole company is aligned towards.\n\nHowever, most outcome metric such as revenue tend to be a lagging measure of the performance of multiple teams – marketing, sales, pre-sales and product. It does not provide timely or actionable feedback on the performance of a specific team. To identify meaningful metrics, we need to identify the factors (inputs and outputs) that drive the desired north star metric – be it revenue, growth or market share- and then track those closely.\nFor example, the Amazon flywheel below clearly identifies the factors that impact growth. Teams at Amazon rigorously track metrics that capture costs, prices, selection, customer experience etc. These are the metrics that they strive to improve.\n\nThe book Working Backwards goes into detail on how this process works at Amazon."
  },
  {
    "objectID": "blog/2022-01-15-when-success-breeds-failure-the-cautionary-tale-of-manchester-united.en/index.html#circle-of-competence",
    "href": "blog/2022-01-15-when-success-breeds-failure-the-cautionary-tale-of-manchester-united.en/index.html#circle-of-competence",
    "title": "When Success breeds Failure: The cautionary tale of Manchester United",
    "section": "Circle of Competence",
    "text": "Circle of Competence\nWarren Buffet and Charlie Munger famously ascribe to a Circle of Competence when making investing decisions.\n\n“If we have a strength, it is in recognizing when we are operating within our circle of competence and when we are approaching the perimeter” – Warren Buffet\n\n\nAt Manchester United, the executives and leaders at the club did not recognize their circle of competence.\nSir Alex excelled at recruiting coaches and players, but did not recognize that hiring a new manager to run the team is outside his circle of competence. Instead of following a rigorous process to identify the next manager, he simply anointed David Moyes as the manager with predictable consequences.\nEd Woodward, the former head of commercial operations at the club when promoted to CEO, refused to accept that running football operations was outside his circle of competence. The consequence was a series of bad decisions that set the club back by years.\nEven in the corporate world, some companies have paid dearly for not recognizing their circle of competence.\nGeneral Electric was the foremost engineering company in the world in the 90’s. It has expertise in technology and engineering going back a hundred years to the 19th century – that was its circle of competence. However, in the 90’s it decided to turn itself into a major player in finance as it looked to boost returns to shareholders. Venturing outside its circle of competence proved very costly to GE as the financial crisis in 2008 nearly crippled the company10. Since then it has sold if its financial assets and has strived to return to its engineering roots.\nAs employees, managers and leaders we should recognize our circle of competence and listen to those closest to the problem. When transitioning between functions – say marketing to sales, we need to understand that being good at one doesn’t automatically make you good at the other. Without a beginner’s mindset, overconfidence can lead you to venture outside your circle of competence.\nAs Adam Grant writes in his book Think Again\n\nKnowledge is power, knowing what we don’t know is wisdom."
  },
  {
    "objectID": "blog/2022-01-15-when-success-breeds-failure-the-cautionary-tale-of-manchester-united.en/index.html#culture-mission-and-vision",
    "href": "blog/2022-01-15-when-success-breeds-failure-the-cautionary-tale-of-manchester-united.en/index.html#culture-mission-and-vision",
    "title": "When Success breeds Failure: The cautionary tale of Manchester United",
    "section": "Culture, Mission and Vision",
    "text": "Culture, Mission and Vision\nAll successful organizations have a clarity of vision – a clear statement of the future they are trying to create and a clear mission - how they will bring this vision into reality.\nE.g., Amazon’s vision is “to be Earth’s most customer centric company, where customers can find and discover anything they might want to buy online”.  Its mission is “to offer our customers the lowest possible prices, the best available selection, and the utmost convenience”\nSuccessful companies also have a strong culture – a shared consciousness and a set of values that guides the organization in decision making, recruitment, goal setting and prioritization.\nManchester United had no clear vision or mission at least when it came to matters on the pitch. It had no well-defined culture or set of values beyond playing “attacking football” and being “patient” with managers. Ex-players and fans talk about a “United Way” but could not concretely describe what it meant and what its core principles are.\nWithout this guiding vision, the club had no long-term planning, adopted a scatter gun approach to recruiting managers and players and had no systematic way of defining metrics that tracked progress towards the right goals. The results of the club on the pitch consequently suffered.\nGetting people to buy into the company’s vision and mission and creating a strong culture to support is perhaps the highest leverage activity a leader or a founder can do.\nLeaders need to create a drumbeat11 of mission, vision and culture, live the same and inspire the rest of the company to march in unison to that drumbeat. Leaders have to communicate this clearly and repeatedly even when they are sick of doing so, before their teams internalize it. They have to clearly describe how the decisions they made were rooted in the company’s values so that employees can see it in action.\nNetflix’s culture doc , Amazon’s leadership principles and Coinbase’s culture are examples of a clear, well-defined culture. It leaves no doubt as to what these companies value, encourage and reward.\nHowever, it is not just enough to state and communicate cultural values. You need to create mechanisms and processes to nurture this culture and ensure it manifests in the employee’s actions and decision making. Netflix’s “Farming for Dissent” and Amazon’s “Working Backwards” are great examples of these mechanisms.\nIf leadership only pays lip service to their culture and values in town halls and on the walls of their offices, without creating the mechanisms to foster that culture, it will be a futile exercise.\nEven if you are leading a team of five rather than a company of five thousand, clearly defining the vision and mission of your team and explaining how it aligns with the vision and strategy of the company can greatly empower your team."
  },
  {
    "objectID": "blog/2022-01-15-when-success-breeds-failure-the-cautionary-tale-of-manchester-united.en/index.html#recruitment-and-talent-development",
    "href": "blog/2022-01-15-when-success-breeds-failure-the-cautionary-tale-of-manchester-united.en/index.html#recruitment-and-talent-development",
    "title": "When Success breeds Failure: The cautionary tale of Manchester United",
    "section": "Recruitment and Talent Development",
    "text": "Recruitment and Talent Development\nIn professional sports and the knowledge industry, talent is king. People are your greatest asset. Only recruiting and retaining the best allows you to be successful.\nManchester United’s owners cannot be accused of penny pinching when it comes to recruiting or compensation. They club has spent more on transfers12 over the last 10 years and has a higher wage bill13 than any other club in England. They have signed proven world class players and paid them astronomical wages. Yet, the return on that investment has been terrible.\nLiverpool on the other hand have been far more economical in the transfer market, they are not even in the top 10 for net transfer spend yet have built a team that are capable of challenging for the highest prizes. Man City have spent heavily but also have the trophies to show for it.\nThe key difference is Liverpool and Man City have a clear vision of how they want to play and the system they want to play. They only buy players that fit the system rather than buy players on reputation or potential and try to improvise a system to accommodate these players.\nSignificantly, very few players that Manchester United have signed over the last 10 years have gone on to become consistent performers on the pitch. Further underperforming players have been rewarded with new contracts instead of being let go. This points to deficiencies in identifying and evaluating talent and an inability to make hard decisions to correct hiring mistakes.\nFurther, several promising players joined Manchester United who failed to live up to their potential. Their performances in fact regressed considerably. On the contrary, Liverpool and to a lesser extent Man City have signed players who appeared underwhelming on paper but have developed into top class players. This points to an inability to coach and develop talent.\nThere are several takeaways here.\nFirst, simply hiring the best people you find does not work, you need to hire people that complement your current team and fully buy into your company’s vision and can adapt to its culture.\nSecond, you need to have a rigorous, thorough process to evaluate talent and a culture that holds people accountable for their performance. Under performing talent has to be let go.\nThird, you need to be able to groom, develop and coach the talent you recruit so that they fulfill their potential. If you can do this you can build strong teams even without paying over the top for talent with the glitziest credentials.\nI don’t believe great recruitment is possible without a clarity of vision and mission and a strong culture.\nOnly a compelling vision, and mission will allow you to recruit and retain missionaries who believe in the company’s mission. Hiring missionaries rather than mercenaries is a key tenet of Amazon’s recruitment policy. Amazon caps their base salary at levels below their competitors as they believe working at the company should be about more than just money. The fact they have succeeded is down to them having an inspiring mission, vision and culture that can attract and retain such talent.\nInformal, adhoc interviewing is prone to biases and is heavily dependent on interviewers making the right gut call. Only a rigorous, scalable and repeatable hiring process can ensure that a company consistently hires the right talent. Amazon has carefully crafted a “Bar Raiser” process to do just this.\nFrom empowering “bar raisers” who can veto a hiring manager’s decision, to designing behavioral interviews designed to evaluate potential hires on leadership principles, to training interviewers on how to run interviews and debrief sessions afterwards – the bar raiser process allows Amazon to maintain standards even as it grows fast.\n\n“A players attract A players. B players attract C players.” – Steve Jobs\n\nWithout a system to identify high quality talent, the talent density at the company will go down and standards will slip.\nNetflix has created a performance-oriented culture where “adequate performance gets a generous severance package”. Netflix asks managers to run a Keeper’s test for their reports. In the book “No Rules Rules” Reed Hasting writes.\n\n“If a person on your team to quit tomorrow, would you try to change their mind? Or would you accept their resignation, perhaps with a little relief? If the latter, you should give them a severance package now, and look for a star, someone you would fight to keep”.\n\nThis might sound harsh, but the key is you need an error correcting mechanism that can correct recruiting errors. Only a system that can identify and fix errors can improve.\nFinally, companies need to incentivize managers to coach and develop employees.\n\n“Coaching is no longer a specialty; you cannot be a good manager without being a good coach” - Bill Campbell\n\nIt is unfortunate that most companies do not evaluate managers along this dimension or incentivize them to take coaching seriously. Organization that requires this from every manager will see higher performing individuals and teams and are far more likely to thrive."
  },
  {
    "objectID": "blog/2022-01-15-when-success-breeds-failure-the-cautionary-tale-of-manchester-united.en/index.html#conclusion",
    "href": "blog/2022-01-15-when-success-breeds-failure-the-cautionary-tale-of-manchester-united.en/index.html#conclusion",
    "title": "When Success breeds Failure: The cautionary tale of Manchester United",
    "section": "Conclusion",
    "text": "Conclusion\nCreating institutions that outlast its visionary leaders and continue to thrive and grow in a rapidly changing environment is extremely hard.\nIt requires creating autonomous, decentralized teams, being open minded to changing our beliefs, deeply analyzing our failures and successes, being wise to admit what we don’t know and ask for help, creating an inspiring mission and vision that people believe in, nurturing an adaptable culture that allows people to thrive and grow, recruiting, coaching and if needs be, letting go of people.\nAmazon prides itself on being a Day 1 company. When asked what Day 2 looks like, Jeff Bezos replied\n\nDay 2 is stasis. Followed by irrelevance. Followed by excruciating, painful decline. Followed by death. And that is why it is always Day 1\n\nIn 2013, when Sir Alex stepped down, Manchester United became a Day 2 company. Learning the right lessons from its decline can help us avoid a similar fate.\n\nAcknowledgements\nThanks to Neeraja Suresh Kumar and Aparna Gautham for comments and feedback."
  },
  {
    "objectID": "blog/2022-01-15-when-success-breeds-failure-the-cautionary-tale-of-manchester-united.en/index.html#footnotes",
    "href": "blog/2022-01-15-when-success-breeds-failure-the-cautionary-tale-of-manchester-united.en/index.html#footnotes",
    "title": "When Success breeds Failure: The cautionary tale of Manchester United",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nApplication Programming Interface↩︎\nThis is just a metaphor for someone leaving your team for any reason↩︎\nhttp://paulgraham.com/ecw.html↩︎\nhttps://www.joincolossus.com/episodes/74059914/murti-exxon-mobil-an-aging-energy-empire?tab=blocks↩︎\nhttps://mastersofscale.com/nadella/↩︎\nhttps://stratechery.com/2018/the-end-of-windows/↩︎\nPoor Charlie’s Almanack↩︎\nhttps://www.tableau.com/learn/articles/vanity-metrics↩︎\nhttps://mixpanel.com/blog/shreyas-doshi-product-metrics/↩︎\nhttps://www.investopedia.com/insights/rise-and-fall-ge/↩︎\nhttps://mastersofscale.com/jeff-weiner-how-to-set-the-drumbeat/↩︎\nhttps://www.footballtransfers.com/en/transfer-news/uk-premier-league/2021/09/transfer-net-spend-man-utd-clubs-who-have-spent-most-10-years↩︎\nhttps://www.spotrac.com/epl/payroll/↩︎"
  },
  {
    "objectID": "blog/2020-05-30-model-uncertainity/index.html",
    "href": "blog/2020-05-30-model-uncertainity/index.html",
    "title": "Model Uncertainity",
    "section": "",
    "text": "This post demonstrates how to incorporate uncertainty into model predictions, describes how this can surface limitations in the data used and how this can provide guidance on when the model can be used with confidence.It also outlines some use cases where incorporating model uncertainty can be crucial for making good decisions."
  },
  {
    "objectID": "blog/2020-05-30-model-uncertainity/index.html#introduction",
    "href": "blog/2020-05-30-model-uncertainity/index.html#introduction",
    "title": "Model Uncertainity",
    "section": "",
    "text": "This post demonstrates how to incorporate uncertainty into model predictions, describes how this can surface limitations in the data used and how this can provide guidance on when the model can be used with confidence.It also outlines some use cases where incorporating model uncertainty can be crucial for making good decisions."
  },
  {
    "objectID": "blog/2020-05-30-model-uncertainity/index.html#data",
    "href": "blog/2020-05-30-model-uncertainity/index.html#data",
    "title": "Model Uncertainity",
    "section": "Data",
    "text": "Data\nI will use the iris dataset with the goal of classifying each record as either virginica or not-virginica. Only two variables - Sepal Length and Petal Width will be used.\nattach(iris)\niris$label = as.factor(ifelse(iris$Species=='virginica',1,0))\niris2 &lt;- iris[,c('Sepal.Length','Petal.Width','label')]\nhead(iris2)\n##   Sepal.Length Petal.Width label\n## 1          5.1         0.2     0\n## 2          4.9         0.2     0\n## 3          4.7         0.2     0\n## 4          4.6         0.2     0\n## 5          5.0         0.2     0\n## 6          5.4         0.4     0\nA plot of the data is shown below.\nplot(iris2[,c(1,2)],col=iris$label,pch = 16)\nlegend('bottomright',legend=c('virginica','not virginica'),\n       col=c('red','black'),pch =c(16,16),bty=\"n\")"
  },
  {
    "objectID": "blog/2020-05-30-model-uncertainity/index.html#model",
    "href": "blog/2020-05-30-model-uncertainity/index.html#model",
    "title": "Model Uncertainity",
    "section": "Model",
    "text": "Model\nA simple logistic regression model is fit to the data.\nglm1 &lt;- glm(label ~ ., data = iris2,family = 'binomial')\nsummary(glm1)\n## \n## Call:\n## glm(formula = label ~ ., family = \"binomial\", data = iris2)\n## \n## Deviance Residuals: \n##      Min        1Q    Median        3Q       Max  \n## -2.08560  -0.08211  -0.00012   0.01869   2.47786  \n## \n## Coefficients:\n##              Estimate Std. Error z value Pr(&gt;|z|)    \n## (Intercept)  -22.8736     6.8160  -3.356 0.000791 ***\n## Sepal.Length   0.3064     0.8375   0.366 0.714530    \n## Petal.Width   12.8447     2.8731   4.471  7.8e-06 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## (Dispersion parameter for binomial family taken to be 1)\n## \n##     Null deviance: 190.954  on 149  degrees of freedom\n## Residual deviance:  33.287  on 147  degrees of freedom\n## AIC: 39.287\n## \n## Number of Fisher Scoring iterations: 9\nThe resulting decision boundary is also plotted below. We are interested in learning how the uncertainty in prediction varies between the five points A, B, C ,D and E.\n#Grid for getting predictions\ngrid &lt;- expand.grid(Sepal.Length =seq(0,8,by=0.1),Petal.Width = seq(0,8,by=0.1))\ngrid_preds &lt;- predict(glm1,newdata=grid,type='response')\ncontour(x=seq(0,8,by=0.1), y=seq(0,8,by=0.1), z=matrix(grid_preds,nrow =81),levels=0.5,\n        col=\"cornflowerblue\",lwd=2,drawlabels=FALSE)\npoints(iris2[,1],iris2[,2],col=iris$label,pch = 16)\npoints(x = c(7,5,2,2,7), y = c(2,2,2,7,7),pch=4,cex =2)\ntext(x = c(7.25,5.25,2.25,2.25,7.25), y = c(2.25,2.25,2.25,7.25,7.25),\n     labels=c('A','B','C','D','E'))"
  },
  {
    "objectID": "blog/2020-05-30-model-uncertainity/index.html#frequentist-prediction-intervals",
    "href": "blog/2020-05-30-model-uncertainity/index.html#frequentist-prediction-intervals",
    "title": "Model Uncertainity",
    "section": "Frequentist Prediction Intervals",
    "text": "Frequentist Prediction Intervals\nWe create prediction intervals using the approach described here\nnewdata &lt;- data.frame(Sepal.Length = c(7,5,2,2,7), Petal.Width = c(2,2,2,7,7))\npreds &lt;-  predict(glm1,newdata=newdata,type='link',se.fit=TRUE)\n\n#Calculate 95% prediction interval bounds\ncritval &lt;- 1.96 ## approx 95% CI\nupr &lt;- preds$fit + (critval * preds$se.fit)\nlwr &lt;- preds$fit - (critval * preds$se.fit)\nfit &lt;- preds$fit\n\n\n#These are logits, use inverse link function to get probabilties\nfit2 &lt;- glm1$family$linkinv(fit)\nupr2 &lt;- glm1$family$linkinv(upr)\nlwr2 &lt;- glm1$family$linkinv(lwr)\n\nresults &lt;- data.frame(label=c('A','B','C','D','E'),mean = fit2,lb = lwr2,ub =upr2)\nprint(results)\n##   label      mean         lb        ub\n## 1     A 0.9930371 0.91128970 0.9994952\n## 2     B 0.9872256 0.75751893 0.9994772\n## 3     C 0.9685803 0.01729603 0.9999815\n## 4     D 1.0000000 1.00000000 1.0000000\n## 5     E 1.0000000 1.00000000 1.0000000\nAll 5 points are predicted to be virginica with a very high probability given they fall on one side of the decision boundary. However the width of the prediction intervals for these five points vary considerably. The points D and E are so far away from the decision boundary that there is little uncertainty in the predictions, the points A,B and C are more interesting.\nlibrary(ggplot2)\nggplot() + geom_errorbar(data=results,mapping=aes(x=label,ymin=lb,ymax=ub),\n                         width =0.2,size=1,color='blue')+\n  geom_point(data=results,mapping = aes(x=label,y=mean),size=2,shape=21,fill='white')\n\nFor point A, the model indicates fairly high confidence in its predictions, however for point C, the prediction interval indicates that model has very little confidence in the prediction although the point estimate is a high 0.96.\nIf the threshold for making a decision is 0.8, for a decision point corresponding to point C, a decision in the affirmative will be made even though the model has very little confidence in it’s prediction.\nHowever, it is important to keep in mind that a frequentist 95% prediction interval only means that in the context of repeated trials, 95% of such prediction intervals will contain the true predicted value.Given this interpretation of the prediction interval, there are two ways this can be used.\n\nProvide the prediction interval for each prediction for downstream analysis and review\nGet the width of the prediction intervals for a grid of points and specify this is the optimal operating region for the model.\n\nBelow we identify the region where the width of the prediction interval is greater than 0.25\ngrid &lt;- expand.grid(Sepal.Length =seq(0,8,by=0.1),Petal.Width = seq(0,8,by=0.1))\npreds &lt;-  predict(glm1,newdata=grid,type='link',se.fit=TRUE)\n\n#Calculate 95% prediction interval bounds\ncritval &lt;- 1.96 ## approx 95% CI\nupr &lt;- preds$fit + (critval * preds$se.fit)\nlwr &lt;- preds$fit - (critval * preds$se.fit)\nfit &lt;- preds$fit\n\n\n#These are logits, use inverse link function to get probabilties\nfit2 &lt;- glm1$family$linkinv(fit)\nupr2 &lt;- glm1$family$linkinv(upr)\nlwr2 &lt;- glm1$family$linkinv(lwr)\n\nci_width &lt;- upr2 - lwr2\n\noperating_region &lt;- cbind(grid,ci_width)\noperating_region$ci_flag &lt;- ifelse(operating_region$ci_width &lt; 0.25,1,0)\ncontour(x=seq(0,8,by=0.1), y=seq(0,8,by=0.1), z=matrix(operating_region$ci_flag,\n                                                       nrow =81),levels=1,\n        col=\"green\",lwd=2,drawlabels=TRUE)\npoints(iris2[,1],iris2[,2],col=iris$label,pch = 16)\n\nThe region between the two green lines above represents the region where the model’s prediction does not have the required degree of confidence, this region represents the region where the model’s predictions should not be used for making decisions."
  },
  {
    "objectID": "blog/2020-05-30-model-uncertainity/index.html#bayesian-prediction-intervals",
    "href": "blog/2020-05-30-model-uncertainity/index.html#bayesian-prediction-intervals",
    "title": "Model Uncertainity",
    "section": "Bayesian Prediction Intervals",
    "text": "Bayesian Prediction Intervals\nAn alternative way to incorporate uncertainty is by using bayesian models.Bayesian prediction intervals have a probabilistic interpretation. If the 95% bayesian prediction interval for a new data point is between 0.75 and 0.85, it is justified in concluding that there there is a 95% probability that the true value is between 0.75 and 0.85 according to the model. This means instead of defining a score threshold for the model based on point estimates, we can define a score threshold using the prediction interval itself.\nWe can define the score threshold such that the entire 95% prediction interval should be greater that score threshold.\nA bayesian logistic regression model is specified and fitted below.\nlibrary(rstan)\noptions(mc.cores = parallel::detectCores())\nrstan_options(auto_write = TRUE)\nlibrary(rstanarm)\n\nbayes_glm1 &lt;- stan_glm(label ~ . , data = iris2,\n                       family = binomial(link = \"logit\"),\n                       prior = normal(0,25) , prior_intercept = normal(0,25),QR=TRUE)\nThe 95% credible intervals on the model parameters are given by\nposterior_interval(bayes_glm1,prob=0.95)\n##                    2.5%      97.5%\n## (Intercept)  -39.151105 -12.370249\n## Sepal.Length  -1.390665   2.049905\n## Petal.Width    8.636345  20.015803\nModel predictions on new data points are computed below\nnewdata &lt;- data.frame(Sepal.Length = c(7,5,2,2,7), Petal.Width = c(2,2,2,7,7))\nnew_pred_means &lt;- posterior_linpred(bayes_glm1,transform=TRUE,newdata=newdata)\n## Instead of posterior_linpred(..., transform=TRUE) please call posterior_epred(), which provides equivalent functionality.\n###Get 95 %  credible intervals intervals\np_2.5 &lt;- apply(new_pred_means,2,function(x){ quantile(x,0.025)})\np_97.5 &lt;- apply(new_pred_means,2,function(x){ quantile(x,0.975)})\nmedian &lt;- apply(new_pred_means,2,function(x){ quantile(x,0.5)})\n\nresults &lt;- data.frame(label=c('A','B','C','D','E'),median = median ,lb = p_2.5,ub=p_97.5)\nprint(results)\n##   label    median         lb        ub\n## 1     A 0.9946407 0.95395165 0.9997456\n## 2     B 0.9902417 0.83807582 0.9997677\n## 3     C 0.9751805 0.01987324 0.9999941\n## 4     D 1.0000000 1.00000000 1.0000000\n## 5     E 1.0000000 1.00000000 1.0000000\nThe bayesian prediction/credible intervals for the five points are shown below\nlibrary(ggplot2)\nggplot() + geom_errorbar(data=results,mapping=aes(x=label,ymin=lb,ymax=ub),\n                         width =0.2,size=1,color='blue')+\n  geom_point(data=results,mapping = aes(x=label,y=median),size=2,shape=21,fill='white')\n\nUsing the posterior predictive distribution, the probability mass of the distribution exceeding some threshold can also be computed as shown below. Assume the threshold is 0.8\ncalc_prob &lt;- function(v,t){\n  #Get no of prediction exceeding threshold\n  e &lt;- sum(v&gt;t);\n  return(e/length(v))\n}\n\nprobs &lt;- apply(new_pred_means,2,calc_prob,0.8)\nnames(probs) &lt;- LETTERS[1:5]\nprint(probs)\n##       A       B       C       D       E \n## 1.00000 0.98200 0.72275 1.00000 1.00000\nThis can allow us to set thresholds like,the posterior predictive distribution should have a probability mass of 0.5 exceeding the chosen threshold."
  },
  {
    "objectID": "blog/2020-05-30-model-uncertainity/index.html#bayesian-models-for-imbalanced-datasets",
    "href": "blog/2020-05-30-model-uncertainity/index.html#bayesian-models-for-imbalanced-datasets",
    "title": "Model Uncertainity",
    "section": "Bayesian Models for Imbalanced Datasets",
    "text": "Bayesian Models for Imbalanced Datasets\nBayesian models can also be useful in rare event modelling by incorporating uncertainty resulting from the fact that a certain class is very rare into model estimates and predictions.\nConsider the model built on the original data below which has a higher proportion of ‘negative’.\nlibrary(RKEEL)\nimb_data &lt;- read.keel(file = 'page-blocks0.dat')\nimb_data[,c(1:10)] &lt;- as.data.frame(lapply(imb_data[,c(1:10)],as.numeric))\nimb_data &lt;- na.omit(imb_data)\nsummary(imb_data$Class)\n## negative positive \n##     4912      559\nA bayesian logistic regression model is built on this imbalanced data set. Only 2 features are used for simplicity.\nlibrary(rstanarm)\nlibrary(bayesplot)\n\nbayes_glm2 &lt;- stan_glm(Class ~ Height + P_black , data = imb_data,\n                       family = binomial(link = \"logit\"),QR=TRUE)\nThe 95% credible interval of the model coefficients are given below\nposterior_interval(bayes_glm2,prob=0.95)\n##                    2.5%       97.5%\n## (Intercept) -5.61684669 -5.05640186\n## Height       0.03593999  0.05116209\n## P_black      5.57297983  6.49314443\nThis is visualized below\nposterior_ub &lt;- as.array(bayes_glm2)\ncolor_scheme_set(\"red\")\nmcmc_intervals(posterior_ub,prob = 0.95,pars = c('Height','P_black'))\n\nNow consider rebalanced dataset - using oversampling.\nlibrary(ROSE)\nbal_data_over &lt;- ovun.sample(Class~Height + P_black, data = imb_data,\n                        p = 0.5 , seed = 1 , method = 'over')$data\n\n\nsummary(bal_data_over$Class)\n## negative positive \n##     4912     4875\nThe same model is now fit to this rebalanced data.\nbayes_glm3 &lt;- stan_glm(Class ~ Height + P_black , data = bal_data_over,\n                       family = binomial(link = \"logit\"),QR=TRUE)\nThe 95% credible interval of the model coefficients are given below\nposterior_interval(bayes_glm3,prob=0.95)\n##                   2.5%       97.5%\n## (Intercept) -2.9301851 -2.67279134\n## Height       0.0400952  0.04899063\n## P_black      4.8666753  5.33718942\nThis is visualized below\nposterior_ub &lt;- as.array(bayes_glm3)\ncolor_scheme_set(\"red\")\nmcmc_intervals(posterior_ub,prob = 0.95,pars = c('Height','P_black'))\n\nAlso consider a dataset rebalanced by undersampling.\nlibrary(ROSE)\nbal_data_under &lt;- ovun.sample(Class~Height + P_black, data = imb_data,\n                        p = 0.5 , seed = 1 , method = 'under')$data\n\n\nsummary(bal_data_under$Class)\n## negative positive \n##      539      559\nThe same model is now fit to this rebalanced data.\nbayes_glm4 &lt;- stan_glm(Class ~ Height + P_black , data = bal_data_under,\n                       family = binomial(link = \"logit\"),QR=TRUE)\nThe 95% credible interval of the model coefficients are given below\nposterior_interval(bayes_glm4,prob=0.95)\n##                    2.5%       97.5%\n## (Intercept) -3.49022006 -2.63906206\n## Height       0.04146041  0.07188549\n## P_black      4.89423139  6.48174478\nThis is visualized below\nposterior_bal &lt;- as.array(bayes_glm4)\ncolor_scheme_set(\"red\")\nmcmc_intervals(posterior_bal,prob = 0.95,pars = c('Height','P_black'))\n\nIt can be seen from the figure above that the width of the credible interval has has actually expanded when using this dataset that has been re-balanced by undersampling.\nNow consider the prediction on a new data point from each of these three models.\nnewdata &lt;- data.frame(Height = c(110), P_black = c(750))\npred_imb &lt;- posterior_linpred(bayes_glm2,transform=TRUE,newdata=newdata)\n## Instead of posterior_linpred(..., transform=TRUE) please call posterior_epred(), which provides equivalent functionality.\npred_over &lt;- posterior_linpred(bayes_glm3,transform=TRUE,newdata=newdata)\n## Instead of posterior_linpred(..., transform=TRUE) please call posterior_epred(), which provides equivalent functionality.\npred_under &lt;- posterior_linpred(bayes_glm4,transform=TRUE,newdata=newdata)\n## Instead of posterior_linpred(..., transform=TRUE) please call posterior_epred(), which provides equivalent functionality.\npreds_all &lt;- cbind(pred_imb,pred_over,pred_under)\n\n###Get 95 %  credible intervals intervals\np_2.5 &lt;- apply(preds_all,2,function(x){ quantile(x,0.025)})\np_97.5 &lt;- apply(preds_all,2,function(x){ quantile(x,0.975)})\nmedian &lt;- apply(preds_all,2,function(x){ quantile(x,0.5)})\n\npred_intervals &lt;- data.frame(cbind(p_2.5,p_97.5,median))\npred_intervals$data &lt;- c('imbalanced','over sampled','under sampled')\n\nggplot() + geom_errorbar(data=pred_intervals,mapping=aes(x=data,ymin=p_2.5,ymax=p_97.5),\n                         width =0.2,size=1,color='blue')+\n  geom_point(data=pred_intervals,mapping=aes(x=data,y=median),size=2,\n             shape=21,fill='white')\n\nAs seen above the width of the prediction interval resulting from using training data with different volumes of the target class is different. The main takeaway is that while building models on rare event data, the fact that the rare incidence of a class leads to greater prediction uncertainty needs to be considered."
  },
  {
    "objectID": "blog/2020-05-30-model-uncertainity/index.html#use-cases",
    "href": "blog/2020-05-30-model-uncertainity/index.html#use-cases",
    "title": "Model Uncertainity",
    "section": "Use Cases",
    "text": "Use Cases\n1) Making a binary(yes/no) decision by comparing a score against a threshold.\nAlmost all model predictions and subsequent actions are typically taken when a model’s prediction exceeds a threshold.\nConsider the ‘ML scenario’ use case, here the score generated by the model will be compared against a threshold. An alert is generated only if the score exceeds a threshold.\nAssume the optimal score threshold for the model is 0.6.\nConsider an ML scenario event where the predicted score is (0.58), assume this is the mode of the posterior predictive distribution. Two completely different distributions can yield the same point prediction as shown below.\n\nNeither of these ML events will lead to alerts but the first event should be escalated to an alert given the much larger uncertainty associated with the prediction and the high cost of false negatives. The probability mass exceeding the threshold of 0.6 for event 1 is ~ 0.33 whereas this probability mass is 0 for event 2.\n2) Rank Ordering Events\nIn many applications the model’s predictions are used to rank items. In fraud, analysts have only a limited time and budget to review fraud cases, so the cases are reviewed from the top-down until the available budget is exhausted.\nAlso, consider the event scoring use case in AML. The model is used to rank order events with the higher ranked events being worked first. Here there is no risk of incurring false negatives as all events will be eventually worked in the assigned order. So here the priority is to reduce the risk of false positives i.e. minimize the possibility of working an unproductive alert first.\nConsider two events with a predicted score of 0.6 (Event 1) and 0.58 (Event 2) respectively but with the following posterior predictive distributions.\n\nLet the baseline be the proportion of ‘effective events’ in your data set , assume it is 0.5 here. In this case we would want to review ‘event 2’ first because it has a predicted probability score greater than the baseline with much higher certainty. A couple of metrics that can potentially be used to prioritize events\nM1 : (Probability Score - baseline)/S.D. of distribution M2 : Probability mass exceeding your baseline (0.5 here)\nfirst metric (M1), the scores for the two events will be:\nE1: 2.07\nE2: 16.17\nUsing the second metric (M2) the score will be:\nE1: 0.9784\nE2: 1\nIn both cases , event E2 will be ranked over event E1.\n3) Higher Risk Tolerance\nThere may be certain models that are used to detect events/activity that are considered low risk to the bank or might be considered so by institutions with a higher risk tolerance. In such cases, an institution might want to escalate an event/activity only if the model has high confidence in its predictions.\nSuch institutions will chose to escalate an event only if the 95% credible interval of the posterior predictive distribution entirely exceeds the chosen threshold.\nThis may also be an option for models which are observed to exhibit an unacceptably high False Positive Ratio."
  },
  {
    "objectID": "blog/2020-04-04-a-citizen-s-guide-to-the-economics-of-a-city/index.html",
    "href": "blog/2020-04-04-a-citizen-s-guide-to-the-economics-of-a-city/index.html",
    "title": "A Citizen’s Guide to the Economics of A City",
    "section": "",
    "text": "The world is rapidly urbanizing. More than half the world lives in urban areas. By 2050, it is projected that close to 7 billion people will live in urban areas in 2050. If you are reading this, you are almost certainly living in a city.\nThis blog post is in essence a selective synopsis of the fantastic book - Order Without Design — How Markets Shape Cities’ by Alain Bertaud. The book is largely targeted at urban planners and is probably not going to be a NY Times best seller. However, it lucidly describes several ideas that we residents of cities need to keep in mind.\nWhat recent events have taught is that only an enlightened citizenry can truly safeguard a democracy. A citizenry who is able to critically evaluate proposals and policies of politicians and exercise their franchise accordingly will be rewarded with a responsible government. I hope that this article sheds some light on how cities evolve and what kind of policies support and stymie that evolution; the discerning voter will hopefully find this helpful as you vote for your local councilor or mayor. This is not a substitute for reading the book, so I would still encourage you to pick up a copy and give it a read."
  },
  {
    "objectID": "blog/2020-04-04-a-citizen-s-guide-to-the-economics-of-a-city/index.html#cities-as-labor-markets",
    "href": "blog/2020-04-04-a-citizen-s-guide-to-the-economics-of-a-city/index.html#cities-as-labor-markets",
    "title": "A Citizen’s Guide to the Economics of A City",
    "section": "Cities as Labor Markets",
    "text": "Cities as Labor Markets\nCities are essentially labor markets. It is a geographical area where people live in so that they can meet the demand for labor from the government or private enterprises operating in the area. Without a functioning labor market and jobs, no one would have a reason to move to a city. In developing countries, there is a massive influx of people from rural areas to cities because cities offer better job prospects and higher incomes.\nBesides a well-functioning labor markets that often depends on more national level policies, a well-functioning city should offer the following:\n\nA reasonably short commute to your work place\nAffordable living spaces with easy access to nature or social life\n\nWhile the first point refers to mobility in a city that is not discussed in this post, the second refers to affordability which is discussed at greater length.\nIt has been well understood that cities are centers of innovation due to their spatial concentration and that larger cities are more innovative than smaller cities. They benefit from spillover effects where innovations rapidly diffuse across organizations due to flexible labor markets. Consider Silicon Valley where these spillover effects have created a vibrant entrepreneurial hub.\nHowever, a city’s ability to benefit from its scale depends on the two conditions listed above. A city that is not able to offer these will suffer diminishing returns as its size increases.\nThe speed and efficiency of transportation i.e. mobility, circumscribes the labor market that is accessible to a citizen. Typically, the labor market is defined as the average number of jobs accessible to an individual within a 1-hour commute. With modern and faster modes of transport, the labor market and hence the size of a city increases because the jobs accessible within an hour increase.\nIf a city’s transportation infrastructure is too slow or inefficient that you cannot travel from point A to point B within an hour or so, it causes the labor market to fragment. This means an employer cannot hire the best talent from across the city but is restricted to hiring from a smaller fraction of the city’s population. A citizen cannot take the most rewarding job he is qualified for if that employer is located outside a reasonable commuting distance. The end result being both employers and citizens suffer and the city’s economic output will be below its true potential. The objective of policy makers here should be to increase the mobility of its citizens, so that the time they need to spend commuting is reduced. Reducing congestion or pollution should be constraints not the objective.\nAffordability refers to the ability of households to locate in an area which maximizes their welfare such that they are not forced to spend more than a reasonable portion of their income (usually 30%) on rent. It is natural to think rent control might be the solution to affordable housing at this point, but this approach has serious issues that will be discussed later.\nAlthough we have a clear objective when it comes to maximizing mobility which is to increase the number of jobs accessible to an individual within a one-hour commute, affordability is not so well defined. Different households have different ‘formulas’ for calculating welfare. Households need to tradeoff between rent, location, floor space and quality of construction to determine what is optimal for them. One household may prioritize location and may prefer to live close to the city center sacrificing floor space while a second may prioritize floor space and hence choose to move further away. What policy makers needs to ensure is that households have the freedom to make these trade-offs and choose for themselves, not to make these choices for them."
  },
  {
    "objectID": "blog/2020-04-04-a-citizen-s-guide-to-the-economics-of-a-city/index.html#how-markets-shape-cities",
    "href": "blog/2020-04-04-a-citizen-s-guide-to-the-economics-of-a-city/index.html#how-markets-shape-cities",
    "title": "A Citizen’s Guide to the Economics of A City",
    "section": "How Markets Shape Cities",
    "text": "How Markets Shape Cities\nMarkets shape cities through real estate prices. The shape and profile of a city is a function of real estate prices and household incomes. The ratio of the floor space available to the land area available is called the Floor Area Ratio (FAR)\nHigher land prices support the construction of more floor space on a given piece of land, i.e. the FAR will high. Consider midtown Manhattan where land prices are as high $25,000 per square meter, the cost of construction of an office building is only $5,000 per square meter. If land and capital are the two factors of production at play here, it makes economic sense to substitute capital for land and build more floor space on a given piece of land leading to the creation of skyscrapers. The average FAR in Manhattan is 15.\nOn the outskirts of cities, the land prices are much lower. If the land prices 40 miles away from Manhattan is only $450 per square meter and construction of a wooden frame home costs $1,600 per square meter, it makes more sense to substitute land for capital leading to low FAR.\n\nThe spectacular skylines of Manhattan, Dubai or Downtown Chicago are therefore not a result of intelligent design on the part of planners but the result of markets and brilliant architects.\nIf this is the case, why don’t all cities have these spectacular skylines? Consider the heart of Paris for instance, where there are only low-rise buildings.\n\nThis is because city planners have imposed a limit on the FAR in the center of Paris with the explicit goal of preserving the traditional 18th century appearance of the city. Although this probably does make Paris a more beautiful city in the eyes of many, it does come with a cost. Limited supply of floor space in the center of Paris means price of floor space there ($17,945 per square meter) is much higher than in the center of Chicago ($ 1,944 per square meter) making it unaffordable to low income and middle-income households.\nSlums, as in Mumbai are created when the poor simply don’t have the resources to substitute capital for land. It is estimated that in developing countries, a household needs at least $6,000 to be able to afford a studio apartment of 12 square meters. People who cannot afford this have to live in slums.\n\nThese people prefer to live in an urban area in squalid conditions due to the better access to the city’s labor market and higher incomes. The extremely small floor space and the narrow lanes that characterizes slums are not a result of poor design but the poorest people maximizing their welfare based on floor space, rent and location. Given the high price of land and the lack of capital that prevents them from building towers, they minimize land consumption by drastically reducing the land allocated to roads and open spaces.\nSome governments plan to eliminate slums by building public housing far from the city and shifting slum dwellers there. Typically, these slums fill back up in no time. The government fails to understand that location is an important factor in the housing decision of these people. Moving them to a location far away from the city without adequate public transport restricts their access to the city’s labor market, limiting their incomes.\nHousehold incomes and quality of transportation also greatly influence the shape and densities of cities. As household incomes increase their land consumption increases leading to a fall in density. Faster transportation also means households can move further away from the city where they can consume more land without a commensurate increase in commute times.\nThe figure below shows how the density of the Chinese city Tianjin decreased over time due to economic growth and higher incomes.\n\nThe population density shown above decreases over time as household incomes continue to grow and people move from informal slums into formal construction. The problem of slums in developing countries can thus be addressed only if economic growth makes it possible for slum dwellers to afford formal housing. Governments can support this by making financing more easily available and incentivizing the creation of cheaper building technology that reduces the cost of a studio apartment below $6,000 per unit.\nGovernments often think they can eliminate slums through regulation, especially by fixing minimum housing consumption. Sanctioning construction only if a housing unit meets certain requirements around floor area can ostensibly ensure satisfactory living conditions for all residents. The consequence of this is that households who cannot afford the price of these units are forced into informal slums. If no such regulations existed, developers might have constructed smaller residential units which would have been affordable to this poorer demographic.\nFurther designation of such slums as illegal settlements due to their failure to meet minimum housing standards deprives them of essential municipal services such as clean water and garbage disposal that further deprives these people living in poverty."
  },
  {
    "objectID": "blog/2020-04-04-a-citizen-s-guide-to-the-economics-of-a-city/index.html#making-housing-affordable",
    "href": "blog/2020-04-04-a-citizen-s-guide-to-the-economics-of-a-city/index.html#making-housing-affordable",
    "title": "A Citizen’s Guide to the Economics of A City",
    "section": "Making Housing Affordable",
    "text": "Making Housing Affordable\nAffordable housing is arguable the biggest challenge facing cities today. We are reaching a situation where in cities like San Francisco, only tech workers are able to afford to live in the city. How can a city thrive when its most essential workers such as teachers or hospital nurses have to commute two hours to get to their jobs?\nA study by the economists Chang-Tai Hseh and Enrico Moretti reveals that high pricing of houses in US cities leads to a distortion in the spatial allocation of labor that costs about 9.4% of national GDP!\nThe housing affordability problem is succinctly summarized by the chart below\n\nA household with income d will be able to consume a quantity of housing at a given location corresponding to g. If you have no income , you will not be able to get any housing in a free market system. Clearly the government should intervene in such situations, for instance providing shelters to homeless people. Society may also decide that all households living in the city should be able to enjoy housing consumption equal to h. In a free market, only households with an income e will be able to afford this level of housing consumption, the choice for society and policy makers is how best to allow households with incomes less than e to increase their housing consumption to h.\nThere are supply side as well as demand side initiatives of varying efficacy that policymakers can pursue to accomplish this. Some of these and their resulting costs and benefits are outlined below."
  },
  {
    "objectID": "blog/2020-04-04-a-citizen-s-guide-to-the-economics-of-a-city/index.html#increase-urban-land-supply",
    "href": "blog/2020-04-04-a-citizen-s-guide-to-the-economics-of-a-city/index.html#increase-urban-land-supply",
    "title": "A Citizen’s Guide to the Economics of A City",
    "section": "Increase Urban Land Supply",
    "text": "Increase Urban Land Supply\nThe government can increase the supply of housing stock by simply removing supply side constraints. This may include simplifying administrative procedures for sanctioning new developments, eliminating FAR restrictions, maximum height of buildings or limits on construction design. The government can also invest in public infrastructure that reduces commute times which will encourage development in other parts of the city that are currently constrained by low connectivity.\nThe effect of these measures on housing consumption is shown below.\n\nAlthough these measures would raise housing consumption across all income groups, the impact on the lowest income groups may be muted. Further supply side reforms may be necessary to increase hosing consumption of the poor."
  },
  {
    "objectID": "blog/2020-04-04-a-citizen-s-guide-to-the-economics-of-a-city/index.html#increase-access-to-mortgages",
    "href": "blog/2020-04-04-a-citizen-s-guide-to-the-economics-of-a-city/index.html#increase-access-to-mortgages",
    "title": "A Citizen’s Guide to the Economics of A City",
    "section": "Increase Access to Mortgages",
    "text": "Increase Access to Mortgages\nAs long as developers are able to quickly respond to demand by building new housing, increasing availability to credit or mortgages can allow households to increase their housing consumption as shown below.\n\nIt is to be noted that mortgages are typically available only to creditworthy borrowers meaning this is able to impact only the population earning more than some specified level of income. In the chart above, the population with incomes above d will have increased access to mortgages allowing them to increase consumption from g to g1.\nIt is to be kept in mind that this policy will simply make housing more expensive if there isn’t an accompanying increase in supply. Also, it is possible to take this policy too far as was the case in the United States in the 2000’s where a concerted policy to make mortgages available to borrowers with low credit ratings led to the sub-prime mortgage boom and bust that imperiled the entire financial system."
  },
  {
    "objectID": "blog/2020-04-04-a-citizen-s-guide-to-the-economics-of-a-city/index.html#vouchers-demand-side-subsidy",
    "href": "blog/2020-04-04-a-citizen-s-guide-to-the-economics-of-a-city/index.html#vouchers-demand-side-subsidy",
    "title": "A Citizen’s Guide to the Economics of A City",
    "section": "Vouchers: Demand Side Subsidy",
    "text": "Vouchers: Demand Side Subsidy\nThe government can provide a cash transfer or a voucher to households below a certain income level so as to bridge the gap between the housing consumption they can currently afford and the market cost of socially acceptable housing.\n\nThe cost of such a program is easy to calculate, it is simply the number of households at each income level times the size of the subsidy (the difference between the lines mh and curve ah). The higher the income of the target beneficiaries, the lower the required subsidy.\nIt is important that government design the subsidy program keeping budgetary implications in mind. If the program targets more beneficiaries than it can afford, it simply leads to a long wait list that can last for years. For instance, in Jan 2016, 86,610 households in New York City receive vouchers while there are 143,033 households in the wait list.\nAdditionally, the benefits of a voucher program are constrained by the supply of housing. It is conceivable that new supply of housing markets will lag the announcement of a voucher program, housing prices will simply go up until new supply becomes available. This program however has the benefit that it gives households the freedom to pick their optimal mix of location, rent, floor space and quality of construction to maximize welfare."
  },
  {
    "objectID": "blog/2020-04-04-a-citizen-s-guide-to-the-economics-of-a-city/index.html#supply-side-subsidies",
    "href": "blog/2020-04-04-a-citizen-s-guide-to-the-economics-of-a-city/index.html#supply-side-subsidies",
    "title": "A Citizen’s Guide to the Economics of A City",
    "section": "Supply Side Subsidies",
    "text": "Supply Side Subsidies\nSupply side subsidies are given to developers to build a predetermined type of housing for a specified price and specified location. This means that to receive the subsidy, a household has to move to a dwelling that receives this subsidy whose location, size and price has been chosen by a planner.\nThis is a drawback of this approach given it no longer gives households the freedom to make their own choices.\n\nPublic Housing\nThe government decides to build public housing for a target segment of the population as shown below. The quality of this public housing permits the target population to consume much more housing than would be permissible in a free market as shown below.\n\nWhen the original consumption of this target segment was initially ph, it now rises to p1h1. The rent of the new public housing will be set at an affordable percentage of the target beneficiaries’ income.Although this sounds like a promising idea, there are several unintended consequences.\nGiven the population with income less than n now enjoys a standard of housing much higher than the population with income slightly higher than n. Large number of the original beneficiaries will likely sublet their units to higher income households at market or sub-market rents and move to new accommodations.\nThis makes complete economic sense as long as the sum of the market rent of their new accommodations and the subsidized rent of the public housing (i.e. expense) is less than the market rent of the public housing (i.e. income). The original beneficiaries can pocket the difference as an additional income stream.\n\nThe potential beneficiaries of the public housing program now become a much larger segment of the income distribution (qn1 vs qn) which contravenes the original purpose of the program. The only way to prevent this is by imposing and enforcing draconian identification standards on all public housing beneficiaries. If this cannot be done, public housing will simply crowd out private development that would have occurred to cater to the population in the income range n — n1.\nAdditionally, public housing results in the creation of monolithic buildings that typically end up becoming segregated ghettos for low income families. Further, mobility of these households is severely restricted since they will lose their subsidies if they move. If a household were to increase their income slightly above n, they would lose public housing benefits, the rational decision here would be simply to conceal their high income so as to continue enjoying these benefits.\n\n\nInclusionary Zoning\nInclusionary zoning is creative regulation to get developers to pay housing subsidies to the poor without burdening the taxpayer. It typically comprises of a municipal zoning ordinance that requires developers of housing projects to provide 20%–30% of units at a price or rent fixed by the municipal government below market price and defined as affordable. The remaining units can be rented or sold at market rates. Planners incentivize developers to do this by providing a FAR bonus if they include such affordable housing i.e., they can build more apartment units on the same piece of land provided 20% -30% of units are ‘affordable’.\nOne consequence of this policy should be obvious. The developer will simply pass on the cost of the subsidized units to the buyers of the non-subsidized units. So, the adage that there are no free lunches still rings true.\nn example of such an inclusionary zoning project is a building called VIA 27, in New York that has a total of 709 units, out of which 142 (20 %) are being offered as affordable housing. To qualify potential beneficiaries must have an annual income between $ 19,000 and $ 50,000; as of 2016, 29 % of NY households fall in this part of the income distribution.\nThe households who rent at market price have incomes varying from $ 160,000 to $ 470,000 representing 9% of NY households.\nIt is clear from these numbers that 9% of the population cannot cross subsidize housing for 29% of the population unless each of these households rent or buy multiple apartments.\nIn fact, there were 91,000 applicants for 254 such affordable units in a recently build NY apartment who are to be selected by lottery. An affordable housing policy that benefits only 0.27% of qualified beneficiaries is not an effective system.\nGiven such inclusive zoning is only provided in areas where prices and demand are very high, what it does is provide a small pool of luxury housing for the few poor who are lucky. At the VIA 57 building, the subsidized rents vary from $565 to $ 1,067 per month while market rents vary from $ 3,400 to $ 8,700 per month. This means a single household is getting a subsidy as high as$107,000 a year — a truly staggering amount. Most of these beneficiary households will probably prefer to get a fraction of that amount in cash rather than as an indirect housing subsidy.\nNotwithstanding the inefficiency of this mechanism to disperse taxes collected from market tenants, it also raises a question of fairness. A household earning $ 51,000 a year is ineligible for the subsidy while a household earning $ 1000 less gets a $ 100,000 a year subsidy. An equitable allocation of benefits should ensure that all households below a certain income are guaranteed a minimum level of housing consumption.\nAs in the case of public housing, this creates incentives for existing beneficiary households to game the system and under report incomes so that they can continue to enjoy the massive subsidy. There is also an incentive to sublet at market rates so as to create an additional income stream. Given the subsidy is tied to the specific unit, it is also unlikely that any occupant will ever move out which adversely impacts mobility.\nThere is a further hidden cost to the government resulting from tax subsidies given to developers to incentivize inclusive zoning. In New York city, the tax incentive program cost the city $ 905,000 per apartment rented below market for the duration of the tax incentive. This amounted to an annual tax cost of $ 45,000 per housing unit. The fact that these costs are hidden and incurred in the future makes it easy to ignore them, but even so the costs are still incurred, just not visibly.\n\n\nRent Control\nRent control in its current form was created during WWII as a means for households to cope with the terrible dislocations of war. The destruction of houses, the influx of refugees, the migration of people to work in war industries and soldiers returning home after the war all drove up rents which the government understandably addressed be introducing rent control regulations. However, policies introduced during the midst of war continue to be used as a tool of economic policy.\nRent control does work but only for the few who are lucky enough to live in rent-controlled housing. The first order effect of rent control is in the absence of adequate returns, landlords have little incentive to build new housing meaning that new migrants to the city will not have access to affordable housing.\nFurther, rent controlled housing can lead to over consumption of housing just as in the case with inclusive zoning. Given the subsidy is tied to a unit rather than a households, rent control curtails mobility significantly. If a family of six with four children moved into a 3-bedroom rent controlled unit 30 years ago, the family will likely continue to stay in the same apartment even though all the children are adults and have left to start their own families elsewhere. A 1997 study by Ed Glaeser and Erzo Luttmer showed that such misallocation of space can lead to a loss in welfare amounting to over $500 million annually.\nAn empirical study of the effects of rent control in San Francisco by Rebecca Diamond, Tim McQuade and Franklin Qian from Stanford University finds that ‘rent control decreases mobility by 20%’ (as expected) and ‘lowers displacement from San Francisco, especially for minorities’. So, the policy does seem to yield some of the outcomes desired by its architects. However, landlords react to this by reducing housing supplies by 15% by selling the apartments or tearing them down and building new construction. The study concludes that ‘ While rent control prevents displacement of incumbent renters in the short run, the lost rental housing supply likely drove up market rents in the long run, ultimately undermining the goals of the law.’\nAnother study by David Autor, Parag Pathak and Chris Palmer from MIT showed that lifting of rent controls in Cambridge, Massachusetts resulted in an increase in quality of rental units and a decrease in street crime. However, this did not lead to an increase in affordable housing.\nRemoval of rent controls without an easing of supply side constraints is only likely to drive out families in rent-controlled housing without resulting in creation of any new dwelling units."
  },
  {
    "objectID": "blog/2020-04-04-a-citizen-s-guide-to-the-economics-of-a-city/index.html#conclusion",
    "href": "blog/2020-04-04-a-citizen-s-guide-to-the-economics-of-a-city/index.html#conclusion",
    "title": "A Citizen’s Guide to the Economics of A City",
    "section": "Conclusion",
    "text": "Conclusion\nAs Hayek famously said, “The curious task of economics is to demonstrate to men how little they really know about what they imagine they can design.”\nThere needs to be closer collaboration between urban economists and city planners so that markets are factored into city planning and decision making; and a recognition that the role of governments and planners is to facilitate and enable the growth of cities not to control and direct it.\nAs Bertaud succinctly puts it, designing cities without considering markets is like designing a rocket while ignoring gravity. I hope this article has shed light on some of the underlying market forces that shape cities and allows you to critically assess policies that are likely to be put forth by policy makers to address the housing affordability crises facing most cities in the world today.\nIt also remains to be seen what kind of impact a once in a century disaster such as the Covid-19 pandemic has on cities. Will remote work become a norm for many companies that frowned upon it earlier? Will this be a secular trend across several industries? If so, it will radically alter the shape of cities as they become more dispersed with people move away from cities seeking open spaces and less congestion. The demands on urban infrastructure and transportation could also change considerably as a result.\nPS: Please note that I have only discussed a sample of the topics that are originally addressed in the book. I have completely skipped over the topic of the design of urban transportation which is the lifeblood of any city."
  },
  {
    "objectID": "blog/2020-04-04-a-citizen-s-guide-to-the-economics-of-a-city/index.html#other-references",
    "href": "blog/2020-04-04-a-citizen-s-guide-to-the-economics-of-a-city/index.html#other-references",
    "title": "A Citizen’s Guide to the Economics of A City",
    "section": "Other References",
    "text": "Other References\n1)[https://ourworldindata.org/urbanization] 2)[https://freakonomics.com/podcast/rent-control/]"
  },
  {
    "objectID": "blog/2021-04-03-the-secretary-problem.en/index.html",
    "href": "blog/2021-04-03-the-secretary-problem.en/index.html",
    "title": "The Secretary Problem",
    "section": "",
    "text": "Imagine you are recruiting for an open position at your company. You have the time budget to interview a 100 people. How many people do you interview before deciding on a candidate? If you like the first candidate you interview, do you extend an offer?\nLet us say you are a very attractive single on Tinder looking for matches. You have 100 swipes a day that you can use. You are so desirable that you are certain anyone who sees your profile is going to swipe right. You don’t want to disappoint folks you matched with by not responding; so you want to swipe right on only one potential match who you think is perfect for you. How long do you wait before you swipe right?\nYou are in the market for your dream house. You are looking to close a deal in the next 3 months. You figure that you have enough time to visit 20 houses. How many houses should you visit before submitting a bid?\nLet us also make a reasonable assumption that once you have rejected a potential hire, date or home;there is no going back. A rejected candidate goes on to a different job, a rejected match goes on to date someone else and a rejected home is bought by a different bidder.\nThese are difficult choices to make. Admittedly, I am familiar only with the first one but I guess some of you have experienced all three!\nWell, worry not! There exists a mathematically optimal way to make a choice in such circumstances. The problem is generally referred to as the Secretary Problem."
  },
  {
    "objectID": "blog/2021-04-03-the-secretary-problem.en/index.html#the-solution-strategy",
    "href": "blog/2021-04-03-the-secretary-problem.en/index.html#the-solution-strategy",
    "title": "The Secretary Problem",
    "section": "The Solution Strategy",
    "text": "The Solution Strategy\nFaced with this problem, most people would choose to review and reject the first $r $ candidates to determine the quality of the candidate pool before settling on one of the subsequent candidates.\nTo use a concrete example, let us say you are set to interview a 100 candidates, you will speak to the first 20 candidates and rate these candidates on a scale of 1-10. You will reject these 20 candidates.Say the best candidate in the first 20 scored 7 points. Starting with the 21st candidate, if anyone scores above 7, you select that candidate.\n\n\n\nFigure 1: Solution Strategy\n\n\nMore generally, you interview \\(N\\) candidates, \\(r\\) is the stopping point; i.e. you interview and reject the first r candidates while keeping track of the best candidate \\(r^*\\). You then meet a new candidate \\(i^*\\) at position \\(n + 1\\) such that he or she is better than \\(r^*\\). You select this candidate.\nFor a given stopping point \\(r\\), we can estimate the total probability of selecting the best overall candidate \\(i^*\\) as a function of \\(r\\). We can then find the value of \\(r\\) that maximizes this function \\(\\mathbb{F}(r)\\). This will be the optimal stopping point."
  },
  {
    "objectID": "blog/2021-04-03-the-secretary-problem.en/index.html#solution",
    "href": "blog/2021-04-03-the-secretary-problem.en/index.html#solution",
    "title": "The Secretary Problem",
    "section": "Solution",
    "text": "Solution\nIf we select the candidate at \\(n+1\\) ( event \\(E_1\\) ) it is optimal if the following two conditions are fulfilled\n\nThe candidate at \\(n+1\\) is the best candidate overall; let this be event \\(E_2\\) \nThe best candidate in \\([1,n]\\) must also be the best candidate in \\([1,r]\\); let this be event \\(E_3\\); In other words there should not be a candidate better that \\(r^*\\) in the interval \\((r,n]\\). If such a candidate exists we would select that candidate and not the candidate at \\(n + 1\\).\n\n\\[ \\mathbb{P}(E_1) = \\mathbb{P}(E_2 \\cap E_3) = \\mathbb{P}(E_2) \\times \\mathbb{P}(E_3)    \\]\nAny of the \\(N\\) candidates could be the best candidate overall, therefore\n\\(\\mathbb{P}(E_2) = \\frac{1}{N}\\)\nFurther, Let event \\(E_4\\) be the event that the best candidate in the interval \\([1,n]\\) lie in the internal \\((r ,n]\\).\n\\(\\mathbb{P}(E_4) = \\frac{n-r}{n}\\)\nThe \\(\\mathbb{P}(E_3)\\) is given by the following expression\n\\(\\mathbb{P}(E_3) = 1 - \\mathbb{P}(E_4) = \\frac{r}{n}\\)\nThis gives:\n\\[ \\mathbb{P}(E_1) =  \\frac{r}{n} \\frac{1}{N} \\] Given \\(r\\) is fixed, \\(n\\) can be any value in the interval \\([r,N-1]\\). This is because the best candidate \\(i^*\\) occupying position \\(n + 1\\) lies in the interval \\([r + 1, N ]\\)\nSumming over all these probabilities gives the function we want to maximize\n\\[ \\mathbb{F}(r) =  \\frac{r}{r} \\frac {1}{N} + \\frac{r}{r+1} \\frac {1}{N} +..... +\\frac{r}{N-1} \\frac {1}{N} \\]\n\\[  =  \\frac{r}{N} ( \\frac{1}{r} + \\frac{1}{r+1} +.... \\frac{1}{N-1}) \\]\n\\[  = \\frac{r}{N} \\sum\\limits_{n=r}^{N-1} \\frac{1}{n} \\] We can now maximize this function analyticaly or numerically.\n\nAnalytical Solution\nLet us look at the quantity being summed over. Let us set \\(r = 1\\) and \\(N = 21\\)\n\\[ \\sum\\limits_{n=1}^{21-1} \\frac{1}{n} \\]\nlibrary(ggplot2)\nn &lt;- c(1:20)\nf_n &lt;- 1/n\n\ndf &lt;- data.frame(x=n,y=f_n)\nfun1 &lt;- function(x) 1/x\n\nggplot(data=df,aes(x=x,y=y))+ geom_bar(stat=\"identity\",fill='cornflowerblue')+ geom_function(fun=fun1,aes(col='1/x'))+\ntheme(legend.title=element_blank())\n\nClearly, the sum simply gives the sum of the areas of each rectangular bar. Here each bar has a unit width and there are 20 bars, now let us shrink the width of each bar to 0.1 so that there are 200 bars.\nn &lt;- seq(0,20,0.1)\nf_n &lt;- 1/n\n\ndf &lt;- data.frame(x=n,y=f_n)\nggplot(data=df,aes(x=x,y=y))+ geom_bar(stat=\"identity\",fill='cornflowerblue')+ geom_function(fun=fun1,aes(col='1/x'))+\ntheme(legend.title=element_blank())\n\nThe sum above is simply giving the area under the curve for the function \\(f(x) = \\frac{1}{x}\\) as \\(N \\rightarrow \\inf\\). Technically the sum above is the Reimann approximation of the integral over \\(\\frac{1}{x}\\). We can replace the approximation with the actual integral as follows.\n\\[ \\mathbb{F}(r) = \\frac{r}{N} \\int_r^N \\frac{1}{x}dx  \\]\nSolving for this integral is fairly easy.\n\\[  \\int_r^N \\frac{1}{x}dx  = [ ln\\ x] _r^N = ln (\\frac{N}{r}) \\]\nThe function to be maximized is now \\(\\mathbb{F}(r) = \\frac{r}{N}ln (\\frac{N}{r}) = - \\frac{r}{N}ln (\\frac{r}{N})\\)\nTaking the derivative with respect to \\(r\\) gives\n\\[ \\mathbb{F}'(r) = -1 - ln(\\frac{r}{N}) \\]\nTo find the maximum we set this to 0 and solve it.\n\\[ ln(\\frac{r}{N})  = -1 \\]\n\\[ \\frac{r}{N} = e^{-1}  = 0.367 \\] This means out stopping point \\(r\\) should be set at a value equal to \\(0.367 N\\).\n\n\nNumerical Solution\nIf you do not wish to work through the math, you can also arrive at the same solution numerically.\nRemember the function we wish to optimize is given by:\n\\[ \\mathbb{F}(r) = \\frac{r}{N} \\sum\\limits_{n=r}^{N-1} \\frac{1}{n} \\]\nWe can code this function as shown below. Let us assume \\(N = 100\\), or we will be interviewing 10000 candidates.\nFr &lt;- function(r,N=10000){\n  \n  (r/N)*sum(1/(seq(from=r,to=N-1)))\n  \n\n}\nWe can now pass this function into an optimizer.\noptimize(Fr,interval = c(1:10000),maximum=TRUE)\n## $maximum\n## [1] 3695\n## \n## $objective\n## [1] 0.3679074\nAs expected the optimal stopping point is approximately \\(10,000 \\times \\frac{1}{e} = 3,695\\). At this point the probability of selecting the optimal candidate is 0.367."
  },
  {
    "objectID": "blog/2021-04-03-the-secretary-problem.en/index.html#experiment",
    "href": "blog/2021-04-03-the-secretary-problem.en/index.html#experiment",
    "title": "The Secretary Problem",
    "section": "Experiment",
    "text": "Experiment\nWe will assume we are interviewing 100 candidates.We will assign a measure of quality to the 100 candidates as shown below\nset.seed(999)\nq &lt;- runif(100)\nThe best candidate has a quality given by\n(best_candidate_quality &lt;- max(q))\n## [1] 0.9914061\nThis candidate occupies the following position\nwhich(q==best_candidate_quality)\n## [1] 61\nGiven a list of candidates and their quality measure, the following function returns the best candidate and the value of the best candidate for the given stopping point.\nbest_candidate &lt;- function(r,q){\n  #r : stopping point\n  #q : vector giving quality of candidates\n  \n  #Step 1: Find the best candidate in the range 1:r (i.e. up to the stopping point)\n  best_1 &lt;- max(q[1:r])\n  # Step 2: Find the first candidate after the stopping point who is better than the best candidate before the stopping point\n  best_2 &lt;- which((q[(r+1):length(q)]&gt;=best_1) == TRUE)\n  selected_candidate &lt;- ifelse(length(best_2)==0,length(q),r+best_2)\n  selected_candidate_value &lt;- q[selected_candidate]\n  \n\n   return(c(selected_candidate,selected_candidate_value))\n  \n}\nWe will run a thousand simulations for various stopping points(10,20,37,40,50,60,70,80 and 90) and tabulate the number of experiments where the optimal candidate was selected.\nexperiment &lt;- function(r,N){\n  #N: No of experiments to run\n  #r: stopping point to be tested\n  \n  \n  #C : counter to keep track of number of times optimal candidate was selected\n  C &lt;- 0\n  \n  for (i in seq_len(N)){\n    q_new &lt;- sample(q)\n    if (best_candidate(r,q_new)[2]==best_candidate_quality){\n      C &lt;-  C + 1\n    }   \n  \n  }\n  \n  return(C)\n}\nLet us now cary out the experiments\nset.seed(1)\nstop_points &lt;- c(10,20,37,40,50,60,70,80,90)\n(results &lt;- sapply(stop_points,experiment,1000))\n## [1] 231 351 385 373 312 299 261 196  92\ndf &lt;- data.frame(x=stop_points,y=results)\nggplot(df,aes(x=x,y=y))+ geom_bar(stat='identity')+labs(x='Stopping Point',y='No of Optimal Selections')+\n  scale_x_continuous(breaks=stop_points,labels=stop_points)\n\nAs you can, see when the stopping point is 37, you will pick the optimal candidate the maximum number of times."
  },
  {
    "objectID": "blog/2021-04-03-the-secretary-problem.en/index.html#conclusion",
    "href": "blog/2021-04-03-the-secretary-problem.en/index.html#conclusion",
    "title": "The Secretary Problem",
    "section": "Conclusion",
    "text": "Conclusion\nSo there you have it, you have a way to optimally systematize a decision making process instead of second guessing yourself.It is also a wonderful example of how some simple probability and calculus can be practically very useful.\nWhen faced with making such a choice, reject the first 37% of candidates,then select the first candidate you encounter who is better than the best candidate you have seen so far. If you don’t see a better candidate, pick the last candidate."
  },
  {
    "objectID": "blog/2021-04-03-the-secretary-problem.en/index.html#references",
    "href": "blog/2021-04-03-the-secretary-problem.en/index.html#references",
    "title": "The Secretary Problem",
    "section": "References",
    "text": "References\n\nhttps://www.youtube.com/watch?v=XIOoCKO-ybQ&t=335s\nhttps://rs.io/the-secretary-problem-explained-dating/\nhttps://www.cantorsparadise.com/math-based-decision-making-the-secretary-problem-a30e301d8489\nhttps://www.khanacademy.org/math/ap-calculus-ab/ab-integration-new/ab-6-3/a/definite-integral-as-the-limit-of-a-riemann-sum"
  },
  {
    "objectID": "blog/2021-08-14-solving-tic-tac-toe-with-reinforcement-learning.en/index.html",
    "href": "blog/2021-08-14-solving-tic-tac-toe-with-reinforcement-learning.en/index.html",
    "title": "Solving Tic-Tac-Toe with Reinforcement Learning",
    "section": "",
    "text": "In this earlier blog post, I covered how to solve Tic-Tac-Toe using the classical Minimax algorithm. Here we will use Reinforcement Learning to solve the same problem.\nThis should give you an overview of this branch of AI in a familiar setting. As argued in this paper by pioneers in the field, RL could be the key to Artificial General Intelligence. Therefore, it would behoove us to better understand this fascinating field.\nTo see the full code, please refer to the notebook here. I will only focus on the results here, lest the blog become too code heavy."
  },
  {
    "objectID": "blog/2021-08-14-solving-tic-tac-toe-with-reinforcement-learning.en/index.html#introduction",
    "href": "blog/2021-08-14-solving-tic-tac-toe-with-reinforcement-learning.en/index.html#introduction",
    "title": "Solving Tic-Tac-Toe with Reinforcement Learning",
    "section": "",
    "text": "In this earlier blog post, I covered how to solve Tic-Tac-Toe using the classical Minimax algorithm. Here we will use Reinforcement Learning to solve the same problem.\nThis should give you an overview of this branch of AI in a familiar setting. As argued in this paper by pioneers in the field, RL could be the key to Artificial General Intelligence. Therefore, it would behoove us to better understand this fascinating field.\nTo see the full code, please refer to the notebook here. I will only focus on the results here, lest the blog become too code heavy."
  },
  {
    "objectID": "blog/2021-08-14-solving-tic-tac-toe-with-reinforcement-learning.en/index.html#introduction-to-reinforcement-learning",
    "href": "blog/2021-08-14-solving-tic-tac-toe-with-reinforcement-learning.en/index.html#introduction-to-reinforcement-learning",
    "title": "Solving Tic-Tac-Toe with Reinforcement Learning",
    "section": "Introduction to Reinforcement Learning",
    "text": "Introduction to Reinforcement Learning\nI will refer you to this post by someone way smarter than me if you want to really spend some time understanding Reinforcement Learning.\nI will just skim the surface in this section. Feel free to skip this section entirely.\n\nAs shown in the figure above, the reinforcement learning framework comprises the following elements\n\nAgent : Entity learning about the environment and making decisions. We need to specify a learning algorithm for the agent that allows it to learn a policy \nEnvironment: Everything outside the agent, including other agents \nRewards: Numerical quantities that represent feedback from the environment that an agent tries to maximize \n\nGoal reward representation: 1 for goal, 0 otherwise\nAction penalty representation: -1 for not goal, 0 once goal is reached\n\nState: A representation of the environment. At time step $ t $,the agent is in state $ S_t $ where $ $ is the set of all possible states \nAction: At time step $ t $, an agent takes an action $ A_t (S_t) $ where $ (S_t) $ is the set of actions available in state $ S_t $ \nPolicy: A policy tells the agent what action to take in a given state. $ (a|S) $\n\nA policy can be deterministic i.e. there is one action that is deterministically selected in a given state $ (s)=a $, or stochastic i.e. the policy maps a state onto a set of probabilities for taking each action. $ [a^i|s] &lt; 1 $ subject to $ _i [a^i|s] =1 $\nTo solve a problem using RL, we should be able to formulate it as a markov decision process (MDP).\n\nMarkov Decision Process\nIn an MDP, the environment is completely characterized by the transition dynamics equation\n\\[ p(s',r|s,a) \\] That is, the probability of each possible value for $ s’ $ (the subsequent state) and $ r $ (reward) depends only on the immediately preceding state and action, $ s $ and $ a $, and, given them, not at all on earlier states and actions. In other words, given the present, the future is independent of the past.\nThe state must include information about all aspects of the past agent–environment interaction that make a difference for the future. If it does, then the state is said to have the Markov property\nIf the transition dynamics equation is fully known by the agent, it means an optimal policy can be computed without interacting with the environment. This is planning. Some kind of search algorithm can be used here.\nWhen the environment is not fully known, the agent has to learn by interacting with the environment. i.e. learning. If an agent constructs a model of the environment , it is called model based RL, else it is called model free RL.\nIf you are building a self driving car, learning from real experience can be too expensive so you want to build a model of then environment which you can query for information to make decisions.\nWhen an agent in state $ S_t $ takes an action $ A_t $ as prescribed by a policy $ $, it transitions to a state $ S_{t+1} $ and receives a reward $ R_{t+1} $. The Agent interacting with the MDP environment thus gives rise to a sequence or trajectory\n\\[ S_0,A_0,R_1,S_1,A_1,R_2,S_2,... \\] The goal of an agent is to maximize the long term reward or return.\nLong term reward or return is formally defined as the discounted sum of future rewards.\n\\[ G_t = R_{t+1} + \\gamma R_{t+2} + \\gamma R_{t+3} +... = \\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1} \\]\n\\[ = R_{t+1} + \\gamma G_{t+1} \\]"
  },
  {
    "objectID": "blog/2021-08-14-solving-tic-tac-toe-with-reinforcement-learning.en/index.html#value-functions",
    "href": "blog/2021-08-14-solving-tic-tac-toe-with-reinforcement-learning.en/index.html#value-functions",
    "title": "Solving Tic-Tac-Toe with Reinforcement Learning",
    "section": "Value Functions",
    "text": "Value Functions\nTo navigate an environment optimally, we need the concept of a value function that comes in two flavors\n\nState Value Function \nAction Value Function\n\nThe State - Value function of a state $ s $ under a policy $ $,is the expected return from following policy $ $ when starting in state $ s $\n\\[ v_{\\pi}(s) \\doteq \\mathbb{E}_{\\pi}[G_t | S_t =s]  \\]\nThe Action-Value function is the value of taking action $ a $ in state $ s $ under policy $ $ and thereafter following the policy $ $\n\\[ q_{\\pi}(s,a) \\doteq \\mathbb{E}_{\\pi}[G_t| S_t =s ,A_t=a] \\]"
  },
  {
    "objectID": "blog/2021-08-14-solving-tic-tac-toe-with-reinforcement-learning.en/index.html#bellman-equations",
    "href": "blog/2021-08-14-solving-tic-tac-toe-with-reinforcement-learning.en/index.html#bellman-equations",
    "title": "Solving Tic-Tac-Toe with Reinforcement Learning",
    "section": "Bellman Equations",
    "text": "Bellman Equations\nThe above definitions of the state and action value functions suggest equations to evaluate them known as Bellman Equations.\nThe Bellman expectation equation for the state value function follows naturally from the above definition of the state value function.\n\\[ v_{\\pi}(s) \\doteq \\mathbb{E}[G_t | S_t =s] = \\mathbb{E}_{\\pi}[R_{t+1} + \\gamma G_{t+1} | S_t =s]\\]\n\\[ = \\sum_{a} \\pi(a|s) \\sum_{s'}\\sum_{r} p(s',r|s,a) \\Big[ r + \\gamma \\mathbb{E}_{\\pi}[G_{t+1}|S_{t+1} = s']\\Big] \\] \\[ = \\sum_{a} \\pi(a|s) \\sum_{s',r}  p(s',r|s,a) [r + \\gamma v_{\\pi}(s')] \\]\nThis is easily understood from the backup diagram shown below.\n\nThe value of a state $ s $ is obtained by considering all possible paths to all possible successor states , and weighting the rewards obtained and value of these successor states by the probabilities of taking each path.\nThe Bellman expectation equation for the action value function is similarly given by\n\\[ q_{\\pi}(s,a) \\doteq \\mathbb{E}_{\\pi}[G_t| S_t =s ,A_t=a] \\] \\[ = \\mathbb{E}_{\\pi}[R_{t+1} + \\gamma G_{t+1} | S_t =s',A_t=a'] \\]\n\\[ =  \\sum_{s',r} p(s',r|s,a)[r + \\gamma \\sum_{a'} \\pi(a'|s')q_{\\pi}(s',a')] \\]\nTo make this idea concrete, we can calculate the state value functions for the simplified version of the example we covered in the previous blog post.We will assume that there is only a single player carrying out a series of actions following a random policy with an equal probability of taking either action - L or R\nWe will assume the discount factor $ = 1 $\n\nThe value of any given state is derived from the value of the successor states.E.g. \\[ V_{\\pi}(B) = 0.5V_{\\pi}(D) + 0.5 V_{\\pi}(E)  = 0.5 \\times 4 + 0.5 \\times 3 = 3.5 \\]\nOnce the value of the successive states are known, the agent can pick the action that leads to the optimal state. In this example, the agent wants to move to state B, and takes action “L” to move to that state. A limitation of the state value function is that once you have determined the optimal state, you have to then identify the action that leads to that state.\nThe action value function does not have this limitation, it directly gives the value of each action at a given state making it easy to pick the optimal actions.\nThe action-value function at states B, C and A are given by\n\\[ Q(\\mathcal{S}=B,\\mathcal{A}=L) = 4 \\] \\[ Q(\\mathcal{S}=B,\\mathcal{A}=R) = 3 \\] \\[ Q(\\mathcal{S}=C,\\mathcal{A}=L) = 2 \\] \\[ Q(\\mathcal{S}=C,\\mathcal{A}=R) = 1 \\] \\[ Q(\\mathcal{S}=A,\\mathcal{A}=L) = 3.5 \\] \\[ Q(\\mathcal{S}=A,\\mathcal{A}=R) = 1.5 \\]"
  },
  {
    "objectID": "blog/2021-08-14-solving-tic-tac-toe-with-reinforcement-learning.en/index.html#optimal-policy",
    "href": "blog/2021-08-14-solving-tic-tac-toe-with-reinforcement-learning.en/index.html#optimal-policy",
    "title": "Solving Tic-Tac-Toe with Reinforcement Learning",
    "section": "Optimal Policy",
    "text": "Optimal Policy\nTheorem  For any MDP  - There exists an optimal policy $ * $, that is better than or equal to all other policies, $* * , $ - All optimal policies achieve the optimal value function $ v_{} = v_(s) $ - All optimal policies achieve optimal action-value function, $ q{_}(s,a) = q_*(s,a) $\nAn optimal policy can be found by maximizing over the optimal value function \\[ \\pi_*(s) = \\underset{a} \\arg\\max q_*(s,a) \\]\n$ q_*(s,a) $ is given by the Bellman optimality equations.\nThe Bellman Optimality equation for state values is given by\n\\[  v_*(s) =  \\max_{a} \\sum_{s',r}  p(s',r|s,a) [r + \\gamma v_{\\pi}(s')] \\]\nThe Bellman Optimality equation for action-values is given by\n\\[ q_{*}(s,a) = \\sum_{s',r} p(s',r|s,a)[r + \\gamma\\ \\underset{a'}max \\ \\pi(a'|s')q_{\\pi}(s',a')] \\]\nAll RL algorithms solve for the Bellman Equations exactly or approximately. To solve the Tic-Tac-Toe, I will use an algorithm called Q-Learning. I will not not go into details of Q-Learning as there are plenty of free online resources that cover this."
  },
  {
    "objectID": "blog/2021-08-14-solving-tic-tac-toe-with-reinforcement-learning.en/index.html#design",
    "href": "blog/2021-08-14-solving-tic-tac-toe-with-reinforcement-learning.en/index.html#design",
    "title": "Solving Tic-Tac-Toe with Reinforcement Learning",
    "section": "Design",
    "text": "Design\nTo solve this problem I will create a TicTacToe class that represents the board as a player sees it. Each of the two players retain their own copy of the board. This is possibly an inefficient design, but this is what I will run with.\nGiven tic-tac-toe is a 2 player game, I essentially simulate two different environments. In the first one, the agent is player X and in the second one the agent is player Y.\nAfter the agent plays, the move by the opposing player is considered a change in the environment resulting from the agent’s actions. The new state the agent lands in is a board where the opposing player has already made his/her move.\nimport numpy as np\nfrom itertools import product\nimport pandas as pd\nimport random\nfrom collections import defaultdict\nfrom tqdm import tqdm\nfrom collections import Counter\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\n\nclass TicTacToe():\n    def __init__(self,player = 'X',reward_type ='goal_reward'):\n        '''\n        player: Role agent should play. If X, agent has the first turn else agent has second turn\n        reward_type: 'goal_reward' or 'action_penalty'\n        '''\n        self.board = np.array(['__']*9).reshape(3,3)\n        self.reward_type = reward_type\n        self.winning_seqeunce = None #Keep track of winning move made by agent\n        self.first_move = None #Keep track of first move made by agent\n        if player == 'X':\n            self.me ='X'\n            self.id = 1\n            self.opponent = 'O'\n        else:\n            self.me = 'O'\n            self.id = 2\n            self.opponent = 'X'\n     \n        self.game_over = False #Flag indicating whether game is over\n        # Mapping of action representation in board to action representation in tuple \n        self.b_to_s = {'__':0,'X':1,'O':2} \n        # Mapping of action representation in tuple to action representation in board\n        self.s_to_b = {0:'__',1:'X',2:'O'} \n        \n        #Create mapping from 2D position in board to 1D position in tuple\n        positions = self.available_positions()\n        self.b2_to_s1 = {position:i for (i,position) in enumerate(positions)}\n        \n        #Create mapping from 1D position in tuple to 2D position in board \n        self.s1_to_b2 = {i:position for (i,position) in enumerate(positions)}\n        \n        #State the current player is in\n        self.starting_state = self.board_to_state()\n        \n        #Initialize all possible states of the game\n        l_o_l = [list(range(3)) for _ in range(9)]\n        states = set(product(*l_o_l))\n        \n\n        \n        #Player X states include states with odd number of blanks and both players have occupied equal number of slots\n        #Player O players after Player X, so player O states include states with even number of blanks and where\n        #player X has occupied one more slot than player O\n        playerX_states = {state for state in states if (state.count(0)%2 == 1 and state.count(1)==state.count(2))} #\n        playerO_states =  {state for state in states if (state.count(0)%2 == 0 and state.count(1)==(state.count(2)+1))}\n        \n        #States \n        #self.board_full_states = {state for state in states if state.count(0)==0}\n        if player == 'X':\n            self.my_states = playerX_states\n        else:\n            self.my_states = playerO_states\n          \n    \n    def reset_board(self):\n        \"Function to reset game and reset board to starting state\"\n        self.board = np.array(['__']*9).reshape(3,3)\n        self.starting_state = self.board_to_state()\n        self.game_over = False\n        self.winning_sequence = None\n        self.first_move = None\n    \n    def show_board(self):    \n        \"Shows board as a pandas dataframe\"\n        return pd.DataFrame(self.board)\n    \n    def board_to_state(self):\n        \"Convert a board to a state in tuple format\"\n        return tuple([self.b_to_s[x] for x in np.ravel(self.board)])\n    \n    @staticmethod\n    def possible_actions(state):\n        \"Return possible actions given a state\"\n        return [i for i,x  in enumerate(state) if x ==0]\n    \n\n        \n    def is_game_over(self):\n        \"Function to check if game is over\"\n        if not np.any(self.board == '__') :\n            self.game_over = True\n            \n        return self.game_over\n    \n    def available_positions(self):\n        \"Return available positions on the board\"\n        x,y = np.where(self.board =='__')\n        return[(x,y) for x,y in zip(x,y)]\n    \n    \n    def win(self,player):\n        \"Check if player won the game and record the winning sequence\"\n        if np.all(self.board[0,:] == player):\n            self.winning_sequence = 'R1'\n        elif np.all(self.board[1,:] == player): \n            self.winning_sequence = 'R2'\n        elif np.all(self.board[2,:] == player):\n            self.winning_sequence = 'R3'\n        elif np.all(self.board[:,0] == player):\n            self.winning_sequence = 'C1'\n        elif np.all(self.board[:,1] == player):\n            self.winning_sequence = 'C2'\n        elif np.all(self.board[:,2] == player):\n            self.winning_sequence = 'C3'\n        elif np.all(self.board.diagonal()==player):\n            self.winning_sequence = 'D1'\n        elif  np.all(np.fliplr(self.board).diagonal()==player):\n            self.winning_sequence = 'D2'\n        else:\n            return False\n        \n        return True\n    \n    \n    def my_move(self,position):\n        \"Fills out the board in the given position with the action of the agent\"\n        \n        assert position[0] &gt;= 0 and position[0] &lt;= 2 and position[1] &gt;= 0 and position[1] &lt;= 2 , \"incorrect position\"\n        assert self.board[position] == \"__\" , \"position already filled\"\n        assert np.any(self.board == '__') , \"Board is complete\"\n        assert self.win(self.me) == False and self.win(self.opponent)== False , \" Game has already been won\"\n        self.board[position] = self.me\n        \n        I_win = self.win(self.me)\n        opponent_win = self.win(self.opponent)\n        \n        if self.reward_type == 'goal_reward':\n            if I_win:\n                self.game_over = True\n                return 1\n            \n            elif opponent_win:\n                self.game_over = True\n                return -1\n            \n            else:\n                return 0\n            \n        elif self.reward_type == 'action_penalty':\n            if I_win:\n                self.game_over = True\n                return 0\n            \n            elif opponent_win:\n                self.game_over = True\n                return -10\n            \n            else:\n                return -1\n    \n    def opponent_move(self,position):\n        \"Fills out the board in the given position with the action of the opponent\"\n        assert position[0] &gt;= 0 and position[0] &lt;= 2 and position[1] &gt;= 0 and position[1] &lt;= 2 , \"incorrect position\"\n        assert self.board[position] == \"__\" , \"position already filled\"\n        assert np.any(self.board == '__') , \"Board is complete\"\n        assert self.win(self.me) == False and self.win(self.opponent)== False , \" Game has already been won\"\n        self.board[position] = self.opponent\n            \n    \n    def pick_best_action(self,Q,action_type,eps=None):\n        '''Given a Q function return optimal action\n        If action_type is 'greedy' return best action with ties broken randomly else return epsilon greedy action\n        '''\n        #Get possible actions\n        current_state = self.board_to_state()\n        actions =  self.possible_actions(current_state)\n        \n        best_action = []\n        best_action_value = -np.Inf\n        \n        for action in actions:\n            Q_s_a = Q[current_state][action]\n            if Q_s_a == best_action_value:\n                best_action.append(action)\n            elif Q_s_a &gt; best_action_value:\n                best_action = [action]\n                best_action_value = Q_s_a\n        best_action = random.choice(best_action)\n\n        if action_type == 'greedy':\n            return self.s1_to_b2[best_action]\n        else:\n            assert eps != None , \"Include epsilon parameter\"\n            n_actions =len(actions) #No of legal actions \n            p = np.full(n_actions,eps/n_actions)\n            #Get index of best action\n            best_action_i = actions.index(best_action)\n            p[best_action_i]+= 1 - eps\n            return self.s1_to_b2[np.random.choice(actions,p=p)]\n    \n                  \nBelow is a demonstration of how this class works. Let us assume the we are training Player X to play the game.\nt_board_X = TicTacToe(player = 'X',reward_type ='goal_reward')\nt_board_X.show_board()\n##     0   1   2\n## 0  __  __  __\n## 1  __  __  __\n## 2  __  __  __\nWe will alternate between moves for X(player) and O(opponent) until the game ends with player X winning\nt_board_X.my_move((0,0))\n## 0\nt_board_X.show_board()\n##     0   1   2\n## 0   X  __  __\n## 1  __  __  __\n## 2  __  __  __\nt_board_X.opponent_move((0,1))\nt_board_X.show_board()\n##     0   1   2\n## 0   X   O  __\n## 1  __  __  __\n## 2  __  __  __\nt_board_X.my_move((1,0))\n## 0\nt_board_X.opponent_move((1,1))\nt_board_X.my_move((2,0))\n## 1\nt_board_X.show_board()\n##    0   1   2\n## 0  X   O  __\n## 1  X   O  __\n## 2  X  __  __"
  },
  {
    "objectID": "blog/2021-08-14-solving-tic-tac-toe-with-reinforcement-learning.en/index.html#utility-functions",
    "href": "blog/2021-08-14-solving-tic-tac-toe-with-reinforcement-learning.en/index.html#utility-functions",
    "title": "Solving Tic-Tac-Toe with Reinforcement Learning",
    "section": "Utility Functions",
    "text": "Utility Functions\nThe following primary utility functions will be used. Please refer to the jupyter notebook for the definitions of these functions\n\nplay_games :This function simulates games between the two players a specified number of times and returns relevant statistics from the game \nget_win_statistics : This functions simulates the specified number of games N times(equivalent to sets in a tennis game) and returns statistics collected across these sets \nplot_results : This function visualizes the statistics collected \ninitialize_Q : Randomly initialize a Q table for the given player \ntrain: Function to train the agent using the Q-Learning algorithm"
  },
  {
    "objectID": "blog/2021-08-14-solving-tic-tac-toe-with-reinforcement-learning.en/index.html#experiments",
    "href": "blog/2021-08-14-solving-tic-tac-toe-with-reinforcement-learning.en/index.html#experiments",
    "title": "Solving Tic-Tac-Toe with Reinforcement Learning",
    "section": "Experiments",
    "text": "Experiments\nI will run the following experiments to evaluate how well the agent has learned\n\nRandom X vs Random O \nTrained X vs Random O \nRandom X vs Trained O \nTrained X vs Trained O \nRe-trained X vs Trained O\n\n\nRandom X vs Random O\nAs a baseline, let us see how the results look like when both players follow a random policy. We will maintain two separate boards for each of the players.\nt_board_X = TicTacToe(player = 'X',reward_type ='goal_reward')\nt_board_O = TicTacToe(player = 'O',reward_type ='goal_reward')\nWe will first enumerate the states for both sets of players.\nStates_X = t_board_X.my_states\nStates_O = t_board_O.my_states\nCreate a Q-table for both.\nQ_X = initialize_Q(States_X)\nQ_O = initialize_Q(States_O)\nLet us see what the results look like over 1000 games.\nwin_statistics = get_win_statistics(Q_X, Q_O,sets = 10, games_in_set = 100, X_strategy ='eps_greedy',O_strategy='eps_greedy',eps_X=1.0,eps_O=1.0)\n# Setting eps = 1.0 ensures purely random policy\nplot_results(win_statistics)\n\nIt seems like X typically wins around 60% of games.\nAs expected X wins more games than O as it gets to start first and make more moves than O. For both players, occupying the central square in the first move maximizes the chances of winning.\nFurther, for both players, the winning sequence is most likely to be along the diagonal.\n\n\nTrained X vs Random O\nNow we will train X to play optimally against a random O.\nnp.random.seed(1)\nQ_X,_,rewards_X,rewards_O = train(n_games=5000,alpha = 0.5, gamma = 0.9,train_X=True,train_O=False,is_random=True)\n\nQ_X_trained = Q_X #Save trained X\nThe learning curve indicates the training has converged after around 4000 games.\nwin_statistics = get_win_statistics(Q_X_trained,Q_O,sets = 5, games_in_set = 100,X_strategy = 'greedy',O_strategy='eps_greedy',eps_X=1.0,eps_O=1.0)\nplot_results(win_statistics)\n\nThese results indicate that player X has learned to easily beat a random player O. Player X consistently chooses the top right hand box and seem to win the majority of games through the right most column (C3) or the off diagonal(D2).\n\n\nRandom X vs Trained O\nNow we will train O to play against a random X\nnp.random.seed(1)\n_,Q_O,rewards_X,rewards_O = train(n_games=20000,alpha = 0.5, gamma = 0.5,train_X=False,train_O=True,is_random=True)\n\nQ_O_trained = Q_O #Save trained O\nThe learning curve indicates training has converged after about 15000 games.\nwin_statistics = get_win_statistics(Q_X,Q_O_trained,sets = 10, games_in_set = 100,X_strategy = 'eps_greedy',O_strategy='greedy',eps_X=1.0,\n                   eps_O=1.0)\nplot_results(win_statistics)\n\nThe above results indicate that Player O has learned to consistently beat the random player X. What is interesting to note is that the RL approach results in Player O beating Player X more consistently (85%) of the time than when using the Minimax approach (~ 81%).\nWe can also see the player O consistently picks boxes along the leading diagonal as its first move. Most of its wins come from occupying the leading diagonal(D1) or the middle column(C2).\n\n\nTrained X vs Trained O\nNow we will pit the two trained players against each other.\nwin_statistics = get_win_statistics(Q_X_trained,Q_O_trained,sets = 10, games_in_set = 100,X_strategy = 'greedy',O_strategy='greedy',eps_X=1.0,\n                   eps_O=1.0)\nplot_results(win_statistics)\n\nWhen the two trained agents face off, all games end in ties.\n\n\nRetrained X vs Trained O\nInitially we trained X against a random O, now we will retrain X against a trained O\nnp.random.seed(1)\nQ_X,Q_O,rewards_X,rewards_O = train(n_games=1000,alpha = 0.5, gamma = 0.9,train_X=True,train_O=False,is_random=False,Q_O = Q_O_trained)\n\nQ_X_retrained = Q_X\nIn this case , given player O is following a deterministic policy, training converges in just 500 games.\nwin_statistics= get_win_statistics(Q_X_retrained,Q_O_trained,sets = 10, games_in_set = 100,X_strategy = 'greedy',\\\n                                   O_strategy='greedy',eps_X=1.0,eps_O=1.0)\nplot_results(win_statistics)\n\nThe re-trained player X beats the trained Player in 100% of the games."
  },
  {
    "objectID": "blog/2021-08-14-solving-tic-tac-toe-with-reinforcement-learning.en/index.html#conclusion",
    "href": "blog/2021-08-14-solving-tic-tac-toe-with-reinforcement-learning.en/index.html#conclusion",
    "title": "Solving Tic-Tac-Toe with Reinforcement Learning",
    "section": "Conclusion",
    "text": "Conclusion\nReinforcement learning is a powerful paradigm in AI that can potentially be the key to solving several real world problems. Although the early days of RL has seen an almost exclusively focus on games, there are several practical applications of RL outside of games that industry is working on. At Oracle, we are working on a potentially category defining product that uses reinforcement learning at its core. Watch this space for more!"
  },
  {
    "objectID": "blog/2022-04-02-mvp-vs-mlp-mdp/index.html",
    "href": "blog/2022-04-02-mvp-vs-mlp-mdp/index.html",
    "title": "MVP vs MLP & MDP",
    "section": "",
    "text": "When I got my first PM job in 2019, I immediately purchased some popular Product Management books, including Marty Cagan’s “Inspired” and Dan Olsen’s “Lean Product Playbook”. I even made detailed notes for the latter. See here.\nI loved the concept of the MVP (Minimum Viable Product) and immediately socialized the idea with the team. Creating products that provided minimal end to end functionality and iterating based on feedback made total sense.\nHowever, I have learned that the idea of an MVP may actually hurt rather than help in certain contexts. If you are a scrappy startup trying to get to product market fit, the idea makes sense. If you are an established company serving enterprise customers in highly regulated domains like healthcare or financials services, the concept is far less useful.\nTypically, these enterprises are buying expensive software licenses for mission critical applications solving well understood problems. They are interested in software that is fully functional rather than minimally viable.\nMVPs are suitable when you get continuous feedback, allowing you to iterate rapidly. This is the case for consumer focused companies and many B2B SaaS startups. If you have a quarterly release cycle as many larger enterprises delivering complex software do, getting feedback once in a quarter simply does not allow you to iterate fast enough.\nIn these contexts, MVP driven thinking can have a more deleterious impact. It becomes an excuse for doing just enough, thinking short term and not challenging the status quo. Instead of MVP, we should be thinking MDP (Maximally Delightful Product) and MLP (Minimum Lovable Product).\nWhen faced with a problem, I now encourage my team to think of the maximally delightful experience we can craft for the user. We need to forget about resource and technology limitations. This divergent thinking can help us identify solutions we would never consider with an MVP mindset.\nOnce we have imagined these possibilities, we can then focus on crafting a Minimum Lovable Product that has the critical ’must have” features, the performance features that are integral to our product strategy as well as one or two features that delight.\nAs Brian Chesky1 says, when you have imagined an 11-star experience, crafting a 5-star or 6-star experience seems attainable.\nConstraints can breed creativity, but if you think about constraints too early, you are simply knee capping your imagination, which leads to mediocre solutions."
  },
  {
    "objectID": "blog/2022-04-02-mvp-vs-mlp-mdp/index.html#footnotes",
    "href": "blog/2022-04-02-mvp-vs-mlp-mdp/index.html#footnotes",
    "title": "MVP vs MLP & MDP",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nhttps://mastersofscale.com/brian-chesky-handcrafted/↩︎"
  },
  {
    "objectID": "blog/2021-05-02-a-non-resident-non-immigrant-legal-alien/index.html",
    "href": "blog/2021-05-02-a-non-resident-non-immigrant-legal-alien/index.html",
    "title": "A non resident, non immigrant, legal alien",
    "section": "",
    "text": "I am a non-resident, non-immigrant, legal alien. This is not a catchy title I gave myself. It is my legal status in the United States.\nAs a beneficiary of the contentious H1B visa I have worked in the Unites States since October 2012, only interrupted by a brief stint as a Master’s student under the F1 visa.\nThis work authorization allows me to work in the United States as long as I renew it every three years. There is some level of uncertainty around this as my renewal application could be rejected, sometimes for frivolous reasons. My wife is also allowed to work for the duration of this work permit under the H4-EAD scheme that was introduced by former President Barack Obama.\nMy visa as well as my wife’s work authorization is up for renewal in July 2022. If the renewal doesn’t come through by July 2022, she will not be able to continue working. There is some risk of this happening as H4 EDA renewals can currently take as long as 10 months.\nThis is all small talk before addressing the elephant in the room. Since I was born in India, I will not get a green card in my life time. If I were born anywhere else with the exception of China, I would have gotten my green card circa 2018.\nThis is despite the fact that I pay my taxes regularly and contribute to the Social Security Administration Fund (a benefit I will never enjoy) . Shouldn’t I feel aggrieved?\nNO!\nThe truth I have completely accepted is this - The United States owes me absolutely nothing! I have no claim on this country beyond the right to stay here until Jul 2022 and enjoy any rights that come with it until that date.\nPaying taxes or contributing to SSA is the price of admission. If I don’t like it, I am free to eave. I have already gotten a fairly good return on it through the Chicago and NY Public library systems and two dozes of the Pfizer vaccine.\nIt is entirely up to the United States to decide what kind of people it wants to attract and keep. I am neither an outstanding researcher pushing the frontiers of science forward nor an entrepreneur creating thousands of jobs. I am just a mid-level employee at a tech company with a few valuable skills, but hardly someone who is irreplaceable.\nIf the United States thinks I am fungible, they are probably right. If the Democratic administration prioritizes the welfare of undocumented immigrants over ‘non-resident, non-immigrant, legal aliens’ like myself, it is understandable. Protecting the most vulnerable should take precedence over the welfare of the privileged- and I am privileged. In fact, I would do fairly well in my home country unlike most undocumented immigrants who are fleeing violence and hunger.\nI have been incredibly lucky; I am pretty sure there are billions of people who would happily trade places with me. I was fortunate enough to be born to educated, middle class parents. I was lucky enough to go to good schools, work in the US and save money that allowed me to do my Master’s without going into debt. I have been lucky to continue working in the US after this and build up a little nest egg.\nGiven how lucky I have been, it would be churlish to complain about the US immigration system not working in my favor. In fact, if USCIS (United States Citizenship and Immigrant Services) asks me to pack my bags and leave tomorrow - the only thing I believe I am entitled to say is: “So Long, and Thanks for all the fish!”"
  },
  {
    "objectID": "blog/index.html",
    "href": "blog/index.html",
    "title": "Blog",
    "section": "",
    "text": "Is the Commoditization of AI inevitable ?\n\n\n\n\n\n\nMusings\n\n\nAI\n\n\n\nAn analysis of the economic and societal forces that could shape the industry\n\n\n\n\n\nDec 6, 2024\n\n\n\n\n\n\n\nImportance of Vision - Or is it Mission ?\n\n\n\n\n\n\nMusings\n\n\nBusiness\n\n\nProduct Management\n\n\n\nThoughts on Mission vs vision and why this idea matters\n\n\n\n\n\nOct 19, 2024\n\n\n\n\n\n\n\nExplanations, Causality and Theories - An AML Perspective\n\n\n\n\n\n\nArticle\n\n\nAnti Money Laundering\n\n\nR\n\n\nStatistics\n\n\n\nBringing scientific rigor to the practice of AML\n\n\n\n\n\nJul 13, 2024\n\n\n\n\n\n\n\nMeta’s Anti-fragility\n\n\n\n\n\n\nMusings\n\n\nDecision Making\n\n\nStrategy\n\n\nBusiness\n\n\n\nExploring How Meta consistently turns challenges into strengths\n\n\n\n\n\nMar 16, 2024\n\n\n\n\n\n\n\nBase Rate Neglect in AML\n\n\n\n\n\n\nArticle\n\n\nAnti Money Laundering\n\n\nProbability\n\n\nR\n\n\n\n A Probabilistic approach to AML Detection\n\n\n\n\n\nFeb 10, 2024\n\n\n\n\n\n\n\nThe German Tank Problem\n\n\n\n\n\n\nArticle\n\n\nStatistics\n\n\nR\n\n\nProbability\n\n\n\n Frequentist and Bayesian Solutions to the German Tank Problem\n\n\n\n\n\nOct 15, 2023\n\n\n\n\n\n\n\nApple and Meta’s Strategic Clarity\n\n\n\n\n\n\nMusings\n\n\nStrategy\n\n\n\nAnalyzing the AR and VR strategy of Apple and Meta\n\n\n\n\n\nJul 9, 2023\n\n\n\n\n\n\n\nThe Two Faced PM\n\n\n\n\n\n\nMusings\n\n\nProduct Management\n\n\n\nBalancing optimism and skepticism as a product manager\n\n\n\n\n\nApr 7, 2023\n\n\n\n\n\n\n\nIntroduction to Graph Machine Learning\n\n\n\n\n\n\nArticle\n\n\nAI\n\n\nPython\n\n\nMachine Learning\n\n\n\nCode first Intro into Graph ML\n\n\n\n\n\nMar 11, 2023\n\n\n\n\n\n\n\nEvaluating and Improving Transaction Monitoring Systems - Is there a better way ?\n\n\n\n\n\n\nArticle\n\n\nAnti Money Laundering\n\n\nAI\n\n\n\nRethinking AML Transation Monitoring\n\n\n\n\n\nDec 11, 2022\n\n\n\n\n\n\n\nMVP vs MLP & MDP\n\n\n\n\n\n\nMusings\n\n\nProduct Management\n\n\n\nThinking beyond MVPs\n\n\n\n\n\nApr 2, 2022\n\n\n\n\n\n\n\nOracle MLRG Talks\n\n\n\n\n\n\nConferences\n\n\nAI\n\n\nMachine Learning\n\n\n\nSummary of Oracle MLRG Talks\n\n\n\n\n\nFeb 19, 2022\n\n\n\n\n\n\n\nWhen Success breeds Failure: The cautionary tale of Manchester United\n\n\n\n\n\n\nArticle\n\n\nStrategy\n\n\nDecision Making\n\n\n\nBreaking down what has gone wrong at Manchester United\n\n\n\n\n\nFeb 5, 2022\n\n\n\n\n\n\n\nHow not to be unhappy\n\n\n\n\n\n\nMusings\n\n\nPersonal Growth\n\n\n\n Becoming happy by addressing unhappiness\n\n\n\n\n\nDec 24, 2021\n\n\n\n\n\n\n\nLearning Julia\n\n\n\n\n\n\nMusings\n\n\nProgramming\n\n\nJulia\n\n\n\nFirst Steps with Julia\n\n\n\n\n\nOct 23, 2021\n\n\n\n\n\n\n\nSolving Tic-Tac-Toe with Reinforcement Learning\n\n\n\n\n\n\nArticle\n\n\n\nSolving Tic-Tac-Toe with RL \n\n\n\n\n\nAug 14, 2021\n\n\n\n\n\n\n\nThe Profound Implications of Gene Editing\n\n\n\n\n\n\nMusings\n\n\n\nThe Awesome consequences of gene editing\n\n\n\n\n\nJun 26, 2021\n\n\n\n\n\n\n\nSolving Tic-Tac-Toe with Minimax\n\n\n\n\n\n\nArticle\n\n\nAI\n\n\nAlgorithms\n\n\nPython\n\n\n\nSolving Tic Tac Toe with Minimax\n\n\n\n\n\nJun 19, 2021\n\n\n\n\n\n\n\nStanford MLSys Seminar Series\n\n\n\n\n\n\nConferences\n\n\nMachine Learning\n\n\nAI\n\n\nSoftware Engineering\n\n\n\nNotes from Stanford ML Sys Seminar series\n\n\n\n\n\nMay 9, 2021\n\n\n\n\n\n\n\nA non resident, non immigrant, legal alien\n\n\n\n\n\n\nMusings\n\n\nImmigration\n\n\nUSA\n\n\n\n \n\n\n\n\n\nMay 2, 2021\n\n\n\n\n\n\n\nA Crisis is a terrible thing to waste\n\n\n\n\n\n\nMusings\n\n\nCOVID\n\n\nEconomics\n\n\n\nHow the restaurant industry responded to crisis\n\n\n\n\n\nApr 11, 2021\n\n\n\n\n\n\n\nThe Secretary Problem\n\n\n\n\n\n\nArticle\n\n\nMath\n\n\nProbability\n\n\nDecision Making\n\n\n\nSolving the Secretary Problem\n\n\n\n\n\nApr 3, 2021\n\n\n\n\n\n\n\nObviously Awesome\n\n\n\n\n\n\nBook Summary\n\n\nProduct Management\n\n\nProduct Marketing\n\n\n\nSummary of the Book - Obviously Awesome by April Dunford\n\n\n\n\n\nMar 21, 2021\n\n\n\n\n\n\n\nHealth is the greatest gift\n\n\n\n\n\n\nMusings\n\n\nHealth\n\n\n\nThoughts on Health and HealthCare\n\n\n\n\n\nMar 20, 2021\n\n\n\n\n\n\n\nUnderstanding the Biology of Covid and Covid vaccines\n\n\n\n\n\n\nArticle\n\n\nBiology\n\n\nCOVID\n\n\n\nOverview of Covid and Covid vaccines\n\n\n\n\n\nJan 30, 2021\n\n\n\n\n\n\n\nSVD is (almost) all you need\n\n\n\n\n\n\nArticle\n\n\nMachine Learning\n\n\nStatistics\n\n\n\nIntroduction to Singular Value Decomposition\n\n\n\n\n\nJan 10, 2021\n\n\n\n\n\n\n\nUndestanding Michael Porter\n\n\n\n\n\n\nBook Summary\n\n\nStrategy\n\n\n\nSummary of the book - Understanding Michael Porter \n\n\n\n\n\nDec 19, 2020\n\n\n\n\n\n\n\nThe Ancients and Astronomy\n\n\n\n\n\n\nArticle\n\n\nScience\n\n\n\nHow the ancients used simple observations and geometry to understand our place in the solar system\n\n\n\n\n\nOct 30, 2020\n\n\n\n\n\n\n\nThe Labeled Data Problem in AML\n\n\n\n\n\n\nArticle\n\n\nAnti Money Laundering\n\n\nMachine Learning\n\n\n\nData Challenges in the AML Industry\n\n\n\n\n\nSep 5, 2020\n\n\n\n\n\n\n\nCrossing the Chasm\n\n\n\n\n\n\nBook Summary\n\n\nStrategy\n\n\nProduct Management\n\n\n\nBook Summary for the book Crossing the Chasm\n\n\n\n\n\nAug 1, 2020\n\n\n\n\n\n\n\nShould you rebalance an imbalanced dataset?\n\n\n\n\n\n\nArticle\n\n\nMachine Learning\n\n\nR\n\n\n\nImpact of rebalancing an imbalanced dataset\n\n\n\n\n\nJun 14, 2020\n\n\n\n\n\n\n\n2019 Oreilly AI Conference\n\n\n\n\n\n\nConferences\n\n\nMachine Learning\n\n\nStrategy\n\n\n\nNotes from the 2019 Oreilly AI Conference\n\n\n\n\n\nJun 6, 2020\n\n\n\n\n\n\n\nModel Uncertainity\n\n\n\n\n\n\nArticle\n\n\nR\n\n\nMachine Learning\n\n\n\nIncorporating Uncertainty in Model Predictions\n\n\n\n\n\nMay 30, 2020\n\n\n\n\n\n\n\nLearning with Noisy Labels\n\n\n\n\n\n\nArticle\n\n\nMachine Learning\n\n\nPython\n\n\n\nUsing cleanlab for learning with Noisy labels\n\n\n\n\n\nMay 24, 2020\n\n\n\n\n\n\n\nPutting ML into AML\n\n\n\n\n\n\nArticle\n\n\nAnti Money Laundering\n\n\nStrategy\n\n\n\nMachine Learning in the AML Industry\n\n\n\n\n\nMay 16, 2020\n\n\n\n\n\n\n\nDetecting Concept Drift\n\n\n\n\n\n\nArticle\n\n\nMachine Learning\n\n\nR\n\n\n\nMethods to Detect Concept Drift in supervised models.\n\n\n\n\n\nMay 9, 2020\n\n\n\n\n\n\n\nNever Split the Difference\n\n\n\n\n\n\nBook Summary\n\n\n\nSummary of Book : Never Split the Difference\n\n\n\n\n\nMay 9, 2020\n\n\n\n\n\n\n\nA Citizen’s Guide to the Economics of A City\n\n\n\n\n\n\nBook Summary\n\n\nEconomics\n\n\n\nA summary of the book ‘A Citizen’s Guide to the Economics of A City.’\n\n\n\n\n\nApr 4, 2020\n\n\n\n\n\n\n\nIntroduction to Gaussian Processes\n\n\n\n\n\n\nArticle\n\n\nMachine Learning\n\n\nR\n\n\n\nAn Introduction to Gaussian Processes for Beginners\n\n\n\n\n\nAug 11, 2019\n\n\n\n\n\n\n\nR Studio Conference 2019\n\n\n\n\n\n\nConferences\n\n\nR\n\n\n\nHighlights from the 2019 R Studio Conference\n\n\n\n\n\nMar 17, 2019\n\n\n\n\n\n\n\nIntroduction to Bayesian Methods\n\n\n\n\n\n\nArticle\n\n\nR\n\n\nStatistics\n\n\n\nAn Introduction to Bayesian Methods for Beginners\n\n\n\n\n\nFeb 21, 2019\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/2021-03-20-health-is-the-greatest-gift/index.html",
    "href": "blog/2021-03-20-health-is-the-greatest-gift/index.html",
    "title": "Health is the greatest gift",
    "section": "",
    "text": "“A healthy person wants a thousand different things; A sick person wants only one thing” – Naval\n\nThis past year was a painful reminder of how fragile life and health are, how all the trivialities I am privileged to worry about – am I going to get promoted? will I receive a raise? will my visa be renewed? – can quickly become utterly inconsequential.\nIn the last 12 months,\n• My manager passed away after bravely fighting cancer for over 25 years  • I found out a co-worker at my previous company is in palliative care fighting terminal cancer  • I learned a friend was fighting multiple sclerosis  • A distant family friend passed away succumbing to pancreatic cancer  • I learned that a former co-worker is fighting Myasthenia Gravis \nIt is important to note that in the cases above, there is nothing any of these individuals could have done to prevent what happened. While acknowledging that individuals do bear responsibility for looking after their own health, it is evident that our Creator does love playing dice with our health and lives.\nMore depressingly, of the five cases listed above, three individuals had to raise money through online campaigns to help fund the cost of healthcare. What kind of society are we if we have to rely solely on the kindness of strangers to nurse our loved ones back to health?\nA few things I have resolved to do on the back of all this.\n• Be grateful for my health every day.  • Even though there is no escaping the whims of fate, eat healthy and exercise regularly to minimize any regrets I might have in the future.  • Don’t skimp and save on things that could affect your health. Spending on health and education are investments that can return many times over  • Listen more keenly to politicians and policy makers who are committed to providing universal health coverage"
  },
  {
    "objectID": "blog/2023-04-07-the-two-faced-pm/index.html",
    "href": "blog/2023-04-07-the-two-faced-pm/index.html",
    "title": "The Two Faced PM",
    "section": "",
    "text": "A Product Manager is responsible for building products that delivers value to customers while meeting the goals of the business. As a product manager who works in B2B, I am often called on to support sales and pre-sales team in calls with prospective customers.\nNow, enterprise selling was a completely new beast that I needed some time to grok. As Steve Sinofsky writes here:\nSelling to enterprise is as much about demonstrating the capabilities of your product today as it is about sketching out the vision of what it will become in the quarters to come and inspiring confidence in that vision."
  },
  {
    "objectID": "blog/2023-04-07-the-two-faced-pm/index.html#a-salespersons-optimism",
    "href": "blog/2023-04-07-the-two-faced-pm/index.html#a-salespersons-optimism",
    "title": "The Two Faced PM",
    "section": "A Salesperson’s Optimism",
    "text": "A Salesperson’s Optimism\nOne thing I have consistently seen in salespeople is their optimism. To be able to sell a product, you need to believe in the product. Only then can you convince the customer that it is the right solution for them. The best salespeople are able to sell the broader vision for the solution within the enterprise, even while overlooking certain limitations in the short term."
  },
  {
    "objectID": "blog/2023-04-07-the-two-faced-pm/index.html#a-product-managers-skepticism",
    "href": "blog/2023-04-07-the-two-faced-pm/index.html#a-product-managers-skepticism",
    "title": "The Two Faced PM",
    "section": "A Product Manager’s Skepticism",
    "text": "A Product Manager’s Skepticism\nA product manager should be relentlessly focused on improving a product. Being skeptical about the product and the value it offers is essential. Only this skepticism and self-awareness can spur improvements in the product."
  },
  {
    "objectID": "blog/2023-04-07-the-two-faced-pm/index.html#finding-a-balance",
    "href": "blog/2023-04-07-the-two-faced-pm/index.html#finding-a-balance",
    "title": "The Two Faced PM",
    "section": "Finding a Balance",
    "text": "Finding a Balance\nAs a data scientist turned product manager, I tend to be super critical of the products I work on, and I tend to see flaws and areas for improvement everywhere. This often means I am prone to saying something that may not be very constructive in a sales call. You often need to look at the glass as half full rather than half empty in these instances.\nIf you are coming from a background in sales, you might be susceptible to the opposite problem. You are so used to seeing the glass half full that you can overlook the gaps and potential weaknesses in your product, your strategy, or your vision. An extra dose of skepticism here is essential.\nAs a PM, finding the balance between the sales mindset and the product mindset; between optimism and skepticism; is critical. You might have to switch between the two within the same day. You might have to be a skeptic in your quarterly retrospective in the morning while being an optimist in a sales call in the afternoon.\nThe optimal mix of these two can depend very much on your role and company. If you are a PM in a B2B company with a sales led go to market, becoming comfortable with this sales mindset will be critical if you want to thrive."
  },
  {
    "objectID": "blog/2020-12-19-undestanding-michael-porter.en/index.html",
    "href": "blog/2020-12-19-undestanding-michael-porter.en/index.html",
    "title": "Undestanding Michael Porter",
    "section": "",
    "text": "Only by competing to be unique can an organization achieve sustained superior performance\nCompetitive advantage is about how your value chain will be different and your P&L better than the industry average\nA distinctive value proposition will translate into meaningful strategy only if the activities to deliver it is different from the activities performed by rivals\nAim to be unique,not best. Creating value, not beating rivals is the heart of competition\nStrategy is shorthand for a good competitive strategy that will result in sustainable superior performance\nCompetition to be the best leads to flawed strategy and mediocre performance\nWhen all rivals compete on the same dimension, no one gains a competitive advantage\nFocus on creating superior value for the chosen customers, not imitating and matching rivals\nCompeting to be the best feeds on imitation. Competing to be unique thrives on innovation"
  },
  {
    "objectID": "blog/2020-12-19-undestanding-michael-porter.en/index.html#porters-five-forces",
    "href": "blog/2020-12-19-undestanding-michael-porter.en/index.html#porters-five-forces",
    "title": "Undestanding Michael Porter",
    "section": "Porter’s Five Forces",
    "text": "Porter’s Five Forces\nThe following five forces determine an industry’s structure\n\nIntensity of rivalry among existing competitors \nBargaining power of buyers \nBargaining power of suppliers \nThreat of substitutes \nThreat of new entrants \n\nThe fundamental Equation of Business:\n\\[ Unit\\ Profit\\ Margin = Price - Cost \\]\nWithin a given industry, the relative strength of the fiver forces and their specific configuration determine the industry’s profit potential because they directly impact the industry’s prices and costs\n\n\n\nFigure 1: Impact of the 5 Forces\n\n\n\nBuyers\n\nPowerful buyers will force prices down or demand more value in the product thus capturing more value for themselves\n\nE.g. Walmart can impose tough terms on any supplier,\n\nIndustrial and retail consumers tend to be more price sensitive when what they are buying is\n\nundifferentiated\nexpensive relative to their other costs or incomes\ninconsequential to their own performance\n\n\n\n\nSuppliers\n\nPowerful suppliers will charge higher prices or insist on more favorable terms, lowering industry profitability E.g. Intel and Microsoft being suppliers of chips and software captured most of the value from PC manufacturers\n\n\nStrength of Buyers and Suppliers\nBuyers and Suppliers tend to be powerful if\n\nThey are large and concentrated relative to an industry (which they supply or buy from) that is fragmented.  If a large fraction of an industry’s sales comes from a few suppliers or buyers.\nThe industry needs them more than they need the industry . E.g. Doctors have tremendous bargaining power because they are essential and in short supply\nSwitching costs work in their favor.  Microsoft is a powerful supplier as it is difficult for an enterprise to move away from WIndows. Airline Passengers are powerful buyers as they can easily switch between airlines.\nDifferentiation works in their favor.  Intel and Microsoft have differentiated offerings and are more powerful than the PC industry they supply.\nThey can threaten to vertically integrate into producing the industry’s product itself.\n\nE.g. Apple has been a powerful buyer of Intel chips and have now started designing their own chips.\n\n\n\nSubstitutues\n\nProducts or services that meet the same basic need as the industry’s product in a different way put a cap on industry profitability\n\nE.g. Tax preparation software such as turbo tax is a substitute for a professional tax preparer such as H&R Block\n\n\nNew Entrants\nEntry barriers protect an industry from newcomers who would add new capacity.\nEntry barriers can be one of the following\n\nEconomies of scale\n\nNetflix is able to spread the cost of developing new content over a large customer base and keep prices low. New entrants will have to charge much higher prices to able to make similar investment in content.\n\nSwitching costs\nNetwork effects\n\nIt is difficult to start a new social network to compete with Facebook if all your friends are already on Facebook\n\nCost of entering a business such as capital investment required.\n\nDrug companies don’t have to worry about new entrants as it requires massive investments to develop new drugs.\n\nUnique advantages that incumbents enjoy such as proprietary technology, brand,prime location and access to distribution channels\nRegulations Regulatory compliance software that has been blessed by regulators makes it challenging for a new start up to dislodge the incumbent. Banks are reluctant to experiment with something new rather than an established player.\nHow aggressively can an incumbent defend its position, does it have the resources to compete aggressively ?\n\n\n\nRivalry\nIf rivalry is intense, companies compete away the value they create, passing it on to buyers in lower prices or dissipating it in higher costs of competing.\nIntensity of rivalry in an industry can be assessed by considering the following\n\nAre competitors of even size and power. Can an industry leader enforce practices that align the whole industry?\nIs the industry growing? If not competitors have to fight to get a bigger share of a fixed pie.\nHigh exit barriers such as specialized assets that can’t be sold that prevent a competitor from exiting the industry\nIrrational commitments such a state driven enterprise run for national pride as against profit.\n\nPrice competition is the most damaging for of rivalry and is most common when\n\nNo differentiation in competitor’s offerings\nHigh fixed costs and low marginal costs creates pressure to drop prices because new customers will “contribute to covering overhead”. True for airlines and software but software is highly differentiated.\n\n\n\n\nFigure 2: Impact of the 5 Forces on Profitability\n\n\n\nWhen there are differences in more than one force, or where differences in any one force is large, you are likely dealing with distinct industries\nStrategy can be viewed as building defenses against competitive forces or finding a position in the industry where the forces are weakest. e.g. Paccar trucks which had an ROIC of 30.6% compared to the industry average of 10.5%. It accomplished this by uniquely positioning trucks for individual owner operator truck drivers."
  },
  {
    "objectID": "blog/2020-12-19-undestanding-michael-porter.en/index.html#competitive-advantage",
    "href": "blog/2020-12-19-undestanding-michael-porter.en/index.html#competitive-advantage",
    "title": "Undestanding Michael Porter",
    "section": "Competitive Advantage",
    "text": "Competitive Advantage\n\nIf you have a real competitive advantage, it means that compared with rivals, you operate at lower cost, command a premium price or both.\nCompetitive advantage is superior performance resulting from sustainable higher prices, lower costs or both\nIn gauging competitive advantage, returns must be measured relative to other companies within the same industry\nThe best financial measure of competitive advantage in Return on Invested Capital (ROIC).ROIC weights the profits a company generates versus all the funds invested in it, operating expenses and capital.\nFor higher profitability you need to be able to\n\n\nCharge a higher price. Differentiation refers to the ability to charge a higher relative price. By creating more buyer value, you raise their willingness to pay (WTP)E.g. Apple\n\n\nProduce at a lower cost. Cost advantages stem from a culture of low cost operation that permeates the entire company e.g. IKEA\n\n\n\nCompetitive advantage stems from having a superior and differentiated value chain where value chain is the sequence of activities a company performs to design, produce, sell, deliver and support it’s products. Your activities along with that of your suppliers, channels and customers creates a value system.\n\nValue Chain Analysis\nLayout the major value creating activities specific to your industry. Do the same for different business models and competitors\n\n\n\nFigure 3: Value Chain for NGOs supplying wheel chairs\n\n\n\nIf your value chain looks like everybody else’s , you don’t have a competitive strategy and are engaged in competition to be the best\nZero in on price drivers ,those activities that have a high current or potential impact on differentiation. See if you can create value by performing any of these activities distinctively or by performing activities competitors are not performing E.g. Source only fresh ingredients like In-N-Out burger\nZero in on cost drivers, especially activities that represent a large or growing percentage of costs. See if you can do these activities more efficiently. E.g. Southwest Airlines identified gate turnaround times are a major cost driver , and these turn around times are high because of the time it takes to drain the lavatories. Southwest solved this problem by getting the plane supplier Boeing to reposition the service panel where the equipment to drain the lavatory is plugged into.\nMatch the vales chain and activities performed by the company to the customer’s definition of value, an activity that does not add value to the end customer can be eliminated. E.g. Charles Schwab reduced brokerage fees by doing away with investment advice and offering just low cost trades.\nValue chains can span across organizations, so designing and managing the value chain across the entire supply chain from suppliers to distributors is critical to developing a successful competitive strategy."
  },
  {
    "objectID": "blog/2020-12-19-undestanding-michael-porter.en/index.html#creating-value",
    "href": "blog/2020-12-19-undestanding-michael-porter.en/index.html#creating-value",
    "title": "Undestanding Michael Porter",
    "section": "Creating Value",
    "text": "Creating Value\n\nTo have sustainable competitive advantage, you must tailor a value chain specifically to deliver your value proposition. Strategy means deliberately choosing a unique mix of activities to deliver a unique set of values.\nValue proposition is the answer to the following three fundamental questions\n\nWhich customers are you going to serve?\n\nWhat end user users?\nWhat channels?\n\nWalmart focused on rural customers initially\nProgressive focused on “non standard” drivers more likely to be involved in an accident\n\n\nWhich needs are you going to meet ?\n\nWhich products?\nWhich features?\nWhich services?\n\nThe needs you choose to address means the customer segment you are targeting is non-standard.\nEnterprise chose to address needs of city dwellers at affordable prices unlike Hertz who focused on travelers willing to pay a premium price.\nZipcar addresses needs of city dwellers who choose not to own a car and provided hourly pricing unlike Enterprise’s daily pricing\n\n\nWhat relative price?\n\nPremium or Discount?\nWhen customer needs are underserved, you can have premium pricing. E.g. Bang & Olufsen serves customers who want great sound and design.\nWhen customers are overserved, you can have discounting by serving only the critical and essential needs. E.g. Southwest offers lots of flights at lower price and with convenient service.\n\n\nIf you are trying to serve the same customers and meet the same needs and sell at the same relative price, then by Porter’s definition, you don’t have a strategy.\nThe essence of strategy and competitive advantage lies in choosing to perform activities differently or to perform different activities from those of rivals.\n\nE.g. To serve city dwelling customers at a lower price points, enterprise chose small offices, often simple storefronts spread over a metropolitan area.\nZipcar on the other hand eliminates stores entirely by making the booking experience fully online.\n\nA common strategic mistake is to try and be all things to all customers and be outflanked by cost leaders on one side, who meet just enough customer needs and by differentiators on the other side.\nChoices in the value proposition that limit what a company will do are essential to strategy because that creates the opportunity to tailor activities in a way that best delivers that kind of value\n\n\n\n\nFigure 4: Unique value propistions and unique value chains\n\n\n\nOnly a value proposition that requires a tailored value chain to deliver it can serve as the basis for a robust strategy. This is the first line of defense against rivals.\nThe value proposition and the value chain are the two core dimensions of strategic choice. The value proposition focuses externally on the customer. The value chain focuses internally on operations. Strategy integrates these two sides(supply and demand) together."
  },
  {
    "objectID": "blog/2020-12-19-undestanding-michael-porter.en/index.html#trade-offs",
    "href": "blog/2020-12-19-undestanding-michael-porter.en/index.html#trade-offs",
    "title": "Undestanding Michael Porter",
    "section": "Trade-offs",
    "text": "Trade-offs\n\nCompetitive advantage depends on making choices that are different from those of rivals, on making trade offs\n\nE.g. Taiwan Semiconductor(TSMC) chose to become a manufacturer of chips unlike other players who were integrated device manufacturers(IDM) who did design and manufacturing.\n\n\nReal trade-offs are choices that make strategies sustainable because competitors cannot easily copy them without rolling back or contradicting some of the choices they made.\n\nMcDonald’s attempt to introduce more customization in their menus to match Wendy’s and Burger King but failed as it meant compromising on it’s strategy of a uniform, limited menu which allowed for a fast and efficient process.\nBlockbuster , with it’s 5000 plus local stores was unable to match Netflix mail-order delivery model that was built on the back of 50 plus regional warehouses and a state of the art distributions system\nWhen you try to offer something for everyone, you tend to relax the trade-offs that underpin your competitive advantage.\nUnless executives make trade-offs and deliberately choose not to serve all customers and needs, then they are unlikely to do a good job of serving any customers and needs\n\n\n“Strategy is making trade - offs in competing. The essence of strategy is choosing what not to do” - Michael Porter"
  },
  {
    "objectID": "blog/2020-12-19-undestanding-michael-porter.en/index.html#fit",
    "href": "blog/2020-12-19-undestanding-michael-porter.en/index.html#fit",
    "title": "Undestanding Michael Porter",
    "section": "Fit",
    "text": "Fit\n\nGood strategies depend on the connection among many things, on making interdependent choices\n\nE.g. IKEA uses flat packs for shipping and choose sub-urban locations outside the city. These choices are interdependent. Value of flat packs are amplified by car friendly locations that make it easier for customers to load purchases into the car\n\nFit means that the value or cost of one activity is affected by the way other activities are performed\nThe first kind of fit is basic consistency where each activity is aligned with the company’s value proposition and each contributes incrementally to its dominant themes.\n\nE.g. Zara is focused on quickly responding to changes in fashion trends. So its activities are configured for rapid response.\n\nPlants are located nearby\nIt owns a fleet of trucks to ensure rapid delivery\nInvest in IT speed communications between design and manufacturing\n\n\nA second type of fit occurs when activities complement or reinforce each other. Real synergy is when the value of one activity is reinforced by another.\n\nE.g.: Home depot’s huge selection of items in large warehouses would make customers feel lost if it were not for excellent service.\n\nA third type of fit is substitutions. Performing one activity makes it possible to eliminate another\n\nE.g. IKEA’s full-room displays and product hang-tags substitute for sales associates\n\nA common mistake in strategy is to choose the same core competencies as everyone else in your industry.\nOutsourcing activities that are or could be tailored to strategy and are strongly complementary with others is risky\nFit makes competitive advantage sustainable against new entrants. Replicating a complex system whose parts fit together seamlessly is hard.\n\n\nMapping Activity System\n\nIdentify the core elements of the value proposition e.g. For IKEA, this is distinctive design, low prices and immediate use\nIdentify the salient activities performed in the business, those responsible for creating customer value or generate significant cost\nPlace activities on the map, and draw lines connecting them if an activity contributes to the value proposition or where two activities affect each other. E.g. flat packs contribute to low price and immediate use.\nA tangled dense map suggests a strong strategy, if it is sparse the strategy is probably weak.\nDetermine if there are services, features or product varieties you can offer because of the other things you already do.\n\n\n\n\nFigure 5: Map of IKEA’s Activity System\n\n\n\nStrategy is a system of interconnected choices"
  },
  {
    "objectID": "blog/2020-12-19-undestanding-michael-porter.en/index.html#continuity",
    "href": "blog/2020-12-19-undestanding-michael-porter.en/index.html#continuity",
    "title": "Undestanding Michael Porter",
    "section": "Continuity",
    "text": "Continuity\n\nContinuity enables the development of strategy as the other elements of strategy such as tailoring a value chain, trade-offs and fit take time to develop.\nWhy is continuity essential?\n\nContinuity reinforces a company’s identity - it builds a company’s brand, its reputation, and its customer relationships. A good strategy defined by a unique value proposition and accompanying elements maintained over time through repeated interactions with customers is what gives power to a brand.\nContinuity helps suppliers, distributors, employees and other outside parties to contribute to a company’s competitive advantage\nContinuity fosters improvements in individual activities and fit across activities; it allows an organization to build unique capabilities and skills tailored to its strategy. Continuity allows companies to institute and hone their culture and HR practices. Continuity ensures employees get what the company is trying to do and contribute to its mission more meaningfully.\nShifts in strategy requires realigning a whole host of activities that are often very hard to get right. If it involves aligning 5 new activities and the probability of getting each activity right is 0.9, the probability of getting the new strategy right is $0.9^5 = 0.59 $\n\nContinuity of strategy does not mean that an organization should stand still. As long as there is stability in the core value proposition, there can, and should be enormous innovation in how it is delivered.\nGreat strategies are rarely, if ever, built on a particularly detailed or concrete prediction of the future. You only need a very broad sense of which customers and needs are going to be relatively robust five or ten years from now. Strategy is essentially a bet that the chosen customers or needs and the essential trade-offs for meeting them at the right price-will be enduring.\n\nDell bet that as people became more familiar with computers, they will forego retailers and resellers when buying a computer\nSouthwest only had to predict people want low-cost, convenient transportation.\n\n\n\n“That is what strategy is all about . It’s about a point of view of the future and then making decisions based on that. The worst think you can do is not have a point of view, and not make decision” - Alan Mulally"
  },
  {
    "objectID": "blog/2020-12-19-undestanding-michael-porter.en/index.html#five-tests-of-good-strategy",
    "href": "blog/2020-12-19-undestanding-michael-porter.en/index.html#five-tests-of-good-strategy",
    "title": "Undestanding Michael Porter",
    "section": "Five tests of good strategy",
    "text": "Five tests of good strategy\n\nA unique value proposition\nAre you offering distinctive value to a chosen set of customers at the right relative price\nA tailored value chain\n\nIs the best set of activities to deliver your value proposition different from the activities performed by rivals?\n\nTrade-offs different from rivals\n\nAre you clear about what you don’t do so that you can deliver your kind of value most efficiently and effectively?\n\nFit across the value chain\n\nIs the value of your activities enhanced by the other activities you perform?\n\nContinuity over time\n\nIs there enough stability in the core of your strategy to allow your organization to get good at what it does, to foster tailoring, trade-offs and fit?"
  },
  {
    "objectID": "blog/2020-12-19-undestanding-michael-porter.en/index.html#when-does-strategy-need-to-change",
    "href": "blog/2020-12-19-undestanding-michael-porter.en/index.html#when-does-strategy-need-to-change",
    "title": "Undestanding Michael Porter",
    "section": "When does strategy need to change?",
    "text": "When does strategy need to change?\n\nWhen customer needs change. As this happens, a company’s core value proposition may become obsolete\n\nInnovation of all sorts can invalidate the essential trade-offs on which a strategy relies.\n\nE.g. Rise of cheap Original Design Manufacturers has allowed competitors like HP to outsource design and assembly wiping out Dell’s cost advantage\n\nA technological or managerial breakthrough can completely trump a company’s existing value proposition.\n\nE.g. Digital photography disrupted Kodak\n\n\nWhat must change?\n\nA company must be on the frontier of operational effectiveness and quality of execution, else strategy won’t matter. Assimilate best practices that do not conflict with your strategy or the trade-offs essential to it.\n\nE.g. BMW using crash simulations rather than relying solely on physical crash testing\n\nA company must change when there are ways to extend your value proposition or better ways to deliver it.\n\nNetflix was always focused on transitioning from mail order DVDs to internet streaming.\n\n\nIf you don’t have a strategy, then anything and everything could be important. A strategy helps you to decide what’s important because you know who you are trying to serve, what needs you are trying to meet, and how your value chain is distinctively configured to do so at the right price."
  },
  {
    "objectID": "blog/2020-12-19-undestanding-michael-porter.en/index.html#ten-practical-implications",
    "href": "blog/2020-12-19-undestanding-michael-porter.en/index.html#ten-practical-implications",
    "title": "Undestanding Michael Porter",
    "section": "Ten Practical Implications",
    "text": "Ten Practical Implications\n\nVying to be the best is an intuitive but self-destructive approach to competition\nThere is no honor in size or growth if those are profitless. Competition is about profits, not market share\nCompetitive advantage is not about beating rivals; it’s about creating unique value for customers. If you have a competitive advantage , it will show up on your P & L\nA distinctive value proposition is essential for strategy. But strategy is more than marketing. If your value proposition doesn’t require a specifically tailored value chain to deliver it, it will have no strategic relevance\nDon’t feel you have to “delight” every possible customer out there. The sign of good strategy is that it deliberately makes some customers unhappy\nNo strategy is meaningful unless it makes clear what the organization will not do. Making trade-offs is the linchpin that makes competitive advantage possible and sustainable\nDon’t overestimate or underestimate the importance of good execution. It’s unlikely to be a source of a sustainable advantage, but without it event the most brilliant strategy will fail to produce superior performance.\nGood strategies, depend on many choices, not one and on the connections among them. A core competence alone will rarely produce a sustainable competitive advantage\nFlexibility in the face of uncertainty may sound like a good idea, but it means that your organization will never stand for anything or become good at anything. Too much change can be just as disastrous for strategy as too little.\nCommitting to a strategy does not require heroic predictions about the future. Making the commitment actually improves your ability to innovate and to adapt to turbulence."
  },
  {
    "objectID": "blog/2020-05-09-never-split-the-difference/index.html",
    "href": "blog/2020-05-09-never-split-the-difference/index.html",
    "title": "Never Split the Difference",
    "section": "",
    "text": "Below are some of my notes/highlights from the book Never Split the Difference: Negotiating as if your life depended on it\n\nChapter 1\n\nThe goal of negotiation is discover information and any surprises, not to win an argument.\nAlways have a BATNA (Best Alternative to a Negotiated Agreement) before you enter into a negotiation\nPay attention to the use of personal pronouns. If someone tries to make themselves unimportant by using we/they, they probably are important and have decision making power. If they use I/me, the converse is true.\nMirroring or isopraxism is a valuable technique to build trust. Copying/imitating is a neurobehavior shown by animals/humans to comfort each other. In the FBI, mirroring simply means repeating the last three words someone just said. By repeating back what people say, you trigger the mirroring instinct and your counterpart will elaborate on what was just said and sustain the process of connecting. It also encourages the other side to empathize and bond with you.\nWhen you mirror, pause for at least 4 seconds to let the ‘mirror’ work.\nThere are three voice tones available to negotiators:\n\nThe late-night FM DJ voice: Use selectively to make a point. Inflect your voice downward, keeping it calm and\nslow.When done properly, you create an aura of authority and trustworthiness without triggering defensiveness.\nThe positive/playful voice: Should be your default voice. It’s the voice of an easygoing, good-natured person. Your attitude is light and encouraging. The key here is to relax and smile while you’re talking .\nThe direct or assertive voice: Used rarely. Will cause problems and create push back.\n\n\n\n\nChapter 2\n\nShow empathy to build intimacy and trust.\nYou can show empathy by deeply listening to a person and observing their face, gestures and tone of voice\nAcknowledge a person’s emotion by labeling it, labels always begin with roughly the same words:\n\nIt seems like . . .\nIt sounds like . . .\nIt looks like . .\n\nLabeling negatives diffuses them (e.g. It looks like my decision to do X has upset you) while labeling positives reinforces them\nPause after labeling. The other party will break the silence\nDo an ‘accusation audit’ to start with by listing all the most critical things your counterpart can say about you. This takes the sting out of the negotiation.\n\n\n\nChapter 3\n\nThe word ‘No’ indicates a negotiation has meaningfully started as ‘No’ is often just a temporary decision to maintain the status quo\nWe should tailor questions that elicits a ‘No’ from the counter-party. It gives the speaker feelings of safety and control. After having said ’No’, people are more open to moving towards new options and ideas. It also gets the counter-party to pay serous attention to the discussion. Asking ‘Is now a bad time to talk?’ is always better than ‘Do you have a few minutes to talk?’\nAn early ’Yes’ is mostly a counterfeit ‘Yes’ to dodge the real decisions, just like you say ‘Yes’ to get rid of a pushy salesman. ‘Yes’ should be the final goal of a negotiation.\nPushing people to say ‘Yes’ makes them defensive e.g. ’Do you like to drink water, Mr Smith?’ in order to sell a water purifier\nOne way to get people to say ‘No’ is by mislabeling an emotion or desire. e.g. ‘ It seems like you are eager to leave the job’ when they clearly want to stay or ‘It seems like you want the project to fail’.\n\n\n\nChapter 4\n\nInstead of ‘Yes’, strive to get to a ‘That’s right’.\nWhen a person says ‘that’s right’, it means he feels understood and positively affirmed opening the door for more constructive behavior.\nUse a summary to trigger a ‘that’s right’\n\n\n\nChapter 5\n\nReveal your deadline in a negotiation. This reduces the risk of an impasse and your opponent will get to the real deal and concession making more quickly\nStrive for a reputation of being fair. Early on in a negotiation, you can say, “I want you to feel like you are being treated fairly at all times. So please stop me at any time if you feel I’m being unfair, and we’ll address it.”.\nDon’t let an accusation of being ‘not fair’ throw you off kilt, people often quickly make concessions when they are accused of not being ‘fair’. Instead ask how you are treating them unfairly?\nGenerally in negotiations involving risk, people are drawn to sure things over probabilities (Certainty effect), and people take greater risks to avoid loss than to achieve gains (Loss Aversion). So make sure that your counterpart feels that there is something to lose if the negotiation fails.\nIn salary negotiations, quote a range with the low end of the range being the number you actually want. It also is better to let the company quote a number first in a negotiation.\nSet an extreme anchor as a starting point, letting them know how bad it can be so that the real offer seems reasonable\nIt pays to be kind to people as they often feel the need to reciprocate with kindness. This can also be a problem if you ask a question with an easy ‘Yes’ answer, you will be expected tor reciprocate.\nUsing precise numbers (e.g. 456.7) rather than round numbers (400) forces the opponent to think that you have carefully thought through your argument and position.\n\n\n\nChapter 6\n\nUse calibrated questions instead of bluntly saying ‘No’\nIf the price is too high when you are buying or too low when you are selling, ask ‘How am I supposed to do that’? By asking this question, you give your opponent an illusion of control and invite him/her to help solve your problem.\nCalibrated questions cannot be close ended that can be answered with a ‘Yes’ or ‘No’. Instead, they start with a list of words people know as reporter’s questions: “who,” “what,” “when,” “where,” “why,” and “how”. It is best to start with ‘what’, ‘how’ and sometimes ‘why’. ’Why’ can backfire as it can come across as accusatory.\n‘Does this look like something you like?’ can be rephrased as ‘What about this works for you?’ or ‘What about this doesn’t work for you?’\nWhen attacked in a negotiation, pause and avoid angry emotional reactions, instead ask your counterpart a calibrated question\nThere is always a team on the other side, you need to be able to influence even those not sitting directly behind the table\n\n\n\nChapter 7\n\nA listener always has more control in a conversation as the talker is revealing information.\nYou can get to the answer you want by asking carefully calibrated ‘How’ questions.\n‘How’ questions also get your opponent to lay out implementation details that they are now more likely to abide by.\nIf the other party responds with ‘I will try’, it means they are planning to fail. Go back to ‘How’ questions so that they define the terms of successful implementation in their own words. Then get a ‘That’s right’ by summarizing what they said.\n\n\n\nChapter 8\n\nKeep using calibrated ‘How’ questions to get your counterpart to come up with answers.\nThis also forces your counterpart to explain how the deal will be implemented and makes it more likely that he will stick to it given he came up with the idea himself.\nAny project or deal requires clear criteria for success, get you counterpart to think they are defining success by asking\n\nHow will we know we are on track?\nHow will we address things if we are off track? Once they answer, summarize and replay until you get a ‘That’s right’\n\nBe wary of the response ‘You are right’. It means they are not bought into the idea. Go back to the ‘How’ questions and re-summarize until you get a ‘That’s right’.\nFace to Face conversations matter. Research in UCLA has shown that only 7% of a message is based on the words, 38% is based on tone of voice while 55% is based on a speaker’s face and body language.\nThree kinds of Yes: Commitment, Confirmation and Counterfeit. Beware of Counterfeit and seek Commitment.\nRule of Three: Get the counterpart to agree to the same thing three times. It triples the strength of the dynamic you are seeking and surfaces and problems as it is hard to repeatedly fake or lie.\nSay ‘No’ more tactfully by asking’ How can I do that?’ or ‘How am I supposed to do that?’. Deliver this in a deferential way so that it becomes a plea for help. To be more direct you can say ‘I am sorry, I am afraid I just can’t do that’.\n\n\n\nChapter 9\n\nIf you are a well-prepared negotiator, you want to see the other side name their price first, because you want to see their hand.\nHowever, you have to be prepared for an extreme anchor and not get unsettled by it. You can do one or more of the following\n\nDeflect the punch by saying ‘No’ tactfully as described above\nAnother good question is ‘What are we trying to accomplish here?’\nDetour the conversation to non-monetary issues that can make the final price work e.g. ‘Let’s put price off for a moment and talk about what would make this a good deal’ or ‘What else would you be able to offer to make that a good price for me’?\n\nUsing ‘Why?’ : Use the defensiveness the question triggers to get your counterpart to defend your position.\n\nAsk ‘Why would you do that?’ in a way that the ‘that’ favors you. E.g. When trying to lure a client away from a competitor, ask ‘Why would you ever do business with me?’ or ‘Why would you ever change from your existing s supplier?,they are great’.\n\nRecipe for negotiating a purchase:\n\nSet your target price (goal)\nSet your first offer at 65% of your target price\nCalculate three raises of decreasing increments (to 85,90 and 100%)\nUse lots of empathy and different ways of saying ‘No’ to get the other side to counter before increasing your offer\nWhen calculating the final amount, use precise, non-round numbers – this gives the number credibility and weight\nOn the final number, throw in a non-monetary item (that they probably don’t want) to show you are at your limit.\n\n\n\n\nChapter 10\n\nThree types of leverage in negotiations: Positive(the ability to give someone what they want); Negative ( the ability to hurt someone) and Normative( using your counterpart’s norms and principles to bring them around)\nUnderstand your counterpart’s religion , principles or world view – this could be a source of leverage\nReview everything you hear from your counterpart, use back up listeners so you don’t miss something important\nPeople are more likely to make concessions to someone who is similar to them, so try to show that you share common ground\nGet face time with your counterpart and pay special attention to verbal and non-verbal communication at unguarded moments. E.g. At the beginning and end of sessions."
  },
  {
    "objectID": "blog/2023-03-11-introduction-to-graph-machine-learning/index.html",
    "href": "blog/2023-03-11-introduction-to-graph-machine-learning/index.html",
    "title": "Introduction to Graph Machine Learning",
    "section": "",
    "text": "One of the initiatives I am most proud of in my current role is the Monthly Learning Hour. Every Month, each person in my team takes turns to do an hour long presentation on a topic of interest. Most recently, I did a session on Graph Machine Learning.\nYou can access the full notebook on Google Colab here.\nThe notebook covers some interesting algorithms that have emerged in the field between 1998(Page Rank) and 2017(Graph Attention Networks). It also demonstrated how to use them on real datasets with Network X and Pytorch geometric. I am working on a video walk through of the notebook as some parts of it can be quite dense. Check out references at the bottom of the notebook to learn more."
  },
  {
    "objectID": "blog/2019-02-21-introduction-to-bayesian-methods/index.html",
    "href": "blog/2019-02-21-introduction-to-bayesian-methods/index.html",
    "title": "Introduction to Bayesian Methods",
    "section": "",
    "text": "This post will cover the following topics\n\nDifferences between Bayesian and Frequentist Approaches\nA Simple Bayesian Analysis\nSimple Bayesian Linear Regression"
  },
  {
    "objectID": "blog/2019-02-21-introduction-to-bayesian-methods/index.html#confidence-intervals-vs-credible-intervals",
    "href": "blog/2019-02-21-introduction-to-bayesian-methods/index.html#confidence-intervals-vs-credible-intervals",
    "title": "Introduction to Bayesian Methods",
    "section": "Confidence Intervals vs Credible Intervals",
    "text": "Confidence Intervals vs Credible Intervals\nThe Bayesian analogue of the commonly used Frequentist Confidence Interval is the Credible Interval. Although both sound similar and often yield identical results, their interpretations are quite different.\nIn Bayesianism, probability distributions reflect degrees of belief, so a 95 % credible interval [a,b] of a parameter \\(\\mu\\) is equivalent to saying:\nGiven our observed data, there is a 95% probability that the true value of \\(\\mu\\) falls within the credible interval [a,b]\nThe frequentist 95% confidence interval [a,b] is equivalent to saying\nThere is a 95% probability that when I compute this confidence interval from data of this sort, the true mean will fall within this confidence interval\nNote the difference: the Bayesian solution is a statement of probability about the parameter value given fixed bounds. It fixes the credible region, and guarantees that 95% of possible values of \\(\\mu\\) fall within it.\nThe frequentist solution is a probability about the bounds or intervals given a fixed parameter value. It fixes that parameter and guarantees that 95% of possible confidence intervals will contain it.\nA criticism of frequentist inference is that it often answers the wrong question. It relies on data of this sort or a hypothetical space of observations like what we have observed so far. This may lead one to answers that don’t tell you anything meaningful about the particular data observed.\nIf you want to ask what the confidence interval can tell you given the particular data already observed, all it can say is that: Given the observed data, the true value of \\(\\mu\\) is either in the confidence interval or it isn’t. If you are only interested in what particular observed data is telling you, frequentism is not very useful."
  },
  {
    "objectID": "blog/2019-02-21-introduction-to-bayesian-methods/index.html#null-hypothesis-significance-testing",
    "href": "blog/2019-02-21-introduction-to-bayesian-methods/index.html#null-hypothesis-significance-testing",
    "title": "Introduction to Bayesian Methods",
    "section": "Null Hypothesis Significance Testing",
    "text": "Null Hypothesis Significance Testing\nConsider some data we received from an experimenter who was measuring the bias of a coin. The data indicates that were 7 HEADS (z) out of a total of 24 tosses (N).\n\nFrequentist Approach\nExperimental design or setup is very important when using frequentist approaches.In other words the intent of the experimenter has an impact on the inferences we make.\nThe null hypothesis we want to evaluate is:\nH0: The bias of the coin is 0.5 or alternatively H0: \\[ \\theta = 0.5 \\]\nTo evaluate whether the observation suggests that the coin is biased ,we need to calculate the p-value. The p-value is the probability of getting a sample outcome from the sampling distribution that is as or more extreme than the actual outcome.\nNow the sampling distribution depends on the intent of the experimenter which can be 1. Did he do the experiment with the intent to record 24 coin flips 2. Did he continue the experiment until he observed 7 HEADs\nIf the experimented did not tell us what his intent for the experiment was, we could reach two different conclusions based on the same data.\n\n\nFor fixed N\nIf the experimenter’s intent was the first one listed above. The sampling distribution and the p- value are obtained by consider a binomial probability distribution.\nThe p-value is given by:\np_value &lt;- pbinom(7,24,0.5)\nprint(p_value)\n## [1] 0.03195733\nThe sampling distribution is given below.\nlibrary(ggplot2)\n\nz &lt;- c(0:24)\nP &lt;- dbinom(c(0:24),24,prob = 0.5)\n\n\nggplot(data.frame(z,P),aes(x=z,y=P)) + geom_bar(stat = 'identity',fill = 'cornflowerblue')+\n  geom_vline(xintercept =7,col='red')+\n  geom_segment(aes(x=7,y = 0.025,xend = 0,yend=0.025),\n               arrow = arrow(length = unit(0.02,\"npc\")))+\n  geom_text(aes(x = 4, y =0.04),label=paste0('P value = ',round(p_value,3)))\n\n\n\nFor fixed z\nIf the experimenter’s intent was the second one listed above i.e. to continue the experiment until he sees 7 HEADS, the sampling distribution and the p- value are obtained by considering a negative binomial probability distribution.\nThe p-value is given by:\nn_heads =7\nn_tails = 16\n\np_value &lt;-  1 - pnbinom(n_tails, n_heads,0.5)\nprint(p_value)\n## [1] 0.01734483\nThe sampling distribution is given below.\nlibrary(ggplot2)\n\nN &lt;- c(0:50) # Number of possible tails observed before 7th head is observed\nP &lt;- dnbinom(N,size = 7,prob = 0.5)\n\nggplot(data.frame(N,P),aes(x= N,y=P)) + geom_bar(stat = 'identity',fill = 'cornflowerblue')+ geom_vline(xintercept = 24 ,col='red')+\n   geom_segment(aes(x= 24,y = 0.025,xend = 50,yend=0.025),\n                arrow = arrow(length = unit(0.02,\"npc\")))+\n   geom_text(aes(x = 30, y =0.04),label=paste0('P value = ',round(p_value,3)))\n\nIn both cases above the null hypothesis can be rejected given the p-value is less than 0.05, but it can be seen that based on the intentions of the experimenter, the p-value can be different.\n\n\nBayesian Approach\nThe Bayesian approach to hypothesis testing does not depend on the intentions of the experimenter. The likelihood function in this approach is the Binomial likelihood function, just as in the frequentist approach described above.\nIn the Bayesian approach, we also need to choose a prior which encodes our prior belief about the fairness of the coin. To make the analysis easier the prior can be a Beta distribution which is conjugate of the binomial likelihood function.\nOne possible Beta prior is Beta(2,2) which indicates a weak prior belief in the fairness of the coin. This encodes a distribution as shown below:\nset.seed(0)\nplot(density(rbeta(10000,2,2)),main = \"Prior 1\",xlab=\"\")\n\nAnother possible prior is a strong belief in the tail bias of a coin.\nset.seed(0)\nplot(density(rbeta(10000,2,20)),main = \"Prior 2\",xlab=\"\")\n\nNote that while the frequentist approach above uses conditional distributions of data given a fixed hypothesis, e.g. \\(\\theta = 0.5\\), the bayesian uses a probability distribution over all possible hypotheses.\nThe posterior distribution over all possible hypotheses is given by the Baye’s rule.\n\\[ P(H|D) = \\frac{P(D|H) P(H)}{P(D)} \\]\nNow H is a hypothesis and D is data which may give evidence for or against H. Each term in Bayes’ formula has a name and a role.\n\nThe prior P(H) is the probability that H is true before the data is considered.\nThe posterior P(H | D) is the probability that H is true after the data is considered.\nThe likelihood P(D | H) is the evidence about H provided by the data D.\nP(D) is the total probability of the data taking into account all possible hypotheses\n\nFor the prior 1 considered above the posterior will be a Beta(2+7,2+17)\nplot(density(rbeta(10000,9,19)),main = \"Posterior 1\",xlab=expression(theta))\n\nFor the prior 2 considered above the posterior will be a Beta(2+7,20+17)\nplot(density(rbeta(10000,9,37)),main = \"Posterior 2\",xlab=expression(theta))\n\nIn summary, according to the statistician Larry Wasserman:\nThe Goal of Frequentist Inference: Construct procedure with frequency guarantees. (For example, confidence intervals.)\nThe Goal of Bayesian Inference: Quantify and manipulate your degrees of beliefs. In other words, Bayesian inference is the Analysis of Beliefs."
  },
  {
    "objectID": "blog/2023-07-09-apple-and-meta-s-strategic-clarity/index.html",
    "href": "blog/2023-07-09-apple-and-meta-s-strategic-clarity/index.html",
    "title": "Apple and Meta’s Strategic Clarity",
    "section": "",
    "text": "Apple’s unveiling of Apple Vision Pro and the reaction to it has made one thing clear.\nA new era of computing has dawned. Whether you call it spatial computing or virtual/mixed reality , it will almost certainly reshape our digital lives in the decade to come.\nIn many ways, what Apple has done with the Vision Pro is validate Mark Zuckerberg’s bet on VR and AR beginning with Meta’s acquisition of Oculus in 2014.\nWhat is fascinating is the strategic clarity demonstrated by these two companies in tackling this problem.\nAs Michael Porter has described, the essence of competitive strategy is about being different by choosing a different set of activities that fit and reinforce each other to deliver unique value to a specific set of customers.\nTo evaluate whether these two companies have clear competitive strategies, we can analyze both Apple and Meta’s VR strategy through Porter’s test of good strategy which consists of 5 themes."
  },
  {
    "objectID": "blog/2023-07-09-apple-and-meta-s-strategic-clarity/index.html#a-unique-value-proposition",
    "href": "blog/2023-07-09-apple-and-meta-s-strategic-clarity/index.html#a-unique-value-proposition",
    "title": "Apple and Meta’s Strategic Clarity",
    "section": "A unique value proposition",
    "text": "A unique value proposition\n\nApple\nApple has designed the Apple Vision Pro with a specific focus on entertainment and productivity.\nDisney Plus will be available on the Vision Pro when it is released while their demos focused on immersive experiences in sports. Apple’s partnership with MLS means that this experience might be coming to MLS fans soon.\nIt also designed Vision OS to let you use iOS apps and browse the internet and even lets you work on your Mac while using the Vision IOS as a monitor. Apple has focused on more individualized use cases and their price point of $3,500 reflects that.\nApple has bet on Mixed Reality.\n\n\nMeta\nMeta has designed the Meta Quest with a specific focus on gaming and social experiences.\nIt already comes with a library of over 500 video game titles. It also supports use cases such as work outs. It has designed Horizon worlds to deliver a social VR experience , consistent with the mission of the company to connect people.\nThe lower price point of $500 means that a lot more people can afford the Meta Quest, which is essential to delivering a social experience.\nMeta has bet on Virtual Reality."
  },
  {
    "objectID": "blog/2023-07-09-apple-and-meta-s-strategic-clarity/index.html#a-tailored-value-chain",
    "href": "blog/2023-07-09-apple-and-meta-s-strategic-clarity/index.html#a-tailored-value-chain",
    "title": "Apple and Meta’s Strategic Clarity",
    "section": "A tailored value chain",
    "text": "A tailored value chain\n\nApple\nThe Vision Pro is vertically integrated, with Apple developing the software (Vision OS) as well as hardware including 3D cameras, and the M2 and R2 chips. This is consistent with Apple’s focus on seamlessly integrating hardware and software.\n\n\nMeta\nThe Meta Quest is more modular. The Operating System for the Quest is based on Android while the chip comes from Qualcomm."
  },
  {
    "objectID": "blog/2023-07-09-apple-and-meta-s-strategic-clarity/index.html#trade-offs-different-from-rivals",
    "href": "blog/2023-07-09-apple-and-meta-s-strategic-clarity/index.html#trade-offs-different-from-rivals",
    "title": "Apple and Meta’s Strategic Clarity",
    "section": "Trade Offs different from rivals",
    "text": "Trade Offs different from rivals\nThe tradeoffs made by the two companies are already clear, based on the use cases they intend to solve for and the price points they are targeting.\nApple offers incredibly high resolution that makes it possible to even read text using the Vision Pro, a pre-requisite for productivity applications. It has an external battery that makes it less suitable for gaming or anything that involves physical activity.\nMeta Quest has settled for a lower resolution given the gaming use cases it is targeting. It also comes with the battery packed into the headset while also offering customer controllers for game play."
  },
  {
    "objectID": "blog/2023-07-09-apple-and-meta-s-strategic-clarity/index.html#fit",
    "href": "blog/2023-07-09-apple-and-meta-s-strategic-clarity/index.html#fit",
    "title": "Apple and Meta’s Strategic Clarity",
    "section": "Fit",
    "text": "Fit\nThe choices made by each company fits in with the overall mission of each company.\nApple has always been focused on creating delightful consumer products by tightly integrating hardware, software and services. The Vision Pro will be expected to join the ranks of Apple’s iconic devices like the iPhone, the Mac, and iPad enhancing the lineup of delightful personal computing devices. Vision OS will make using this new device seamless for anyone using other Apple products. Apple’s investments in iOS and the App Store is what makes Vision Pro possible and truly compelling.\nFor Meta, the Quest is a gateway to connecting people in virtual worlds. Presence really matters if you want to connect deeply with people. I felt this at a deeply personal level when my nephews visited me for the first time in almost 5 years. The connection I felt with them was unlike anything that a pure digital medium could afford.\nIf the VR experience can be capture even a sliver of that profound connection when you are physically present with someone, Meta will be making a giant stride forward towards accomplishing its mission. Again Meta’s core investments in creating platforms that connect billions of people in the digital world will likely be key in making it the platform of choice to connect with others in the metaverse."
  },
  {
    "objectID": "blog/2023-07-09-apple-and-meta-s-strategic-clarity/index.html#continuity-over-time",
    "href": "blog/2023-07-09-apple-and-meta-s-strategic-clarity/index.html#continuity-over-time",
    "title": "Apple and Meta’s Strategic Clarity",
    "section": "Continuity Over Time",
    "text": "Continuity Over Time\n\nApple\nNo other company in the world has a focus and consistency in strategy as Apple. They are hyper focused on creating consumer technology products that you interact with every waking hour of the day. The Apple Brand is now the gold standard for quality and trust.\nThe Vision Pro represents a continuation of this strategy.\n\n\nMeta\nAlthough the stock market thinks that Meta has erred in investing so heavily on the metaverse project, being a founder driven company means it can commit to a singular vision even if there are naysayers aplenty. Zuck should give Meta the continuity in strategy to pursue this to its conclusion."
  },
  {
    "objectID": "blog/2023-07-09-apple-and-meta-s-strategic-clarity/index.html#differentiation-vs-cost-leadership",
    "href": "blog/2023-07-09-apple-and-meta-s-strategic-clarity/index.html#differentiation-vs-cost-leadership",
    "title": "Apple and Meta’s Strategic Clarity",
    "section": "Differentiation vs Cost Leadership",
    "text": "Differentiation vs Cost Leadership\n\nViewed through the lens of Porter’s generic strategies, Apple will be pursuing a high degree of differentiation for a larger set of tech visionaries interested in productivity and entertainment use cases.\nMeta will strive for cost leadership with tech visionaries interested in gaming and social use cases. Given the emphasis on social use cases, Meta will be aggressively looking to drive down prices even at the expense of highly differentiated use cases so that more people can buy their devices."
  },
  {
    "objectID": "blog/2023-07-09-apple-and-meta-s-strategic-clarity/index.html#risks",
    "href": "blog/2023-07-09-apple-and-meta-s-strategic-clarity/index.html#risks",
    "title": "Apple and Meta’s Strategic Clarity",
    "section": "Risks",
    "text": "Risks\nFor both Apple and Meta, these bets represent a different risk profile.\n\niewing it through Hamilton Hemler’s and Chenyi Shi’s framework, it represents a far lesser risk for Apple given the power of its brand in consumer technology products and its strengths in creating tightly knit hardware and software (process power).\nFor Meta, this requires building new skills in developing consumer hardware which certainly takes it out of its comfort zone - a very high margin advertising business with zero marginal cost. This is why the conviction and backing of its founder will be critical for success."
  },
  {
    "objectID": "blog/2023-07-09-apple-and-meta-s-strategic-clarity/index.html#summary",
    "href": "blog/2023-07-09-apple-and-meta-s-strategic-clarity/index.html#summary",
    "title": "Apple and Meta’s Strategic Clarity",
    "section": "Summary",
    "text": "Summary\nMeta and Apple’s different approaches to VR/AR is one of the best examples I have seen of two companies showing clear strategic thinking in a drive to be unique. Is it any wonder that these two companies are jointly worth 3.75 trillions $ ?"
  },
  {
    "objectID": "blog/2020-05-09-detecting-concept-drift/index.html",
    "href": "blog/2020-05-09-detecting-concept-drift/index.html",
    "title": "Detecting Concept Drift",
    "section": "",
    "text": "Concept drift refers to changes in the joint distribution of of an instance \\(X\\) and its class \\(y\\)\n\\[ P(X,y) = P(X)P(y|X)\\]\nConcept drift can therefore be caused by a change in any of the two terms in the right side of the equation above.\nIf the drift is only due to a change in \\(P(X)\\), it means the distribution of the incoming data has changed but the decision boundary remains unaffected, this is referred to as virtual drift. Virtual drift does not lead to deterioration in model performance.\nIf the drift is due to a change in \\(P(y|X)\\),this affects the decision boundary of the model which in turn affects the performance of the model. This is referred to as real drift.\nWhen ML models are deployed in production, it is essential to detect real drift so that remedial action can be taken.\nThe figure below gives an example of the original data and data resulting from real and virtual drift."
  },
  {
    "objectID": "blog/2020-05-09-detecting-concept-drift/index.html#introduction",
    "href": "blog/2020-05-09-detecting-concept-drift/index.html#introduction",
    "title": "Detecting Concept Drift",
    "section": "",
    "text": "Concept drift refers to changes in the joint distribution of of an instance \\(X\\) and its class \\(y\\)\n\\[ P(X,y) = P(X)P(y|X)\\]\nConcept drift can therefore be caused by a change in any of the two terms in the right side of the equation above.\nIf the drift is only due to a change in \\(P(X)\\), it means the distribution of the incoming data has changed but the decision boundary remains unaffected, this is referred to as virtual drift. Virtual drift does not lead to deterioration in model performance.\nIf the drift is due to a change in \\(P(y|X)\\),this affects the decision boundary of the model which in turn affects the performance of the model. This is referred to as real drift.\nWhen ML models are deployed in production, it is essential to detect real drift so that remedial action can be taken.\nThe figure below gives an example of the original data and data resulting from real and virtual drift."
  },
  {
    "objectID": "blog/2020-05-09-detecting-concept-drift/index.html#drift-detection-in-aml",
    "href": "blog/2020-05-09-detecting-concept-drift/index.html#drift-detection-in-aml",
    "title": "Detecting Concept Drift",
    "section": "Drift Detection in AML",
    "text": "Drift Detection in AML\nDrift detection can be supervised; this requires that we know the ground truth labels to evaluate the performance of the classifier; or unsupervised where it is not necessary to know the ground truth labels.\nIn many domains such as AML(Anti Money Laundering) and even in certain kinds of Fraud, there can often be a significant delay in getting the ground truth labels. In such domains,the unsupervised method is preferable. The intent of using concept drift detection in these domains is to understand any changes that might negatively impact model performance as early as possible so that model risk can be mitigated.\nThere are broadly three types of unsupervised drift detection methods.\n\nNovelty detection/clustering methods\nMultivariate Distribution Monitoring\nModel Dependent Monitoring\n\nMethod 1 is not suitable for binary classification, it is typically used in multi class classification problems where the data generating process can give rise to new classes that have not been considered hitherto.\nMethod 2 may not be suitable where the data is highly imbalanced such as in AML or Fraud. Given one class of labels are in the minority, changes in the distribution of this class typically does not impact the overall distribution of data.\nFurther both these methods assume that any change in the data distribution i.e. \\(P(X)\\) also causes a change in the performance of the classifier i.e. \\(P(Y|X)\\), this can often leads to a lot of false positives.Therefore we will focus primarily on the model based methods.\n\nConfidence Distribution Batch Detection (CDBD)\nMargin Density (MD)\n\n\nCDBD\nThis is an approach that works for probabilistic classifiers by comparing the distribution of predicted scores in a test batch with that of a reference batch. The reference batch is typically the batch of instances classified immediately after the model has been trained while the test batches are data sets from some appropriate recurring time window e.g. 1 month.\nThe distributions are compared after discretizing them and then using a measure such as the Kullback Leibler Divergence. The measures for the test batches are compared to an appropriately chosen threshold which is derived from the reference batch.If x out of the last y test batches (e.g. 3/5) triggers the threshold, then model drift is said to have occurred. Note that in the experiment below, the distributions were not discretized.\nIn order to create the threshold for comparison,the distribution divergence of each of the first n test batches immediately after the reference batch and the reference batch itself are calculated. The threshold is set to one standard deviation above the mean of these divergences.\n\n\nMD\nMargin is the portion of the prediction space most vulnerable to misclassification. The Margin Density metric is motivated by the idea that a significant change in the density of instances occurring in a classifier’s margin is indicative of concept drift.\nMargin Density is defined as follows.\n\\[MD = \\frac{\\Sigma  S_{E} \\left( x \\right) }{|X|}; \\forall x \\epsilon X \\]\nwhere\n\\[ S_E(x) =  \\begin{cases} 1 & \\text{if } |p(y ==+1|x) - p(y == -1|x) |   \\leq \\theta_{margin} \\\\\\\\\n                                          0 & \\text{otherwise}  \\end{cases} \\]\nThe absolute value of the change in marginal density \\(|\\Delta MD|\\) for each new batch of data in relation to the reference batch is measured, if this exceeds some threshold, this is indicative of concept drift.\n\\[ if | MD_t - MD_{ref} | \\gt MD_{\\text{threshold}} \\implies \\text{drift}\\ \\text{suspected} ) \\]\nThe threshold can be learned from the training data set using K-fold cross validation. It can be the mean plus N standard deviations.\n\n\nRandom Forest\nAnother method to detect concept drift is to use a predictive model(e.g. a Random Forest) to classify an observation as belonging to the training set or the incoming production dataset. If there has been no domain shift, the performance of the model should be closed to random, if the performance of the model is significantly better than random, it suggests there has been some shift in the data, we can use the random forest feature importance to identify what is driving the shift."
  },
  {
    "objectID": "blog/2020-05-09-detecting-concept-drift/index.html#experiment",
    "href": "blog/2020-05-09-detecting-concept-drift/index.html#experiment",
    "title": "Detecting Concept Drift",
    "section": "Experiment",
    "text": "Experiment\n\nData\nSimulated data will be used same as before.The original data and drifted data are as follows.\n\nNote that Drifted Data 4 and Drifted Data 5 is a case of virtual drift, our drift detection technique should ideally not raise an alert for this type of virtual drift.\n\n\nAnalysis\nA model is fit to the original data , the resulting decision boundary is shown below.\nglm1 &lt;- glm(class ~ ., data = data_df1,family = 'binomial')\n#Grid for getting predictions\ngrid &lt;- expand.grid(X1=seq(0,1,by=0.01),X2= seq(0,1,by=0.01))\ngrid_preds &lt;- predict(glm1,newdata=grid,type='response')\ncontour(x=seq(0,1,by=0.01), y=seq(0,1,by=0.01), z=matrix(grid_preds,nrow =101),levels=0.5,\n        col=\"cornflowerblue\",lwd=2,drawlabels=FALSE)\npoints(data_df1$X1,data_df1$X2,col=data_df1$class,pch = 16)\n\n\nCDBD\nWe assume that the data distribution for the first 5 test batches are identical to the reference batch. Five batches of test data are therefore generated from the same distribution.\n##Function to generate reference data\n\ngenerate_data &lt;- function(seed,N){\n  \nset.seed(seed)\nN &lt;- N\n# Using polar coordinates\nr1&lt;- runif(N,0,0.15)\ntheta1 &lt;- runif(N,0,2*pi)\nr2 &lt;- runif(N,0,0.15)\ntheta2 &lt;- runif(N,0,2*pi)\n\n#Original data \nregion1 &lt;- cbind(r1 * cos(theta1) + 0.55, r1 * sin(theta1) + 0.5)\nregion2 &lt;- cbind(r2 * cos(theta2) + 0.35, r2* sin(theta2) + 0.5)\ndata &lt;- rbind(region1,region2)\ndata_df &lt;- data.frame(data,class=as.factor(c(rep(1,N),rep(2,N)))) \n  \nreturn(data_df)  \n}\n\ntest_data &lt;- lapply(c(1:5),generate_data,50)\nThe predictions of the model on the test data sets are as follows\ntest_data &lt;- do.call(rbind,test_data)\ntest_preds &lt;- predict(glm1,newdata=test_data,type='response')\ntest_preds_list &lt;- split(test_preds,rep(c(1,2,3,4,5),each=100))\n#predictions on original data\noriginal_preds &lt;- predict(glm1,type='response')\nThe KL divergence of the original distribution with respect to the five test distributions are calculated below\nlibrary(LaplacesDemon)\nKL_divergence &lt;- rep(NA,5)\nfor (i in c(1:5)){\n  KL_divergence[i] &lt;- KLD(original_preds,test_preds_list[[i]])$sum.KLD.px.py  \n}\nThe thresholds for comparison is given by the mean + 1 SD\nkl_threshold = mean(KL_divergence) + sd(KL_divergence)\ncat(paste0('The kl_threshold is ',round(kl_threshold,3)))\n## The kl_threshold is 0.571\nNow the kl divergence with respect to each of the drifted data sets are computed.\ndrifted_data &lt;- list(data_df2,data_df3,data_df4,data_df5,data_df6)\nKL_divergence_drift &lt;- rep(NA,5)\ndrifted_preds &lt;- list()\nfor (i in c(1:5)){\n  \n  preds &lt;- predict(glm1,newdata = drifted_data[[i]],type='response')\n  drifted_preds[[i]] &lt;- preds\n  KL_divergence_drift[i] &lt;- KLD(original_preds,preds)$sum.KLD.px.py  \n  \n}\nAll three drifted data sets violate the threshold while the two datasets which showed only virtual drift did not violate the threshold.\nprint(KL_divergence_drift)\n## [1] 3.18285779 4.10382091 1.66188152 0.03246057 0.03565353\nprint(KL_divergence_drift&gt;kl_threshold)\n## [1]  TRUE  TRUE  TRUE FALSE FALSE\n\n\nMD\nGiven the simulated data used here is balanced, the range of predictions indicating uncertainty in prediction can be considered to be anything between 0.25 and 0.75. This corresponds to \\(\\theta = 0.5\\).\nAlthough CV can be used, given we are working with simulated data, the margin density of the original data and five test data sets are computed.\nmargin_density_fn &lt;- function(predictions,theta){\n  margin &lt;- abs(predictions - (1-predictions))\n  margin_flag &lt;- margin &lt;= theta\n  margin_density &lt;- mean(margin_flag)\n  return(margin_density) \n}\n\npreds_list &lt;-test_preds_list\npreds_list[[6]] &lt;- original_preds\nmd &lt;- sapply(preds_list,margin_density_fn,0.5)\nmd_threshold &lt;- mean(md) + sd(md)\n\ncat(paste0('The margin density threshold is ',round(md_threshold,3)))\n## The margin density threshold is 0.079\nThe margin density of the drifted data sets are now computed. It can be seen that all three drifted data sets exceed the threshold, but we also have one false positive, with one of the virtually drifted datasets also triggering a false alarm.\nprint(md_drifted &lt;- sapply(drifted_preds,margin_density_fn,0.5))\n## [1] 0.12 0.09 0.16 0.02 0.08\nprint(md_drifted &gt; md_threshold)\n## [1]  TRUE  TRUE  TRUE FALSE  TRUE\n\n\nRandom Forest\nHere we will be training the model to predict whether the data comes from the training dataset or the production dataset, so the class labels have to be appropriately updated\ndata_df1$partition &lt;- 0\ndata_df2$partition &lt;- 1\ndata_df3$partition &lt;- 1\ndata_df4$partition &lt;- 1\ndata_df5$partition &lt;- 1\ndata_df6$partition &lt;- 1\n\n##Combine appropriately to create the final dataframe\n\ndata1 &lt;- rbind(data_df1,data_df2)\ndata2 &lt;- rbind(data_df1,data_df3)\ndata3 &lt;- rbind(data_df1,data_df4)\ndata4 &lt;- rbind(data_df1,data_df5)\ndata5 &lt;- rbind(data_df1,data_df6)\n\n##Convert target variable to a factor\ndata1$partition &lt;- as.factor(data1$partition)\ndata2$partition &lt;- as.factor(data2$partition)\ndata3$partition &lt;- as.factor(data3$partition)\ndata4$partition &lt;- as.factor(data4$partition)\ndata5$partition &lt;- as.factor(data5$partition)\n\n\ndatasets &lt;- list(data1,data2,data3,data4,data5)\nThe below function takes a dataset,fits a random forest model and returns an ROC curve for each dataset.\nlibrary(randomForest)\nlibrary(pROC)\n\nrf &lt;- function(data){\n  \n  rf_model &lt;- randomForest(partition~X1+X2,data=data,importance=TRUE)\n  probs &lt;- predict(rf_model,type = 'prob')[,2]\n  roc_n &lt;- roc(data1$partition,probs)\n  return(list(rf_model,roc_n))\n  \n}\n\n\nresults &lt;- lapply(datasets,rf)\nThe auc for the models trained across the 5 datasets are as follows. Higher the auc, higher the domain shift,if the auc shows a steady increase over time, this can be considered as a case of domain shift. Note that this method is not able to clearly distinguish between virtual drift and real drift. Although this method seems limited in this toy example, it is likely to be far more effective for models with multiple parameters rather than just the two we have considered here.\nfor (i in c(1:length(results))){\n  print(results[[i]][[2]]$auc)\n}\n## Area under the curve: 0.687\n## Area under the curve: 0.7869\n## Area under the curve: 0.8341\n## Area under the curve: 0.7006\n## Area under the curve: 0.6634\nThe ROC curves are as follows\npar(mfrow=c(2,3))\nfor (i in c(1:length(results))){\n  plot(results[[i]][[2]],title= paste0('Dataset ',i))\n  title(paste0('Dataset ',i))\n}\n\nThis approach has the added advantage that you can use random forest feature importance to determine which variable is driving the shift, as long as the variables driving the shift are not of significance, then the shift is probably not of interest."
  },
  {
    "objectID": "blog/2020-05-09-detecting-concept-drift/index.html#conclusion",
    "href": "blog/2020-05-09-detecting-concept-drift/index.html#conclusion",
    "title": "Detecting Concept Drift",
    "section": "Conclusion",
    "text": "Conclusion\nAll these techniques are promising in their ability to detect concept drift without requiring ground truth labels"
  },
  {
    "objectID": "blog/2020-05-09-detecting-concept-drift/index.html#limitations",
    "href": "blog/2020-05-09-detecting-concept-drift/index.html#limitations",
    "title": "Detecting Concept Drift",
    "section": "Limitations",
    "text": "Limitations\n\nThese techniques are able to detect drift only when concept drift occurs with respect to one or more parameters in the model. If concept drift occurs due to a variable which is not part of the model becoming significant, these techniques will not be effective. Ground truth labels and accuracy metrics will be necessary to detect such drift.\nWhen the data set is imbalanced, the threshold \\(\\theta_margin\\) will have to be adjusted appropriately."
  },
  {
    "objectID": "blog/2020-05-09-detecting-concept-drift/index.html#references",
    "href": "blog/2020-05-09-detecting-concept-drift/index.html#references",
    "title": "Detecting Concept Drift",
    "section": "References",
    "text": "References\n\nDrift Detection Using Uncertainty Distribution Divergence link\nOn the Reliable Detection of Concept Drift from Steaming Unlabeled Data link\nMCDiarmid Drift Detection Methods for Evolving Streamslink\nFinding out of domain data link"
  },
  {
    "objectID": "blog/2024-12-15-ai-commoditization/index.html",
    "href": "blog/2024-12-15-ai-commoditization/index.html",
    "title": "Is the Commoditization of AI inevitable ?",
    "section": "",
    "text": "The race for AI dominance has sparked both innovation and intense competition among technology leaders. Foundation model providers like OpenAI, Google DeepMind, and Anthropic are at the forefront, offering groundbreaking models that power everything from chatbots to enterprise automation. However, as the AI market matures, a fundamental question arises: Is AI destined to become a commodity?\nThe pressures driving this commoditization are not just technological but also economic and societal. Market forces, such as competition, open-source alternatives, and growing buyer power, are already reshaping the industry. At the same time, broader social forces demand equitable access to AI’s transformative capabilities, which could push governments to intervene."
  },
  {
    "objectID": "blog/2024-12-15-ai-commoditization/index.html#threat-of-new-entrants",
    "href": "blog/2024-12-15-ai-commoditization/index.html#threat-of-new-entrants",
    "title": "Is the Commoditization of AI inevitable ?",
    "section": "1) Threat of New Entrants:",
    "text": "1) Threat of New Entrants:\nGiven the high barriers to entry due to the significant computational and data resources required to train foundation models, there will only be a handful of players operating in this space.\nEstablished providers like OpenAI, Google, and Anthropic benefit from strong brand recognition and technical expertise, making it challenging for new players to compete."
  },
  {
    "objectID": "blog/2024-12-15-ai-commoditization/index.html#bargaining-power-of-suppliers",
    "href": "blog/2024-12-15-ai-commoditization/index.html#bargaining-power-of-suppliers",
    "title": "Is the Commoditization of AI inevitable ?",
    "section": "2) Bargaining Power of Suppliers:",
    "text": "2) Bargaining Power of Suppliers:\nSuppliers of compute resources (e.g., NVIDIA, AMD, cloud providers) hold substantial power since foundation models rely heavily on GPU and TPU infrastructure.\nData suppliers (organizations owning proprietary datasets) can also exert influence if unique datasets are necessary for training.\nOpen AI exploring options to create their own chips perhaps speaks to their desire to be more in control of their destiny in this regard."
  },
  {
    "objectID": "blog/2024-12-15-ai-commoditization/index.html#bargaining-power-of-buyers",
    "href": "blog/2024-12-15-ai-commoditization/index.html#bargaining-power-of-buyers",
    "title": "Is the Commoditization of AI inevitable ?",
    "section": "3) Bargaining Power of Buyers:",
    "text": "3) Bargaining Power of Buyers:\nBuyers, such as cloud providers, enterprises and developers, have increasing choices as multiple players offer competitive solutions.\nMajor cloud providers like Amazon and Microsoft emphasize that foundation models will become pluggable components, similar to compute, storage, or databases, for application development. These companies have a strong incentive to reduce dependency on major AI labs, as demonstrated by Microsoft’s acquisition of Inflection, Amazon’s acquisition of Adept, and Amazon’s own development of language models.\nLarge enterprises may also exert significant bargaining power. A good example of this is Apple who by virtue of owning the customer relationship seems to be able to dictate terms to foundation model providers like Open AI."
  },
  {
    "objectID": "blog/2024-12-15-ai-commoditization/index.html#threat-of-substitutes",
    "href": "blog/2024-12-15-ai-commoditization/index.html#threat-of-substitutes",
    "title": "Is the Commoditization of AI inevitable ?",
    "section": "4) Threat of Substitutes:",
    "text": "4) Threat of Substitutes:\nOpen-source and Open Weight Models from the likes of AI2, Meta and the Chinese internet giants offer a substitute to proprietary models. Meta in particular has the resources and personnel to create cutting edge frontier models as evidenced by the Llama family of models. If they continue to open source them through cloud providers, it will represent a very viable alternative to proprietary models. Meta already offers free access to high quality chatbots through their apps."
  },
  {
    "objectID": "blog/2024-12-15-ai-commoditization/index.html#industry-rivalry",
    "href": "blog/2024-12-15-ai-commoditization/index.html#industry-rivalry",
    "title": "Is the Commoditization of AI inevitable ?",
    "section": "5) Industry Rivalry:",
    "text": "5) Industry Rivalry:\nIntense competition among major players (OpenAI, Google DeepMind, Anthropic, Microsoft, AWS, etc.) for market share and innovation leadership.\nWhen Open AI released GPT-4, their lead seemed unassailable, but now Claude Sonnet-3.5 is widely recognized as the best frontier model. This also limits the pricing power that both of these companies have.\nIt is also to be noted that a lot of the talent at Open AI has left to found their own start ups which has lead to increased competition in the space.\nIn summary, at least four of the five market forces point to AI becoming increasingly commoditized."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Govind G Nair",
    "section": "",
    "text": "I am a Senior Product Manager at Oracle, where I lead the development of AI and ML-powered products designed to help Financial Institutions prevent and detect financial crime, making our financial system safer and more secure.\nI am a data scientist turned product manager and an infinite learner constantly seeking to become a little wiser every single day."
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Govind G Nair",
    "section": "Education",
    "text": "Education\n\nMS in Computational Finance and Risk Management, University of Washington, Seattle\nB.Tech in Industrial Engineering and Management, College of Engineering, Trivandrum"
  },
  {
    "objectID": "index.html#interests",
    "href": "index.html#interests",
    "title": "Govind G Nair",
    "section": "Interests",
    "text": "Interests\n\nMachine Learning and AI\nData Science\nProduct Management"
  },
  {
    "objectID": "blog/2021-03-27-3-business-management-skills-to-use-in-your-personal-life/2021-03-27-3-business-management-skills-to-use-in-your-personal-life.en.html",
    "href": "blog/2021-03-27-3-business-management-skills-to-use-in-your-personal-life/2021-03-27-3-business-management-skills-to-use-in-your-personal-life.en.html",
    "title": "3 Business Management Skills to Use in Your Personal Life",
    "section": "",
    "text": "This is a guest article by Stephanie Haywood. For more content like this, please visit https://mylifeboost.com/\nWhen you own a small business, learning how to successfully manage your schedule and productivity is vital. If you’ve mastered these skills, it may be time to put them to good use in your personal life. That’s because most of the tools and tactics you use will translate well. To help you out, here are a few examples and how to use them to create more balance."
  },
  {
    "objectID": "blog/2021-03-27-3-business-management-skills-to-use-in-your-personal-life/2021-03-27-3-business-management-skills-to-use-in-your-personal-life.en.html#asking-for-help",
    "href": "blog/2021-03-27-3-business-management-skills-to-use-in-your-personal-life/2021-03-27-3-business-management-skills-to-use-in-your-personal-life.en.html#asking-for-help",
    "title": "3 Business Management Skills to Use in Your Personal Life",
    "section": "Asking for Help",
    "text": "Asking for Help\nThis is such a big one! But learning how to ask for help can make running your business and life so much easier. After all, no one person is an island — we all need a little help from time to time, whether that help comes from other people or from tech tools and services like an online formation service.\nLet’s talk about that last one first. Forming an LLC can be beneficial for freelancers and small business owners. An LLC will protect your personal assets and set you up for tax savings. Using an online formation service instead of figuring out the paperwork yourself will save you time. It will also help save you money since you can forego costly fees of working with an attorney.\nTools that save you time are amazing! But knowing when to ask for help from other people in your professional and personal life is also important. One example is hiring help around your home. For example, hiring cleaning help can save time and your sanity if you have a family."
  },
  {
    "objectID": "blog/2021-03-27-3-business-management-skills-to-use-in-your-personal-life/2021-03-27-3-business-management-skills-to-use-in-your-personal-life.en.html#embracing-change",
    "href": "blog/2021-03-27-3-business-management-skills-to-use-in-your-personal-life/2021-03-27-3-business-management-skills-to-use-in-your-personal-life.en.html#embracing-change",
    "title": "3 Business Management Skills to Use in Your Personal Life",
    "section": "Embracing Change",
    "text": "Embracing Change\nTo manage tasks for your small business, you need to be flexible. Things can change at a moment’s notice, including the needs of your business. As one example, tech needs for businesses and consumers are always evolving, and how those changes are handled can be one of the keys to success. You also need to be able to adapt how you market innovation.\nIn terms of your personal life, you can think of embracing innovation as building resiliency. If you’re used to riding the tides of change in your business, you can apply this same mindset to your personal life. As a result, you’ll start to feel less stressed and affected by the unexpected.\nOne of the best ways to boost resilience is to take better care of yourself. Making time for self-care routines — like getting exercise, eating healthy and connecting with others — is good for your physical and mental health. Practicing more self-care can also be good for your business — not to mention that self-care will add years to your life for you to enjoy the fruits of your labor."
  },
  {
    "objectID": "blog/2021-03-27-3-business-management-skills-to-use-in-your-personal-life/2021-03-27-3-business-management-skills-to-use-in-your-personal-life.en.html#improving-productivity",
    "href": "blog/2021-03-27-3-business-management-skills-to-use-in-your-personal-life/2021-03-27-3-business-management-skills-to-use-in-your-personal-life.en.html#improving-productivity",
    "title": "3 Business Management Skills to Use in Your Personal Life",
    "section": "Improving Productivity",
    "text": "Improving Productivity\nYou can’t think about task management without thinking about productivity. Chances are, you already have a few tricks up your sleeve to help maximize your productivity levels. Those tricks may include tech tools and apps like Google Drive, Trello and Evernote. If you already know how to use these productivity boosters, you can always try applying them to your personal life. You can simply keep separate folders or even set up separate accounts to maintain work-life balance. There are also apps dedicated to managing your personal schedule.\nFor busy parents with kids and packed schedules, highly rated organizing apps can help tame the chaos. Cozi is a great all-in-one app that syncs your schedules, helps you plan meals, and also makes grocery shopping a snap. There are other options, too, so be sure to explore them.\nIf you can manage your business, there’s no reason why you can’t manage your personal life. In fact, you can achieve both at the same time and achieve more balance in the process. Try applying some of the same tools and strategies you use professionally, and then look for more specific ways to maximize your time and, ultimately, maximize your satisfaction with life.\nPhoto Credit: Unsplash"
  },
  {
    "objectID": "blog/2020-06-06-2019-oreilly-ai-conference/index.html",
    "href": "blog/2020-06-06-2019-oreilly-ai-conference/index.html",
    "title": "2019 Oreilly AI Conference",
    "section": "",
    "text": "These are the notes covering my learning from the talks at the O’Reilly Artificial Intelligence Conference held in San Jose in 2019."
  },
  {
    "objectID": "blog/2020-06-06-2019-oreilly-ai-conference/index.html#facebook-keynote---going-beyond-supervised-learning",
    "href": "blog/2020-06-06-2019-oreilly-ai-conference/index.html#facebook-keynote---going-beyond-supervised-learning",
    "title": "2019 Oreilly AI Conference",
    "section": "1) Facebook Keynote - Going beyond supervised learning",
    "text": "1) Facebook Keynote - Going beyond supervised learning\nBroad classes of Machine Learning outside the classical supervised approaches used at Facebook:\n\nWeak Supervision: Use labels are already available in the data e.g. hashtags in Instagram photos. Facebook built the world’s best image recognition system using the 3.5 billion images shared on Instagram using the hashtags.\n\nA state of the art model was also created on 65 million videos to recognize 10,000 actions using the available hash tags.\n\nSemi Supervised Learning: Use a small amount of labelled data to train a teacher model model and use this model to generate predictions on unlabeled data. Now pre-train a student model on this larger data set and fine tune on the original labelled data.\nSelf Supervised Learning: The best example is Language modelling where you can take a sentence of length N and use the first N-1 words to predict the Nth word or predict a word in the middle using the surrounding words.\n\nAt facebook, self supervised learning based on Generative Adversarial Networks is used to create more robust models that can be used to detect images that have been manipulated by overlaying patterns to get around restrictions such as nudity.\nThis is done by training a GAN to recover the original image from a manipulated image.\n\nFB takes images, overlays it with random patterns. The generator tries to recreated the original while the discriminator tries to discriminate between the originals and the image generated by the generator. The model minimizes the adversarial loss which captures how well the system can fool the discriminator as well as the perceptual loss which captures how well the generator is able to recreate the original.\nAfter training, just the generator can be used to remove patterns from images.\nAnother interesting application of this technique at Facebook is machine translation without labels.\nThis relies on creating automatic word dictionaries. First,you create vector representations for vocabularies in each languages and leverage the fact that the vector representations of the same word in different languages share structure to learn a rotation matrix to align them and produce a word by word translation.\nThen you build a language model in each language to learn the sentence structure in each language. This language model can then be used to reorder the word by word translation produced in the previous step. This gives you an initial model to translate between the two languages (L1 to L2 and L2 to L1).\n\nNow a technique called back translation can be used to iteratively improve the model. In this method, you take the original sentence in Language 1 and translate this to Language 2 using the initial L1 to L2 model. Now you translate this back to language 1 using the L2 to L1 model with the original sentence in Language 1 being the target. This will allow you to improve the L2 to L1 model. By flipping the order of the translations, you can also improve the L1 to L2 model\n\n\nReinforcement Learning:\n\nFacebook uses RL to recommend notifications to users on Instagram.\nFB also has created a platform called Habitat ‘a research platform for embodied AI agents to operate in real world-like environments and solve tasks like navigation, question-answering, et cetera’."
  },
  {
    "objectID": "blog/2020-06-06-2019-oreilly-ai-conference/index.html#operationalizing-ai-at-scale-from-drift-detection-to-monitoring-business-impact-ibm-watson",
    "href": "blog/2020-06-06-2019-oreilly-ai-conference/index.html#operationalizing-ai-at-scale-from-drift-detection-to-monitoring-business-impact-ibm-watson",
    "title": "2019 Oreilly AI Conference",
    "section": "2) Operationalizing AI at Scale: From drift detection to monitoring business impact | IBM Watson",
    "text": "2) Operationalizing AI at Scale: From drift detection to monitoring business impact | IBM Watson\nNote: See my post on concept drift for a general understanding of the problem.\n\nFair Credit Act requires a sample of model decisions to be explained to the regulator. Same with GDPR.\n\nWatson Openscale provides the following capabilities:\n\nImportant to realize high model performance doesn’t always correlate to high business performance. Need to correlate model KPIs and Business KPIs. IBM openscale will automatically correlate model metrics in run time with business event data to inform impact on business KPIs.\nTypes of concept drift:\n\nClass Imbalance. e.g. The imbalance ratio shifts.\nNovel Class Emergence e.g. A new category of chats in a chatbot\nExisting Class Fusion e.g. A bank wants to merge the loan approved class with the loan partially approved class\n\nAccuracy Drift: Has model accuracy changes because of changing business dynamics? This is what businesses really care about. This could happen due to:\n\nDrop in Accuracy: Model accuracy could drop if there is an increase in transactions similar to those which the model was unable to evaluate correctly in training data\n\n\nUse training and test data to learn what kind of data the model is making accurate prediction on and where it is not. You can build a secondary model to predict the model accuracy of your primary model and see if the accuracy falls\n\n\nDrop in data consistency: Drop in consistency of data at runtime compared to characteristics at running time.\n\nE.g. % of married people applying for your loan has increased from 15% to 80%. These are the kind of explanations that are accessible to a business user.\n\nOpen scale can map a drop in business KPI to the model responsible for the drop and identify the samples of data that has changed."
  },
  {
    "objectID": "blog/2020-06-06-2019-oreilly-ai-conference/index.html#applying-ai-to-secure-the-payments-ecosystem-visa-research",
    "href": "blog/2020-06-06-2019-oreilly-ai-conference/index.html#applying-ai-to-secure-the-payments-ecosystem-visa-research",
    "title": "2019 Oreilly AI Conference",
    "section": "3) Applying AI to secure the Payments Ecosystem | Visa Research",
    "text": "3) Applying AI to secure the Payments Ecosystem | Visa Research\nData Exfiltration: Unauthorized copying, transfer or retrieval of data\nMerchant Data Breach: Occurs when an unauthorized part accesses a merchant’s network and steals cardholder data. Cardholder data is later used to commit fraud.\nOnce a breach occurs, the criminals may enrich the data they have collected and then bundle and resell it on the dark web.\nVisa’s goal is create an AI based solution to detect such breaches as early as possible with high accuracy and recall.\n\nSolution\nVisa uses a semi supervised CNN for label augmentation given incidences of data breaches are extremely rare. For detection or classification of breaches a supervised CNN + a Deep Neural Net is used.\nFor each merchant, a table is created recording the first instance a card is used at a merchant along with subsequent transactions at the same merchant with any incidence of fraud( the fraud may have happened at the same or different merchant). This table is transformed into an image which captures the same information.\n\nIn instances where no breach occurred, the image would show authorizations(green) and fraud(red) being distributed across the image whereas for instances where breach occurred, there would be a clear separation between the two.\nA CNN is trained on the available labelled data to extract features that are indicative of a data breach. This model is used as a ‘feature classifier’ (in image below) to label the unlabeled data. The enriched data with labels is then used to train a second classifier to actually detect fraud.\n\nThe second classifier uses more near time features to enable early detection of breaches.Time series features are prominently used. Multi channel time series that combine multiple time series are created and this is fed into a CNN, The features extracted by the CNN along with more static features such as type of merchant or location of merchant are fed into a deep neural network with the labels being those generated by the first CNN.\n\nThe final solution looks as follows"
  },
  {
    "objectID": "blog/2020-06-06-2019-oreilly-ai-conference/index.html#using-deep-learning-model-to-extract-the-most-value-from-360-degree-images-trulia",
    "href": "blog/2020-06-06-2019-oreilly-ai-conference/index.html#using-deep-learning-model-to-extract-the-most-value-from-360-degree-images-trulia",
    "title": "2019 Oreilly AI Conference",
    "section": "5) Using Deep Learning model to extract the most value from 360-degree images | Trulia",
    "text": "5) Using Deep Learning model to extract the most value from 360-degree images | Trulia\nTrulia needs to rank candidate images based on an objective which can be quality, relevance, informativeness, engagement, diversity or personalization. If there are multiple images, a few need to be selected to be displayed to the user.\nImage selection can be based on behavioral data or through content understanding.\n\nSelecting Hero Images for Real Estate Listings\nReal Estate images can vary widely in quality and content. Zillow needs to select a primary image(hero image) to display along with the listing.Typically, exterior images are selected as the primary image which may not be appropriate.\nA good hero image must be: * Attractive * Relevant - Informative of the listing * Appropriate - No irrelevant advertisements\n\n\nTurning 3D panoramas into thumbnails\nThe goal is to select an appropriate thumbnail shown on the upper half of the image from the panorama image below it.\n\nThis can be done by using an equiangular projection as a thumbnail or by selecting a thumbnail from possible rectilinear projections.\nThe first approach is to present the entire equiangular projection.\n\nClearly this results in some distortions\nThe second approach is shown below. Here you project a selected portion of the panorama into the 2D domain.\n\nHere you need to select the most salient thumbnail from multiple possibilities. A salient thumbnail must be informative, representative, attractive and diverse.\nFor example. Image C below is the most salient thumbnail.\n\nThe Thumbnail Extraction Pipeline is as follows\n\nOptimal Field of View (FOV) Estimation\n\nThis has to be done because different cameras has different vertical FOV. For e.g. an I-phone has a sixty degree field of view and has to be padded with zero pixels while more professional cameras have an 180 degree vertical field of view .\n\nExtracting 2D viewpoints from 360 degree panoramas\nExtract saliency attributes and compute saliency scores\nRank candidate viewpoints by sailency scores\nApply diversity filter and return top N candidates"
  },
  {
    "objectID": "blog/2020-06-06-2019-oreilly-ai-conference/index.html#computing-sailency-scores",
    "href": "blog/2020-06-06-2019-oreilly-ai-conference/index.html#computing-sailency-scores",
    "title": "2019 Oreilly AI Conference",
    "section": "Computing Sailency Scores",
    "text": "Computing Sailency Scores\nTrulia developed 3 different CNN models for - Image Attractiveness - Image Appropriateness - Image Scene Understanding\nAlso, gaussian kernel smoothing is used to account for the fact the models are trained on 2d images while the test data are 360 degree panorama images.\nThe aggregate score from the 3 models is used to compute overall salience score per viewpoint."
  },
  {
    "objectID": "blog/2020-06-06-2019-oreilly-ai-conference/index.html#training-sailency-models",
    "href": "blog/2020-06-06-2019-oreilly-ai-conference/index.html#training-sailency-models",
    "title": "2019 Oreilly AI Conference",
    "section": "Training Sailency Models",
    "text": "Training Sailency Models\n\nResent/Inception architectures pretrained on Imagenet/Places365 are fine tuned on internal Scenes60 data set.\nThis model is then fine tuned on Trulia’s Appropriateness and Attractiveness data sets.\nBinary Cross Entropy loss is used for the Appropriateness and Attractiveness data sets while a 60 way categorical cross entropy loss is used for the scene understanding model."
  },
  {
    "objectID": "blog/2020-06-06-2019-oreilly-ai-conference/index.html#estimating-image-attractiveness",
    "href": "blog/2020-06-06-2019-oreilly-ai-conference/index.html#estimating-image-attractiveness",
    "title": "2019 Oreilly AI Conference",
    "section": "Estimating Image Attractiveness",
    "text": "Estimating Image Attractiveness\nThis is set up as a supervised problem and requires labels that can be quality ratings on some scale or pairwise comparisons. However this is highly subjective and human labels are expensive to acquire.\nSo a weakly supervised approach using the meta data available from the listings is used. Home price is used as an alias for attractiveness as luxury homes have professionally taken ,well staged photos while low price homes typically have low quality images.\nThe Grad CAM approach is used to visualize saliency to ensure the model is learning the right concepts from these weak labels.\n\nTo ensure diversity in top N recommendation, maximal marginal relevance - MMR is used"
  },
  {
    "objectID": "blog/2020-06-06-2019-oreilly-ai-conference/index.html#a-framework-to-bootstrap-and-scale-a-machine-learning-function-workday",
    "href": "blog/2020-06-06-2019-oreilly-ai-conference/index.html#a-framework-to-bootstrap-and-scale-a-machine-learning-function-workday",
    "title": "2019 Oreilly AI Conference",
    "section": "6) A framework to bootstrap and scale a machine learning function | Workday",
    "text": "6) A framework to bootstrap and scale a machine learning function | Workday\nWorkday uses a single platform on top of which all applications such as payroll, HCM live. The platform simply exposes API for the application teams to use.\n\nWorkday is used by 40% of Fortune 500 and 50% of Fortune 50 giving them vast amounts of clean data to train ML models.\nChallenge: ML service to scan receipts and auto populate an expense report. Ship in 6 months.\nVery import to Define the Win and define success metrics: For this project goal was to create an ML Service that scans receipts with 80% accuracy that is delivered in private and public clouds for all customers in 6 months.\nOff the shelf OCR solutions work best for traditional text and not for receipts.\nSimulating data was not sufficient to get to the expected level of accuracy. Workday did a receipt contest to get employees to upload receipts. 50,000 receipts were invested. Labeling was done by a data labeling team.\nDeep learning framework chosen was MXNet for its Scala and Python support.\nStep 1: Bounding Box Detection\n\nA deep learning model based on residual networks outputs center of the box, height,width ,angle of title and confidence.\n\nStep 2: Text Recognition\n\nA deep learning model based on residual networks outputs text\n\nStep 3: Mapping\n\nAn assortment of models(rule based + deep learning ensembles) that maps a value to a field. E.g. 63.87 is a ‘Total’ and 1/27/2018 is a ‘Date’\n\nAll components were built using microservices architecture.\nThe Production architecture at Workday consists of an ML platform that supports CI/CD deployment,ML as a Service is built on top of this ML Platform which is consumed by Non ML Services and a UI.\nWhen going from 0 to 1 in a new area such as deploying first ML Application, follow the START framework.\nS: Select One Win (Unambiguous Value). Get alignment on success criteria  T: Team. Smaller teams work best.(&lt; 1 pizza pie) Time bound (3 - 6 months). Focus on Learning  A: Articulate(Win and Gameplan) and Align(stakeholders)  R: Rally and Support - Protect team from external noise  T: Take Shortcuts (tech debt -accrue it). Speedy learning is the focus so this is ok. Target release for only one or two customers.\nFinally - GET\nG: Get Credit for the win and Gather Capital  E: Establish repeatable processes and platform  T: Transfer learnings to scale to 10"
  },
  {
    "objectID": "blog/2020-06-06-2019-oreilly-ai-conference/index.html#artifical-and-human-intelligence-in-healthcare-google-brain-cornell-university",
    "href": "blog/2020-06-06-2019-oreilly-ai-conference/index.html#artifical-and-human-intelligence-in-healthcare-google-brain-cornell-university",
    "title": "2019 Oreilly AI Conference",
    "section": "7) Artifical and Human Intelligence in Healthcare | Google Brain & Cornell University",
    "text": "7) Artifical and Human Intelligence in Healthcare | Google Brain & Cornell University\n\nTransfer Learning\n\nTranfer learning is widely used today. More pretraining data is not necessarily better data. Pre-training on curated data is often better. Paper\nBetter performance on imagenet does not always mean better performance on the target task. It depends on factors such as regularization and dataset.\n\n\n\nTransfer Learning in Medical Imaging\nTasks: Chest X -rays and diagnosing diabetic retinopathy  Models: Resent 50 and Inception v3. Also considered smaller lightweight architectries which consisted of sets of CBR (convolution+batchnorm_relu+maxpool) layers\n\nTakeways\n\nTransfer learning and Random initialization performed comparably in many settings\nCBRs perform as well as the big imagenet models\nImagenet performance is not indicative of medical performance\nIn medical applications, you typically have much smaller datasets for fine tuning. On a smaller dataset of 5,000 images the results are as follows. The lift from transfer learning is not signficant.\n\n\nThis suggests that the typical imagenet models might be overparameterised for tasks of interest.\n\n\nRepresentational Analysis of Transfer\n\nThe goal is to learn whether irrespective of initialization , pretrained and randomly intialized networks learn the same concepts in the latent layers. However this is difficult as comparing two networks is challenging due to the distributed alignment problem. Features learned by one network don’t align with those learned by another network. E.g. one network might learn to recognize a dog using a single neuron ,while another network uses concepts learned by three different neurons to identify a dog.\nWas carried out using CCA\n\nResults of comparing representations shown below show that networks trained with random intialization are more similar to each other than those trained with pretrained initializations.\n\nEven though performance is comparable, the latent representations learned are different.\nIt was also observed large overparametrized models change less through training.\n\nTakeaway: Use pretrained weights for lower layers. Redesign and streamline higher layers of the architecture\n\nHaving pre-trained weights converges much faster than random initialization. To speed up convergence of random weights, it is shown that keeping the scaling of the pretrained features,i.e.draw iid weights with same mean and variance as pretrained features, really helps.\n\n\n\n\nAI for Health in Practice\n\nNeed to think about how AI systems work with human experts\nDoctor disagreements occur often. AI can be used to predict doctor disagreements → Direct uncertainty prediction(DUP)\nThis is not far from predicting doctor error($ P_{herr} $). Uncertainty in prediction also gives a sense of AI error $ P_{AIerr} $\nCan rank cases according to the difference between AI error and doctor error. On one end you have cases where AI error is very high and doctor error is low while on the other end you have cases where AI error is low and doctor error is high. For cases at the latter end, you can deploy an AI system, available human doctor budget can be deployed on other cases.\nIn practice you have some fraction of cases being looked at by the AI system, by doctors or by both. Typically a combination of both works best"
  },
  {
    "objectID": "blog/2020-06-06-2019-oreilly-ai-conference/index.html#deep-reinforcement-learning-to-improvise-input-datasets-infilect-technologies",
    "href": "blog/2020-06-06-2019-oreilly-ai-conference/index.html#deep-reinforcement-learning-to-improvise-input-datasets-infilect-technologies",
    "title": "2019 Oreilly AI Conference",
    "section": "8) Deep Reinforcement Learning to improvise input datasets | Infilect Technologies",
    "text": "8) Deep Reinforcement Learning to improvise input datasets | Infilect Technologies\n\nData augmentation is used widely in for training computer vision models. This talk addresses how to optimally prepare data sets using data augmentation.\nSynthesizing images using GANs is also an option, but GANS are difficult to train and reproduce.\nIn the objection detection task being addressed here, there are certain minority classes which might need different types of data augmentation.\nTypes of augmentation typically used are:\n\n\nColor augmentations - Change pixel intensity\nGeometric Augmentations - Shear ,Crop, Translation etc,\nBounding Box augmentations - Change a specific object in an image.\n\nYou can also encounter situations where certain types of augmentations should be strictly avoided. E.g. flipping images of digits, or contrasting traffic light images.\nThe goal is to create an automated data augmentation system so that you don’t have to manually try different types of augmentations and iterate to find the best model as shown below.\n\nThe proposed solution is 1) To learn a set of augmentation policies for the tagged dataset 2) Apply these augmentations to the dataset to create an augmented dataset for training\nPolicy: Apply N augmentations where each augmentation is applied with a magnitude and probability\nStrategy: A set of policies\nAt training time, for an image, you sample a policy and apply the policy. Applying two different policies to the same images will result in 2 different augmented images.\n\nFormulation of Strategy\nA: Universe of augmentations\nC: Classes in the dataset\n$ V_{a,c} $: Magnitude of augmentation a for class c\n$ P_{a,c} $: Probability of augmentation a for class c\nThese are two parameters we want to learn for each possible augmentation.\nThe solution is as follows:\nA controller will generate a sample of a strategy.The environment trains the model using the strategy. The accuracy of the model is the reward based on which the controller is tuned.\nSSD: Single Shot Detector\n\n\n\nController\nThis uses an LSTM architecture which each outputs three softmaxes corresponding to 1) C classes 2) A augmentations 3) B buckets indicating magnitude of transformation\nThere are four LSTMs where the first proposes the class, the second proposes the augmentation for this class, the third proposed a magnitude for this augmentation while the fourth proposes a probability.\nTogether it says, for class c, apply augmentation a with magnitude b with a proposed probability. For each additional policy you want to learn, you have to add 4 more LSTMs.\n\nTraining is carried out using proximal policy optimization\nBased on the accuracy of the model, positive or negative reinforcement can be carried out to tune up or tune down the probability of a certain augmentation scheme.\nThe final results are as follows:\n\n\nThe system learned to carry out augmentations like contrast,edge augmentations, image crops and bounding box flips rather than rotation,blur or shear that are typically used.\nBased on auto augment by Google"
  },
  {
    "objectID": "blog/2020-06-06-2019-oreilly-ai-conference/index.html#defragmenting-the-deep-learning-ecosystem-determined-ai",
    "href": "blog/2020-06-06-2019-oreilly-ai-conference/index.html#defragmenting-the-deep-learning-ecosystem-determined-ai",
    "title": "2019 Oreilly AI Conference",
    "section": "9) Defragmenting the Deep Learning Ecosystem | Determined AI",
    "text": "9) Defragmenting the Deep Learning Ecosystem | Determined AI\n\nChallenges in AI Infrastructure\n\nMight take days to recover from faults. Training has to start all over again if a session fails\n\n\nThis is particularly a challenge when training models like GANs that can take a couple of days\nThere are options to save and reload models intermittently e.g. using tf.saved_model.simple_save and tf.saved_model.loader.load in TF.\nThese might be inadequate as TF only saves weights and the optimizer state when the user also needs to save the TF version, random seeds, model definitions and input read positions which requires custom code. In summary your Deep Learning engineer has to spend a lot of time writing boiler plate code.\n\n\nReproducibility\n\n\nCore tenet of science\nImportant for collaboration so that models can be handed off to a colleague\n\nReproducibility is a challenge in DL because of\n\nArchitecture\nData - New data could be added\nInitializations e.g for hyper parameters\nStochastic nature of optimization techniques\nNon deterministic GPU operations\nCPU multi-threading meaning operations could be carried out in different orders\n\nAddressing this requires significant engineering expertise and use of containerization to pin versions of all underlying libraries.\nImperfect solutions are available in PyTorch and TF\n\nHand implemented and slow tuning methods\n\n\nNeeds cluster management to parallelize testing of multiple combinations of hyper parameters but there is no support for mete data management, fault tolerance or efficient allocation.\n\n\nSystems are not designed for multi-tenancy and sharing of resources\n\n\nClusters today don’t understand the semantics of deep learning\nDL frameworks are built to train a single model for a single user on a single machine\nCompanies often use a calendar schedule to allocate resources between team members or go for fixed assignment.This can lead to low utilization and low scalability.\nCan use tools like kubernetes, Yarn or Mesos but these solutions do not offer job migration or auto scaling and is not much better than a queue\n\nOnly FAMG companies have the necessary infrastructure\nMost tools like PyTorch and TF are focused on just the model training component of the modeling life cycle shown below\n\nDetermined AI is focused on holistic and specialized AI software infrastructure.\n\n\nIdeal AI Infrastructure\nIdeal AI infrastructure will address these challenges by offering the following\n\n\n\n\nCheck pointing would be taken care of OTB\nInfrastructure would monitor and retry failed jobs automatically from the latest checkpoints automatically\nInfrastructure would manage its own checkpoint storage according to user defined rules (e.g. keep models with n best validation errors)\nInfrastructure could leverage checkpoints to enable reproducibility or distributed training\nAll of this would be transparent\n\n\nReproducibility should be addressed by providing the following features\n\n\n\n\n\n\nHyper band is a resource optimized hyper parameter tuning technique that uses active learning and early stopping.It performs far better than conventional techniques\n\n\n\nOn average, it is 50x faster than Random search, 10x faster than Bayesian optimization\nAlso works really well for Neural Architecture Search. Outperforms Google’s internal system by 3x\n\nA holistic and specialized AI infrastructure that Determined AI has built looks like this:"
  },
  {
    "objectID": "blog/2020-06-06-2019-oreilly-ai-conference/index.html#industrialized-capsule-networks-for-text-analytics-walmart-labs-publicis-sapient",
    "href": "blog/2020-06-06-2019-oreilly-ai-conference/index.html#industrialized-capsule-networks-for-text-analytics-walmart-labs-publicis-sapient",
    "title": "2019 Oreilly AI Conference",
    "section": "10) Industrialized Capsule networks for text analytics| Walmart Labs & Publicis Sapient",
    "text": "10) Industrialized Capsule networks for text analytics| Walmart Labs & Publicis Sapient\nBroad overview of NLP and Text Analytics\n\n\nLSTMs outperforms CNNs for text classification as there are long term dependencies in text and outputs are not driven by just one or two key words or phrases.LSTMs also care about ordering of features whereas CNNs do not.\n\nTo make CNNs work, kernels with large dimensions have to be used meaning number of parameters increase exponentially. Pooling also leads to loss of information.\n\nCNNs are invariant, so input perturbations do not change output . It is unable to recognize transformations\nCapsules are equivariant. Outputs change when input changes due to perturbations and hence they are able to recognize transformations of input.\n\n\nOverview of Capsule Networks\n1) Moving from Scalar to Vector. Pooling produces a scalar in CNNs. In a capsule network, the output is a vector instead.\n\n\nThe output vector is able to encode multiple properties of the input region such as color, thickness etc.\nVector length also encodes the probability of the output class.\n\n2) Capsule Output Calculation - Forward Pass\n\n\nApply an affine transformation (matrix multiplication that preserves lines and parallelism). This is supposed to capture the relationship between various components (e.g. face, nose, ear) to the image (face) in the higher layer. Or how ordering of words affect sentiment\nCoupling coefficient learns the strength of the relationship between two layers\nSquashing function changes the length of the vector so that it is between 0 and 1 so that it encodes a probability\n\n3) Dynamic Routing - Backward Pass\n\nAssume there is one lower level capsule and two higher level capsules as shown above. If more lower level capsules are in agreement, assign higher coefficients whereas if lower level capsules disagree(e.g. eyes are spatially co-located), assign lower coefficients.\nThe Capsule network architecture is shown below. \nPaper on application of capsule networks to text classification: https://arxiv.org/abs/1804.00538\n\nCapsule networks outperforms other networks for multi-label classification with little additional data\n\n\n\nIndustrialization of Capsule Networks\n\nKubeflow has been used to productionize capsule networks"
  },
  {
    "objectID": "blog/2020-06-06-2019-oreilly-ai-conference/index.html#building-facebooks-visual-cortex-facebook",
    "href": "blog/2020-06-06-2019-oreilly-ai-conference/index.html#building-facebooks-visual-cortex-facebook",
    "title": "2019 Oreilly AI Conference",
    "section": "11) Building Facebook’s visual cortex | Facebook",
    "text": "11) Building Facebook’s visual cortex | Facebook\nFacebook platform has evolved from being a text heavy platform to a photo and video heavy platform including AR/VR.\nSome applications of applications of visual cortex\n\nAutomatic Alternative Text:\n\nOver 200 million visually impaired people use screen readers to navigate websites. These usually directly reads sentences tied to the images that are usually manually hand curated by the person uploading the images.\nThese sentence descriptions are automatically generated by taking the outputs from FB’s OCR and Image classification system (that categorizes images into various categories)\n\nInstagram explore ranking.\n\nCreate a personalized news feed based on type of photos you upload.\n\nViolating Content Classifiers\n\nVision signals are used as features in multi-modal fusion classifiers that can identify policy violating content.\n\nVisual similarity\n\nUse identified violations (e.g. Marijuana advertisements) to identify similar violations\nVisual Cortex is a system that operates on a stream of uploaded images and videos. It runs models on these and stores results. It uses the same backbone model across all the above tasks.\nSome examples:\n\nModel using MLP + Softmax to identify policy violating content\nModel that works as a feature extractor that produces embeddings for downstream models\nModel to produce embeddings, compress it and use it in a KNN model\n\nMajor considerations while building Visual Cortex given FB’s immense scale:\n\nData annotation is costly\nDynamic demands and trends . e.g. Identify ice bucket challenge. This is a long tailed problem\nDependencies due to downstream models. Continuously improving visual cortex is challenging given these dependencies\nEfficiency. To run 1 billion images(1 day of data) on 1 machine sequentially through RexNeXt 101 takes 1,736 days.\n\n\nReducing Dependency on Supervision\n\nWeakly supervised Learning: Noisy Labels\n\nConsider using Instagram hashtags. This could have issues such as non-visual tags, missing tags or wrong tags.\nProposed Approach: Pre-train a model to predict hashtags and then transfer this to an image classification model\nDetails are shown below.\n\nFindings:\n\nAs size of data used for pre-trained model increases, the performance on the target task increases.\nInitially used 17,000 hash tags, then reduced to 1000 hash tags to match Imagenet classification task. Accuracy improved as the labels space for both are now similar.\nAs model capability grows in a weakly supervised setting, final accuracy improves. This does not hold for fully supervised models.\n\nb)Semi Supervised Learning: Small amount of labelled data + Large volume of unlabeled data\nThis was considered as\n\nHash tag data is not accessible outside Instagram\nNot utilizing large amounts of unlabeled data\nHard to deploy very large models\n\nProposed Approach:\n\nTrain teacher model(e.g. RexNeXt 101) on labelled data (e.g. Imagenet)\nRun teacher model on unlabeled data\nTake top-k images per each concept/class\nTrain a smaller student model (e.g. ResNet - 50) and fine tune on original image net dataset\nUse student model as a pre-trained network.\n\nFindings: As volume of unlabeled data used increases, accuracy of student model on target task also increases.\n\nSelf Supervision: No labels\n\n\nE.g. Create a model to reassemble an image from scrambled pieces like solving a jigsaw puzzle.\n\nSuch a model will have enough semantic information to transfer to a different task.\nGreater the data used for this pre-text task, greater the accuracy of the transfer learned model on the final task.\nOn Imagenet 1K, performance of self supervised methods falls well short of semi supervised techniques.\n\nHuman labelled data is still a critical component of ML systems.\n\n\nEfficiency\n\nModel Size - Quantization can decrease accuracy\n\nIdea 1: Share same pre-trained network across multiple tasks; run all images only once through this trunk. The common trunk will have multiple heads fine tuned for various tasks\nIdea 2: Invest in efficient backbone architectures.\nIdea 3: Invest in efficient operations such as octave convolutions. This has reduced processing power required by 40% and forward pass latency drops by 50% for a ResNet 50\n\n\nContinuous Improvement\n\nDefine good model APIs to influence the architecture of upstream systems\nContract with downstream systems for compatibility support.\n\nPredictably push new backbone every N months with N -1 backwards compatibility.\n\n\nExample: If a new backbone(V2) has been developed to generate better embeddings. Instead of running v1 and v2, only v2 will be run to emit both v1 and v2 embeddings. This old embedding is backwards compatible and is realized using knowledge distillation. - List of concepts in v2 will be a super set of v1 and impersonate old calibrated scores using a look up table\n\n\nDynamic Demands\n\nCreated platform to quickly train linear classifiers\nuse active learning to quickly get high quality labels from annotators\nConcept drift measured programatically and unit tests to ensure outputs are consistent from day to day.\n\n\nSlide deck available here"
  },
  {
    "objectID": "blog/2020-06-06-2019-oreilly-ai-conference/index.html#deep-learning-in-the-tire-industry-american-tire-distributors-atrd",
    "href": "blog/2020-06-06-2019-oreilly-ai-conference/index.html#deep-learning-in-the-tire-industry-american-tire-distributors-atrd",
    "title": "2019 Oreilly AI Conference",
    "section": "12) Deep Learning in the Tire Industry | American Tire Distributors (ATRD)",
    "text": "12) Deep Learning in the Tire Industry | American Tire Distributors (ATRD)\nCharacteristic of the Tire industry: - Purchases are rare and so data is sparse - Tires are manufactured by certain companies and distributed by ATD to retailers such as Walmart or Costco\nBusiness Problems in Pricing: - Price performance monitoring - Pricing opportunity identification - Dynamic pricing recommendations\n\nAddressed using Anomaly detection, time series decomposition, demand modelling(discounts), pricing recommendations\n\nBusiness Problems in Operation - Warehouse staffing efficiency - How many people required at a given location and at a given point in time?\n\nAddressed using Labor demand forecasting and labor optimization\n\n\nForecasting Staffing requirements\n\nData collected include each staff associate’s activity, number and type of tires being handled. The approach uses dynamic weighting of a Prophet model(Traditional time series) and an LSTM model.Forecasts are made for each distribution center.\n\nSaved 5-10% in labor costs with this approach.\n\n\nPricing Product Demand Model\nCombination of a market demand model and MIP optimization to simulate pricing strategies for different products.\n\nThe two green blocks show modules where Deep Learning is used. Given data is sparse, sales were aggregated at a week and distribution center level. An embedding for the time series is generated by an auto encoder which is then clustered. The clustering identifies product buckets such as snow tires and more commonly bought tires with different profiles.\nA new model is built for each identified cluster.The final model is obtained by stacking a Random Forest Model, an XG Boost model and a DNN model. The second level stacking model is also a DNN.\nModel stacking helped with overfitting and accounting for anomalous behavior.A single model is unable to account for such anomalous behavior. It was also observed that different models perform best for different clusters,so stacking helps avoid manually selecting models for different clusters. DNNs perform best when lots of data are available, tree based models perform better for sparse data.\nData drift is not a concern as tire demand is largely predictable apart from expected seasonality."
  },
  {
    "objectID": "blog/2020-06-06-2019-oreilly-ai-conference/index.html#a-framework-for-human-ai-integration-in-the-enterprise-rakuten",
    "href": "blog/2020-06-06-2019-oreilly-ai-conference/index.html#a-framework-for-human-ai-integration-in-the-enterprise-rakuten",
    "title": "2019 Oreilly AI Conference",
    "section": "13) A framework for Human AI Integration in the enterprise | Rakuten",
    "text": "13) A framework for Human AI Integration in the enterprise | Rakuten\nMoaravec’s paradox: What is easy for humans is difficult for machines and what is easy for machines is difficult for humans\n\nAI can perform well on the head of the distribution while humans work better on the tail.\nMachines are susceptible to adversarial attacks\n\n\nExternal Factors\n\nRegulations sometimes point to AI-human integration. E.g. Article 22 of EU GDPR requires a human in the loop for providing explanations.\nHigh risk applications like medical diagnosis often require human in the loop to mitigate risk\n\n\n\nCompetitive Advantage\nHumans in the loop can reduce the need for:\n\nHuge volumes of labeled training data\nMassive amounts of computing resources\nArmies of data scientists\n\nHuman in the loop can overcome limitations in an algorithm’s accuracy\n\n\nDialog\nHuman - AI integration requires a dialogue between AI and humans\n\nThe human participant can be the end user in cases where the risk is low. e.g A/B Testing , Multi armed bandits\nIf the risk is medium, crowd sourcing can be used . E.g. product categorization\nIf the risk is high, use your own workforce. E.g. data scientists or legal teams to comply with regulations\n\nDesigning a dialogue interface requires an understanding of human psychology, design thinking and paradox of automation. i.e. as automation increases, human skills erode and human intervention cannot be relied upon.\n\n\nAI Understanding Humans\n\nSupervised Learning with training labels is the best example of this.\nActive learning allows AI to ask questions of humans when it is not confident in it’s predictions. This also reduces redundancy and reduces manual labeling effort.\n\n\n\nHuman Understanding AI - Explainability\nExplainability algorithms can be classified along the following dimensions:\n\nModel agnostic vs Model specific\nLocal vs Global\nFeatures vs Instances vs Surrogates\n\nE.g. If a bank tells you a loan was rejected because your income was low: It is a local, feature based explanation. If the explanation for a diagnosis is that the patient is similar to other patients which were diagnosed: it is a local, instance based explanation.\nRakuten has a hierarchical product classifications system that classifies products based on a description. E.g. Electronics → Cell Phone → Smart Phone. The model provides explanations for the classification. E.g. Electronics because it found ‘256 GB’ in the description. Smartphone because it found ‘I phone’ in the description.\nSome of the explanation algorithms provide justifications rather than explanations.\nExplainability: Optimize interpretability given a model  Modifiability: Optimize a model given interpretability. Make interpretability a constraint that needs to be satisfied,and optimize the model.\nDefine an abstraction layer that humans can understand and modify.  E.g. A steering wheel, gear shaft and gas pedal. How this affects the engine is abstracted.\nE.g. You can develop a black box model to extract order number form a receipt but this can be hard to debug. Instead, Rakuten build a model that generates a template; a piece of code that can be run to extract the order number. Human operator can modify this template.\nBenefits of modifiability: * Mitigate accuracy - interpretability tradeoff * Enable debugging even by non-AI specialists * Maintain high quality of end results. Human has to intervene if model is not performing well.\nOther example: Stitch fix algorithms that recommends clothes but is modified by stylists. The selection of clothes acts as an abstraction layer.\n\n\nSystem Design with Human- AI Integration\n\nThink of humans and AI as components in a system.\nThe layer of abstraction is the knowledge graphs which has products and entities and two types of relations. A product can belong to a category and categories are related to each other. The system recommends products from a complementary category to the user.\nProduct categorization has to be monitored. Manually identifying misclassifications is not possible. This can be reduced to an anomaly detection problem. e.g. identify a wine product being misclassified in the tire category.\nHumans can thus focus on anomalies and label just those.\n\nRakuten has moved folks who work as humans in the loop to more AI centric roles."
  },
  {
    "objectID": "blog/2020-06-06-2019-oreilly-ai-conference/index.html#data-science-without-seeing-the-data-advanced-encryption-to-the-rescue-intuit",
    "href": "blog/2020-06-06-2019-oreilly-ai-conference/index.html#data-science-without-seeing-the-data-advanced-encryption-to-the-rescue-intuit",
    "title": "2019 Oreilly AI Conference",
    "section": "14) Data Science without seeing the data: Advanced encryption to the rescue | Intuit",
    "text": "14) Data Science without seeing the data: Advanced encryption to the rescue | Intuit\n\nExplosion in data and number of people accessing it poses information security risks.\nGoal is to do ML without seeing the underlying data.\n\nFully Homomorphic Encryption(FHE): Encrypts for security without losing the information necessary for computation.\nMicrosoft’s SEAL is a popular open source toolkit used for FHE. It abstracts the advanced mathematics for encryption for software engineers.\nAlso see here for a more thorough introduction.\nAll computations cannot be done using FHE\n\nNot possible to do IF statements i.e. threshold functions.\nUse case in Medicine:A gives B encrypted medical data. B runs proprietary algorithm and gets an encrypted diagnosis.A accepts this and decrypts it to get diagnosis.\n\nIntuit wants to encrypt all customer data on AWS and still use the data to run ML.\nTree based models are most widely used at Intuit, but you actually need to do branching which is hard with FHE.\nThresholding functions required to split decision trees can be approximated using a polynomial function. \n\n\nDecision Tree Training\nUnlike regular decision tree where the data is split and only a partition of the data is handed down to subsequent nodes of the tree,with FHE, the entire data has to be passed down through each layer as the data is encrypted. Only when you decrypt the full expression is an observation assigned to a leaf.\nThis can be intractable if the trees are very deep.\nInference with FHE is still very slow. In 2009, the slowdown was $ 10^7 $, in 2019 it is $ 10^4 $\n\nBatch training and inference can be done on encrypted data.Not suitable in an online setting."
  },
  {
    "objectID": "blog/2020-06-06-2019-oreilly-ai-conference/index.html#mozart-in-the-box-interacting-with-ai-tools-for-music-creation-midas",
    "href": "blog/2020-06-06-2019-oreilly-ai-conference/index.html#mozart-in-the-box-interacting-with-ai-tools-for-music-creation-midas",
    "title": "2019 Oreilly AI Conference",
    "section": "15) Mozart in the Box: Interacting with AI Tools for Music Creation | Midas",
    "text": "15) Mozart in the Box: Interacting with AI Tools for Music Creation | Midas\n\nFear of automation is real: Even for an autonomous car, people would like to see the speedometer reading.\n\nComplex systems are hard to innovate on, the airplane pilot’s complex is still extremely complex despite all the innovation\n\nDesigned by experts for experts\nHave many critical parameters ad controls\nRequire intense training and learning effort\nMistakes and failures must be avoided at all costs\nThe users are extremely conservative\n\nComplex systems are also liable to the Active User paradox which reduces user motivation to spend any time just learning about the system. This can manifest in a few ways\n\nProduction Bias: When situation appear the could be more effectively handled by new procedures,they are likely to stick with procedures they already know regardless of efficacy. You want to start using something without reading the user manual\nAssimilation Bias: People apply what they already know to new situations. Irrelevant and misleading similarities between new and old information can blind learners to what they are actually experiencing, leading them to draw erroneous comparisons and conclusions.\n\nMidas launched the world’s first AI and cloud based mixing console platform in Aug 2019.Predictably, this triggered a fear of automation from several sound engineers.\nTo market automation using AI:\n\nExplain the purpose of automation\nMake clear what the system and do and how well it can do it.\n\n\nTypes of Automation\nWe can choose to automate along any of these dimensions\n\nInformation acquisition\nInformation analysis\nDecision selection E.g. Breathing us automated but we can control it to a certain extent\nAction implementation E.g. The heartbeat is completely automated\n\n\n\nLevels of Automation\n0: No Automation  1: Assistance  2: Partial Automation  3: Conditional Automation  4: High Automation  5: Total Automation \nRefer to this paper for more details.\nWe can build systems with various levels of automation(manual to fully automated) across the candidate stages for automation as shown below.\n\nYou can also build adaptable systems by allowing ranges of automation for each type of automation, a human in the loop can then interact with the system specifying the right level of automation as shown below.\nHow the system gives feedback to the user can also be of the following types\n\nOptimistic: Show everything as if it were correct\nPessimistic: Show only what is known to be correct\nCautious: Show the uncertainty of the system\nOpportunistic: Exploit uncertainty to improve the system (Active learning)\n\nThe following principles relating to purpose should be kept in mind while designing the system\n\nExplain the purpose of the AI\nMake clear what the system can do and how well it can do it\nShow the performance of the system choosing appropriate feedback strategies\nShow when the system is not confident\nDesign for appropriate trust, not for higher trust.\n\nThe following principles relating to interaction should be kept in mind while designing the system\n\nMinimize the impact on the existing workflow\nSupport efficient invocation(of AI system)\nSupport efficient correction\nSupport efficient dismissal\nMake the level of automation adaptable\nDesign clear transitions between the different levels of automation\nFocus on UX from the early stages of algorithmic research\n\nThe following canvas can be used for such system design.\n\nThe following image shows how the AI Music system was designed"
  },
  {
    "objectID": "blog/2020-06-06-2019-oreilly-ai-conference/index.html#ai-for-cell-shaping-in-mobile-networks-ericsson",
    "href": "blog/2020-06-06-2019-oreilly-ai-conference/index.html#ai-for-cell-shaping-in-mobile-networks-ericsson",
    "title": "2019 Oreilly AI Conference",
    "section": "16) AI for Cell Shaping in Mobile Networks | Ericsson",
    "text": "16) AI for Cell Shaping in Mobile Networks | Ericsson\nMobile networks present an optimization problem where the network has to provide optimum coverage for a large number of mobile customers using a limited spectrum. Some parameters of an antenna such as power, vertical tilt can be adjusted to modify coverage as shown below.\n\nYou may also wan to control antenna tilt deceptively as special events like a football game and a political rally can cause a sudden increase in the density of people in a location. This is typically done manually by setting rules.\nReinforcement learning can be used to do this.\n\nPolicies were trained on simulated environments.\nLot of domain knowledge from tuning these antennas over the years are also available that need to be utilized by the AI system.\nRLLib framework provided by UC Berkeley’s Rise lab was used. The algorithm used for training was A-pex\n\nRL Formulation\nA simulated environment was used . The simulator was plugged in through Open AI gym environment.\nThe agent was a CNN that takes a state as input and output an action vector.\nState is the parameter of antennas,topology of buildings and user hotspots.\nActions determine increase or decrease of tilt of an antenna.\nReward is average increase in throughput for subscribers.\n\nThe image below shows the setup.\nThe image on the right shows the typology of the city grid being served by the antenna.\nThe image on the left shows distributions of subscribers with warmer colors showing better coverage.\nThe image in the middle shows the position of the antenna and tilt with warmer colors again indicating better coverage.\n\nAs the RL algorithm learns, you can the the image on the left becoming warmer as their coverage improves.\n\n\n\nTraining Set up\n\n\n\nRL Algorithms\n\nThe A-pex algorithm used is an extension of the Deep Q Learning algorithm and was found to perform better.\n\n\n\nUsing Historical Data\nGiven historical records of state of the system and action taken by experts are available, a supervised model or recommender system can be built on this data.You can also extend the historical data to include a reward corresponding to the state,action pair.\nThis can again be used for RL. Without a simulator you can’t do a lot of exploration.\n\n\nTransferring from Simulated to Real Environment\nAdd noise to the simulations to make learning more robust."
  },
  {
    "objectID": "blog/2020-06-06-2019-oreilly-ai-conference/index.html#improving-ocr-quality-of-documents-using-gans-exl",
    "href": "blog/2020-06-06-2019-oreilly-ai-conference/index.html#improving-ocr-quality-of-documents-using-gans-exl",
    "title": "2019 Oreilly AI Conference",
    "section": "17) Improving OCR quality of documents using GANs | EXL",
    "text": "17) Improving OCR quality of documents using GANs | EXL\nThe modern OCR pipeline is as follows.\n\nInput documents are fed into an OCR engine which converts the documents into machine understandable format\nAutomated processing using ML/NLP\nManual validation to ensure 100 % accuracy\n\n\nThis may not work well if the input documents are highly unstructured i.e. have incorrect layouts, low resolutions or is very noisy with wrinkles,watermarks etc. This can be addressed either by improving the OCR engine or by improving the input documents fed into the OCR engine.\n\nThe enhanced pipeline is as follows:\n\nAlthough traditional image enhancement techniques are effective in addressing some problems, GAN based enhancements can improve the quality of input documents even further,\nBelow is an overview of a GAN. It consists of a generator and a discriminator acting in an adversarial fashion, one trying make real and fake images indistinguishable while the other trying to distinguish between the two."
  },
  {
    "objectID": "blog/2020-06-06-2019-oreilly-ai-conference/index.html#resolution-enhancement",
    "href": "blog/2020-06-06-2019-oreilly-ai-conference/index.html#resolution-enhancement",
    "title": "2019 Oreilly AI Conference",
    "section": "Resolution Enhancement",
    "text": "Resolution Enhancement\n\nData Creation\n\nHigh resolution English documents with different font-types, sizes and spacing were first converted to standard resolution(300 dots per inch (DPI)) using standard image processing tools and then converted to low resolution images at 75 DPU using random subsampling and bicubic Downsampling. The high-resolution and low-resolution image pairs are used for training the GANs\nData sets were also created by scanning high resolution English documents at high (300 DPI) and (low) resolution.\n\n\n\nTraining\n\n\nThe generator will not have a max-pooling layer as max-pooling layers lower the resolution of an image.Upsampling layers such as un-pooling (reverse of max-pooling) or de-convolution layers are used.\nCustom loss with two terms are used\n\nAdversarial loss looks just at the performance of the discriminator\nContent loss captures the performance of the generator which again has two terms\n\nMSE between generated image and ground truth image\nMSE between the ground truth image and image generated using a VGG network\n\nAlthough some use cases would require only the first of these two terms, for the OCR use case, this was found to perform better.\n\n\nGiven the generator is very sensitive to initialization, the model is trained on imagenet and the weights are transferred.\n\n\nEvaluation\nIn terms of character level accuracy, the GAN enhanced images resulted in an improvement of 7 percentage points for enterprise grade OCR systems and an improvement of 80 percentage points for open source OCR systems.\nIn terms of word level accuracy, the GAN enhanced images resulted in an improvement of 9 percentage points for enterprise grade OCR systems and an improvement of 79 percentage points for open source OCR systems."
  },
  {
    "objectID": "blog/2020-06-06-2019-oreilly-ai-conference/index.html#document-de-noising",
    "href": "blog/2020-06-06-2019-oreilly-ai-conference/index.html#document-de-noising",
    "title": "2019 Oreilly AI Conference",
    "section": "Document De-noising",
    "text": "Document De-noising\n\nData\n\nClean documents with different font types, sizes and spacing were collected. Noise( Random noise, Gaussian noise, pattern noise) were added to these documents to create noisy documents\nClean datasets from kaggle were taken and synthetic noise was added to this to create noisy documents.\n\n\n\nTraining\nThe training process was identical to the one above and as seen below, the documents were de-noised effectively over successive epochs.\n\nIn other use cases, a Mask-RCNN was used to identify parts of documents that had address like texts,these regions were then enhanced using a GAN."
  },
  {
    "objectID": "blog/2020-06-06-2019-oreilly-ai-conference/index.html#fighting-crime-with-graphs-mit-ibm",
    "href": "blog/2020-06-06-2019-oreilly-ai-conference/index.html#fighting-crime-with-graphs-mit-ibm",
    "title": "2019 Oreilly AI Conference",
    "section": "18) Fighting Crime with Graphs | (MIT + IBM)",
    "text": "18) Fighting Crime with Graphs | (MIT + IBM)\nUse of graphs to fight financial crime has been getting a lot of traction recently. This talk emphasized the use of Graph Convolutional Networks to create node embeddings that can be consumed by traditional machine learning models or standard neural nets.\nAlgorithms like node2vec and deepwalk have been used to create embeddings that capture the topology of a network, but these algorithms cannot capture the node attributes that might have rich information.\nGraphSage is a Graph convolutional network algorithm that allows you to capture both the topology of a network as well as useful node attributes.Besides this is an inductive algorithm meaning that it does not need to be trained on whole graphs and can be used for inference on unseen nodes and graphs.\nMore useful information is available here and here"
  },
  {
    "objectID": "blog/2020-06-06-2019-oreilly-ai-conference/index.html#data-science-design-thinking---a-perfect-blend-to-achieve-the-best-user-experience-intuit",
    "href": "blog/2020-06-06-2019-oreilly-ai-conference/index.html#data-science-design-thinking---a-perfect-blend-to-achieve-the-best-user-experience-intuit",
    "title": "2019 Oreilly AI Conference",
    "section": "19) Data Science + Design Thinking - A Perfect blend to achieve the best user experience | Intuit",
    "text": "19) Data Science + Design Thinking - A Perfect blend to achieve the best user experience | Intuit\nDesign for delight by 1) Demonstrating deep customer empathy - Know customers deeply by observing them and define problems in human centric terms 2) Go broad to go narrow - Generate lots of ideas before winnowing them. Quantity first then focus on quality. 3) Perform rapid experiments with customers - Rapid prototyping and AB testing"
  },
  {
    "objectID": "blog/2020-06-06-2019-oreilly-ai-conference/index.html#explaining-machine-learning-models-fiddler-labs",
    "href": "blog/2020-06-06-2019-oreilly-ai-conference/index.html#explaining-machine-learning-models-fiddler-labs",
    "title": "2019 Oreilly AI Conference",
    "section": "20) Explaining Machine Learning Models | Fiddler Labs",
    "text": "20) Explaining Machine Learning Models | Fiddler Labs\nAttribution problem : Attribute a prediction to input features. Solving this is the key goals of Explaining ML Models\nNaive approaches to do this include:\n\nAblation: Drop each feature and note the change in prediction\nFeature Gradient: $ x_i \\frac {dy}{dx_i}$ where $ x_i $ is feature and $ y $ is the output\n\nIntegrated Gradients - This is a technique for attributing a differentiable model’s prediction to the input features\nA popular method for non-differentiable models is Shapley values.\nBoth Integrated Gradients and Shapley Values come with some axiomatic guaranteed.The former uniquely satisfies 6 axioms while the latter uniquely satisfies 4. Side note: This is my go to reference for interpretability techniques.\nExample of an axiom is the sensitivity axiom:\n\nAll else being equal, if changing a feature changes the output, then that feature should get an attribution. Similarly if changing a feature does not change the output, it should not get an attribution.\n\nIntegrated Gradients is the unique path integral method that satisfies: Sensitivity, Insensitivity, Linearity preservation, Implementation invariance, Completeness and Symmetry\nAnother problem related to interpretability that remains an open problem for many classes of black box models is Influence - i.e. Which data points in the training data influenced the model the most."
  },
  {
    "objectID": "blog/2020-06-06-2019-oreilly-ai-conference/index.html#snorkel",
    "href": "blog/2020-06-06-2019-oreilly-ai-conference/index.html#snorkel",
    "title": "2019 Oreilly AI Conference",
    "section": "21) Snorkel",
    "text": "21) Snorkel\nSnorkel is a weak supervised learning approach that came out of Stanford. More information available here\nThe key operations in the Snorkel workflow include:\n\nLabeling Functions: Heuristics to label data provided by experts\nTransformation Functions: Data Augmentations\nSlicing Functions: Partition the data specifying critical subsets where model performance needs to be high\n\nFor this approach to work at least 50% of the labeling functions need to be better than random."
  },
  {
    "objectID": "blog/2020-06-06-2019-oreilly-ai-conference/index.html#managing-ai-products-salesforce",
    "href": "blog/2020-06-06-2019-oreilly-ai-conference/index.html#managing-ai-products-salesforce",
    "title": "2019 Oreilly AI Conference",
    "section": "22) Managing AI Products | Salesforce",
    "text": "22) Managing AI Products | Salesforce\nTo demonstrate value to business stakeholders, which is the ultimate goal of anyone who works in a corporation, it is essential to tie business metrics to model metrics. This should ultimately inform what kind of ML we decide to use.\n\nThe figure above demonstrates the accuracy of the model(x-axis) required to provide a material lift in the business metric(y-axis) e.g. conversion rate. If the base line improvement rate in conversion that we need to deliver is only 2%, a model that has accuracy in the range 50 - 75% is sufficient. This means we could rule out sophisticated models like Neural Nets that are harder to deploy and maintain and focus on simpler models that are easier to build or maintain."
  },
  {
    "objectID": "blog/2020-06-06-2019-oreilly-ai-conference/index.html#explainability-and-bias-in-ai-and-ml-institute-for-ethical-ai-and-ml",
    "href": "blog/2020-06-06-2019-oreilly-ai-conference/index.html#explainability-and-bias-in-ai-and-ml-institute-for-ethical-ai-and-ml",
    "title": "2019 Oreilly AI Conference",
    "section": "23) Explainability and Bias in AI and ML| Institute for Ethical AI and ML",
    "text": "23) Explainability and Bias in AI and ML| Institute for Ethical AI and ML\nUndesired bias can be split into two conceptual pieces\n1) Statistical Bias (Project Bias) : The error between where you ARE and where you could get caused by modeling/project decisions\n\nSub optimal choices of accuracy metrics/cost functions\nSub optimal choices of ML models chosen for the task\nLack of infrastructure required to monitor model performance in production\nLack of human in the loop where necessary\n\n2) A-priori bias (Societal Bias) : The error between the best you can practically get, and the idealistic best possible scenario - caused by a-priori constraints\n\nSub optimal business objectives\nLack of understanding of the project\nIncomplete resources (data, domain experts etc)\nIncorrectly labelled data (accident or otherwise)\nLack of relevant skill sets\nSocietal shifts in perception\n\nExplainability is key to:\n\nIdentify and evaluate undesirable biases\nTo meet regulatory requirements such as GDPR\nFor compliance of processes\nTo identify and reduce risks (FP vs FN)\n\nInterpretability != Explainability\n\nHaving a model that can be interpreted doesn’t mean in can be explained\nExplainability requires us to go beyond algorithms\nUndesired bias cannot be tackled without explainability\n\nLibrary for Explainable AI: xAI alibi\nAnchor points: What are features that influenced a specific prediction for a data instance? This can be evaluated by roughly by pulling out a feature and estimating its impact on the model prediction.\nCounterfactual: How would the input/features have to change for the prediction to change?"
  },
  {
    "objectID": "blog/2020-06-06-2019-oreilly-ai-conference/index.html#usable-machine-learning---lessons-from-stanford-and-beyond-stanford-university",
    "href": "blog/2020-06-06-2019-oreilly-ai-conference/index.html#usable-machine-learning---lessons-from-stanford-and-beyond-stanford-university",
    "title": "2019 Oreilly AI Conference",
    "section": "24) Usable Machine Learning - Lessons from Stanford and beyond | Stanford University",
    "text": "24) Usable Machine Learning - Lessons from Stanford and beyond | Stanford University\n\nFor deep learning, improvement in performance requires exponential increase in data\nDeep learning still doesn’t work very well with structured data\nDon’t look for a perfect model right out of the gate, instead iterate towards higher quality models\nMeasure implicit signals where possible. e.g. Is a user spending time on a page or closing a window"
  },
  {
    "objectID": "blog/2020-06-06-2019-oreilly-ai-conference/index.html#human-centered-machine-learning-h2o.ai",
    "href": "blog/2020-06-06-2019-oreilly-ai-conference/index.html#human-centered-machine-learning-h2o.ai",
    "title": "2019 Oreilly AI Conference",
    "section": "25) Human Centered Machine Learning | H2O.ai",
    "text": "25) Human Centered Machine Learning | H2O.ai\n\nEstablish a benchmark using a simple model from which to gauge improvements in accuracy, fairness, interpretability or privacy\nOverly complicated features are hard to explain. Features should provide business intuition.(More relevant for regulated industries)\nFor fairness, it is important to evaluate if different sub groups of people are being treated differently by your ML model (Disparate Impact). Need to do appropriate data processing. OSS: AIF360, aequitas\nModel Debugging for Accuracy, Privacy or Security: This involves eliminating errors in model predictions by testing using adversarial examples, explaining residuals, random attacks and what if analysis. Useful OSS: cleverhans, pdpbox, what-if-tool"
  },
  {
    "objectID": "blog/2020-06-06-2019-oreilly-ai-conference/index.html#monitoring-production-ml-systems-datavisor",
    "href": "blog/2020-06-06-2019-oreilly-ai-conference/index.html#monitoring-production-ml-systems-datavisor",
    "title": "2019 Oreilly AI Conference",
    "section": "26) Monitoring production ML Systems | DataVisor",
    "text": "26) Monitoring production ML Systems | DataVisor\nDatavizor is a company that specializes in unsupervised ML for fraud detection.\nThere are potentially several issues that can occur in a production ML systems as shown below. Robust monitoring systems are required to be able to detect and resolve these issues in a timely fashion.\n\nYou can react to these issues by having the ability to roll back your system to the previous stable build or by auto scaling to a bigger cluster but it is more cost effective to be able to detect and prevent these issues.\nSome approaches to monitoring model quality:\n\nBuild a surrogate model(offline) to monitor the performance of the deployed model(online)\nTrack model drift\nCarry out anomaly detection on model outputs and metadata\n\nAnomaly detection using Time series decomposition is a suitable approach.\nAdditive decomposition of a time series:\n\\[ Y_t = T_t + S_t + R_t \\]\nwhere $ T_t $ is the trend component, $ S_t $ is the seasonal component and $ R_t $ is the residual component.\nSubtract the trend and seasonal components from the signal to get the residual component.You should be able to use the residual component to track anomalies.\n\\[ R_t = Y_t - T_t - S_t \\]\nThis approach can create unexpected drops in the residual component as shown in red in the image below.\n\nTo resolve this, obtain the residual component by subtracting the median instead of the trend.\nThe mean absolute deviation (MAD) can then be used to identify anomalies.\n\\[ If\\ Distance\\ to\\ Median &gt; x \\times MAD : anomaly \\]"
  },
  {
    "objectID": "blog/2020-06-06-2019-oreilly-ai-conference/index.html#reference-architectures-for-ai-and-machine-learning-microsoft",
    "href": "blog/2020-06-06-2019-oreilly-ai-conference/index.html#reference-architectures-for-ai-and-machine-learning-microsoft",
    "title": "2019 Oreilly AI Conference",
    "section": "27) Reference Architectures for AI and Machine Learning | Microsoft",
    "text": "27) Reference Architectures for AI and Machine Learning | Microsoft\nDistributed training of models can be implemented via data parallelism or model parallelism.\nIn data parallelism, the entire model is copied to each worker that processes a subset of the total data. The batch size can hence be scaled up to the number of workers you have. Very large batch sizes can cause issues with convergence of the network.\nIn model parallelism, the model is split across workers and there has to be communication of gradients between the nodes during the forward and backward pass.\nData parallelism is more fault tolerant and common.\n\nWhen to use distributed training?\n\nYour model us too big to fit a sufficiently large batch size\nYour data is large\nYour model requires significant GPU computation\n\n\nYou do not need distributed training if: - You want to run hyperparameter tuning - Your model is small\n\nReference Architecture for Distributed Training\n\nAzure ML supports distributed training for TF, Pytorch and Keras. The dependencies are placed in a docker container that runs on the host machine. The storage can be mounted to each of the nodes where training happens.\nAzure ML will create the appropriate docker containers and configure the Message Passing interface (MPI) which is essential for distributed training. Azure ML will run the script on the nodes and push the results to blob storage.\n\n\nArchitecture for Real Time Scoring with DL Models\n\n\nModel served as a REST endpoint\nShould handle requests from multiple clients and respond near instantaneously\nTiming of requests are unknown\nSolution should have low latency, scalable and elastic to seasonal variations\n\nBest practices for deploying an image classification system:\n\nUse GPU for efficient inferencing (faster than CPU)\nSend multiple images with a single request (GPUs process batches more efficiently)\nUse HTTP 2.0 (allows you to accept multiple request)\nSend image as file within HTTP request\n\n\n\nArchitecture for Batch Scoring with DL Models\n\n\nScoring can be triggered (by appearance of new data) or scheduled (at recurring intervals)\nLarge scale jobs that run asynchronously\nOptimize for cost, wall clock time and scalability\n\nThe way Azure implements this is given here"
  },
  {
    "objectID": "blog/2020-06-06-2019-oreilly-ai-conference/index.html#semi-supervised-learning-for-ml-rocket-ml",
    "href": "blog/2020-06-06-2019-oreilly-ai-conference/index.html#semi-supervised-learning-for-ml-rocket-ml",
    "title": "2019 Oreilly AI Conference",
    "section": "28) Semi Supervised Learning for ML | Rocket ML",
    "text": "28) Semi Supervised Learning for ML | Rocket ML\nThree important considerations:\n\nTotal cost - May not be possible to acquire the volume of labels needed to build a good model\nSocial Accountability - Interpretability, Explainability, Traceability\nRobustness - Should not be vulnerable to to easy adversarial attacks\n\n\n\nAnomaly Detection Problem\n\nLabels are rare and expensive to acquire\n\nTypes of Anomalies:\n\nPoint Anomalies : E.g. a point In a time series\nContextual Anomalies : Anomalous in a given context but not in another\nCollective Anomalies: Group of points constitute and anomaly\n\n\nKNN is parameterized by the no of neighbors(K) and the distance metric.\nIn an unsupervised setting, use the Local outlier Factor to identify outliers.\n\n\nAnomaly detection performance\n\nBuild KNN for different values of K\nCompute LOF for the neighboring k points\nUse threshold on LOF to determine anomaly vs normal\nAs k increases the performance of the model increases. This is because for small k, by looking at really close neighbors, density is not too different and hence anomalies are not found. i.e. you miss the forest for the trees. For larger k, by looking beyond the clusters into normal areas, density differences stand out.\nPossible methods to generate better features: Matrix Factorization, Pre-trained models, Auto Encoders.\nReducing dimensionality using SVD can improve accuracy by addressing the curse of dimensionality problem\nKNN is computationally intensive so need highly efficient, parallelized implementations for this approach to work."
  },
  {
    "objectID": "blog/2020-06-06-2019-oreilly-ai-conference/index.html#sequence-to-sequence-learning-for-time-series-forecasting-anodot",
    "href": "blog/2020-06-06-2019-oreilly-ai-conference/index.html#sequence-to-sequence-learning-for-time-series-forecasting-anodot",
    "title": "2019 Oreilly AI Conference",
    "section": "29) Sequence to Sequence Learning for Time Series Forecasting | Anodot",
    "text": "29) Sequence to Sequence Learning for Time Series Forecasting | Anodot\nStructural Characteristics that affect the choice and performance of your Time Series forecasting algorithm:\n\nExisting techniques are unable to address situations when many of these structural characteristics co-occur.\nKey development that allowed the application of Neural Nets to time Series: Recurrent Neural Nets and Back Propagation Through Time\nRNNs can be memory based (GRU and LSTM) or attention based (Transformers).\nConsiderations for getting accurate TS forecasts: 1. Discovering influencing metrics and events - Look at correlations between target and time series features\n\nEnsemble of models - Usually require multiple algorithms\nIdentify and account for unexplainable data anomalies\n\nIdentify anomalies and use this to create new features\nEnhance anomalies that can be explained by external factors\nWeight down anomalies that can’t be explained by external factors\n\nIdentify and account for different time series behaviors\n\nTraining a single model for multiple time series does not work if each series shows a different seasonality. Difference can be in frequency or strength.\nMixing stationary and non stationary time series also does not work"
  },
  {
    "objectID": "blog/2020-06-06-2019-oreilly-ai-conference/index.html#transfer-learning-nlp-machine-reading-comprehension-for-question-answering-microsoft",
    "href": "blog/2020-06-06-2019-oreilly-ai-conference/index.html#transfer-learning-nlp-machine-reading-comprehension-for-question-answering-microsoft",
    "title": "2019 Oreilly AI Conference",
    "section": "30) Transfer Learning NLP: Machine Reading comprehension for question answering | Microsoft",
    "text": "30) Transfer Learning NLP: Machine Reading comprehension for question answering | Microsoft\nAttention can be content based or location based. Question Answering requires content based attention.\nMachine Reading Comprehension systems should be capable of summarizing key points in a piece of texts, it can answer questions and also do reply (e.g Gmail auto suggestion)and comment.\nMachine reading comprises (in increasing order of complexity) Extraction, Synthesis & Generation and Reasoning & Inference.\nOpen Source datasets available: SQUAD (Stanford) and Marco (Microsoft)\nBest performing algorithms:\nFor extraction: BIDAF(for small paragraphs) , DOCQA(large documents), S-NET (small multiple paragraphs)\nFor reasoning and inference: SynNet ( multiple small paragraphs) and Open NMT (multiple small or large paragraphs)\nBIDAF: Bi Direction Attention Flow for Machine Comprehension"
  },
  {
    "objectID": "blog/2020-06-06-2019-oreilly-ai-conference/index.html#generative-models-for-fixing-image-defects-adobe",
    "href": "blog/2020-06-06-2019-oreilly-ai-conference/index.html#generative-models-for-fixing-image-defects-adobe",
    "title": "2019 Oreilly AI Conference",
    "section": "31) Generative Models for fixing image defects| Adobe",
    "text": "31) Generative Models for fixing image defects| Adobe\nTraditional approach is to manually retouch the image. Auto tune functions exist that can enhance global features such as exposure, saturation and lighting but not local features (e.g. color of specific objects in an image.)\n\nPopular GANS\n\nStyle GAN for Face generation at NVIDIA\nCycle GAN for Style Transfer at UC Berkeley\nDual GAN\nDisco GAN\n\nNeural Style Transfer: Transform a given image in the style of a reference(domain) image.This uses only two images.It does not require paired image - a single image of the domain e.g. an art piece is required.\nStyle Transfer (with GAN): Uses a collection of images. E.g. 500 image in domain A and 500 images in domain B. This again does not require paired data.\nThe benefit of this approach is that it does not require paired data i.e. the pre and post image of the same object.\n\n\n\nGAN based Image Enhancer\n\nThere are two generators and discriminators, hence the name DUAL GAN. Learning to create a superior image G2(Defective) is difficult, hence the system is trained to optimize G1(G2(defective)) - this is the cycle in Cycle GAN.\nThe UNET segmentation model helps to learn which part of the image is a tree, sky or some specific object. The regions identified by the UNET model are then locally enhanced.\n\n\nChallenges\n\nWeak features\nSubjective, Noisy, Mislabeled Data - Humans determine whether an image is good or not\nSmall Dataset\nBatch size used is smaller given there are four models to train, hence harder for models to converge\nTraining GANs is hard and time consuming\nGAN inference is time consuming and does not scale\n\n\n\nGAN Model Evaluation\nCreating ground truth labels is a manual process requiring an artists to retouch the images, this is not feasible. In the absence of ground truth labels:\n\nTrain Discriminative models (VGG or Resnet) on good and bad images. Score of the model is the metric."
  },
  {
    "objectID": "blog/2020-06-06-2019-oreilly-ai-conference/index.html#supercharging-business-decisions-with-ai-uber",
    "href": "blog/2020-06-06-2019-oreilly-ai-conference/index.html#supercharging-business-decisions-with-ai-uber",
    "title": "2019 Oreilly AI Conference",
    "section": "32) Supercharging business decisions with AI | Uber",
    "text": "32) Supercharging business decisions with AI | Uber\nThe finance planning department at Uber carries out rolling 12 month forecasts for Uber trips. Some of the challenges involved here are:\n\nTime series of trips vary considerably across cities based on economy, geography, quality of public transport etc.\nShape of time series can change over time. It can grow rapidly initially but then flatten\nAffected by holidays,e vents, weather etc.\nNewly launched cities have little or no data\n\n\nPlanning\nBelow is a planning model used by Uber. The goals of the first part of the system here is to generate cost curves that track the relationship between money spent on driver and rider promotions and no of sign ups. the goals is to find the optimal point maximizing ROI.\nThe levers available are:\n$ \\${Ref} $ Dollars spent on Referrals  $ \\${DPaid} $ Dollars paid to drivers  $ \\$_{Promo} $ Dollars paid in promotions to riders  $ SU_D $ Sign Ups from drivers  $ SU_R $ Sign Ups from riders\n\nThe second part of the system is the trips models\n\n$ FT_D $ First trip per rider  $ FT_R $ First trip per driver  $ RR_D $ Retention Rate pf drivers  $ RR_R $ Retentions rate of riders  $ TPA_D $ Trips per active driver  $ TPA_R $ Trips per active rider  $ RES_D $ Resurrected drivers  $ RESR_R $ Resurrected Riders\nActive drivers are those that are active at least once per month.\nThis a classic funnel. Promotions lead to sign ups and first rides, but many churn at this point looking for new promotions.\nThis variables are used to calculate the no of trips, active riders, active drivers, resurrected drivers, resurrected riders and per trip metrics.\nResurrected riders /drivers are those who haven’t been active in the previous three months.\n\n\nForecasting Models\nRiders and Drivers are cohorted based on month of joining for each city.\nFor each cohort, three models as shown below are used. A black box model ensembles these three models by assigning weights to the predictions of each. Evaluation Metric is MAPE and SMAPE.\n\nModels used include ETS, ARIMA and TBATS.\nModel averaging is done at different training end points to correct for misleading volatility in recent data points.\n\n\nModelling Seasonality\nSeasonal holidays can shift from year to year that can cause problems. Uber uses Prophet’s Day to Month(DTM) forecast model and python holiday library to account for this.\nThe sophistication of the systems used by Uber’s financial planning team is truly remarkable. There was a lot more content in this talk that I didn’t fully follow given my familiarity with this domain is limited."
  },
  {
    "objectID": "blog/2020-06-06-2019-oreilly-ai-conference/index.html#an-age-of-embeddings-usc-information-sciences-institute",
    "href": "blog/2020-06-06-2019-oreilly-ai-conference/index.html#an-age-of-embeddings-usc-information-sciences-institute",
    "title": "2019 Oreilly AI Conference",
    "section": "33) An age of embeddings| USC Information Sciences Institute",
    "text": "33) An age of embeddings| USC Information Sciences Institute\n\n‘You shall know a word by the company it keeps’ - JR Firth(1957)\n\n\nWord embeddings\nSkip Gram: Take a word and predict the surrounding words  CBOW: Take surrounding words and predict the target word\nGlove word embeddings are based on matrix factorization of a matrix with entries corresponding to frequency of their co-occurence.\n\nTFIDF is a simple type of an embedding, but they are sparse and high dimensional.\n\n\nImage Embeddings\nCan use fully connected layers before the softmax layer of a CNN built for classification. Auto encoders are also a popular choice to create embeddings.\n\n\nDocument Embeddings\nTopic models give lower dimensional embeddings of documents.\nBelow is a way to get document embeddings from a word embeddings. \\(d_{test}\\) in the image below is the id for a document inserted as a word into the document.\n\n\n\nGraph and Network Embedding\nGraph CNNs can be used to create embeddings."
  },
  {
    "objectID": "blog/2020-06-06-2019-oreilly-ai-conference/index.html#pytorch-at-scale-for-translation-and-nlp-facebook",
    "href": "blog/2020-06-06-2019-oreilly-ai-conference/index.html#pytorch-at-scale-for-translation-and-nlp-facebook",
    "title": "2019 Oreilly AI Conference",
    "section": "34) PyTorch at Scale for translation and NLP | Facebook",
    "text": "34) PyTorch at Scale for translation and NLP | Facebook\nCommon NLP Tasks:\n\nClassification\nWord Tagging E.g. Part of Speech\nSequence Classification\nSequence to Sequence E.g. Translation\n\nModel should be able to take additional input features such as relevant metadata\n\nInference\n\nPython is not good for inference. Scaling up to multiple CPUs is often necessary for inference which is challenging in Python due to the global interpreter lock\nSaving models by saving the weights as in TF or PyTorch is not very resilient as you need to reconstruct the model and reload the weights for for inference. Small changes such as change in internal variable names can break the model. ONNX and TorchScript are resilient model formats for PyTorch.\nIf models are of reasonable size, they can be duplicated in multiple replicas and scaled up according to traffic needs.\nIf models are very large, you need intra model parallelism or sharding. This might be necessary if vocabularies are very large.\n\n\nAn alternative to sharding is BPE (Byte Pair Encoding). You look over a corpus of text and learn sub word units and tokenize into these sub word units. This reduces the vocabulary size and can work across languages and hence is a good choice for large multilingual models.\n\n\nTraining\nA training framework needs: * Data * Model * Metrics * Trainer - Might make sense to fold this into the model in some cases. e.g. PyTorch ignite\n\nTraining a Classifier in Pytext comprises the following steps\n\nGet data\nTokenize and Numericalize\nConvert to Tensor and pad to equalize lengths\nGet Model outputs\nCompute Loss and Optimize using Backprop"
  },
  {
    "objectID": "blog/2020-06-06-2019-oreilly-ai-conference/index.html#turning-ai-research-into-a-revenue-engine-verta.ai",
    "href": "blog/2020-06-06-2019-oreilly-ai-conference/index.html#turning-ai-research-into-a-revenue-engine-verta.ai",
    "title": "2019 Oreilly AI Conference",
    "section": "35) Turning AI research into a revenue engine | Verta.ai",
    "text": "35) Turning AI research into a revenue engine | Verta.ai\nModelDB: Model Life Cycle Management System built at MIT. Tracks hyperparameters, metrics and other model metadata\nYou need to be agile with ML/AI for it to make a revenue impact with ML. The new model lifecyle is\n\nAgile principles for ML:\n\nGood enough vs best models\nConstant iteration: Build, deploy, repeat\nMonitoring: gather feedback and repeat\n\nThe challenges Verta focuses on tackling are in the lower half of the model lifecycle:\n1)How do you run and manage ML Experiments? Need a git equivalent for ML models  2)Deploying Models in Production  3)Monitoring Model Performance\n\nModel versioning requires code versioning, data versioning, config versioning and environment versioning\nAutomated Model deployment for models equivalent to Jenkins is missing. Automated Monitoring and Collaboration are also required\n\nVerta’s solution includes:\n\nAuto scaling is accomplished through containers and Kubernetes\nFor a sales engineering client, it resulted in the following improvements."
  },
  {
    "objectID": "blog/2020-06-06-2019-oreilly-ai-conference/index.html#deep-learning-applications-in-nlp-and-conversational-ai-uber",
    "href": "blog/2020-06-06-2019-oreilly-ai-conference/index.html#deep-learning-applications-in-nlp-and-conversational-ai-uber",
    "title": "2019 Oreilly AI Conference",
    "section": "36) Deep Learning Applications in NLP and Conversational AI | Uber",
    "text": "36) Deep Learning Applications in NLP and Conversational AI | Uber\nThe right problem to tackle with AI * Involves decision making under uncertainty * Within reach of AI Tech * Touches customer pain point * Delivers business value\nThe best model depends on data available and the data required depends on task complexity.\n\nUber has deployed ML systems for their customer support platform. Based on the customer questions,the system recommends templates to the customer service agent for replies.\nUber also has developed one click chat reply templates for drivers.This is similar to Gmail’s auto reply features. The major challenge here is that the chats are often informal and have lots of typos. However, the task complexity is lower compared to Gmail as the variety of conversations is lower.\nTo solve this Uber has used a two step algorithm.\n\nGiven the types of responses are limited depending on the intent of the question, intent detection is the primary focus.\nIntent detection is carried out as follows\n\nTrain Doc2vec model to get dense embedding for each message (Self Supervised Learning)\nMap labelled data to embedding space (You should have labelled data giving intent of various messages)\nCalculate centroid of each intent cluster\nCalculate distance between incoming message and intent cluster centroid\nClassify into Nearest Neighbor Intent Cluster.\n\nAnother use case allows Uber drivers to converse with the Uber App without touching their phones.\nConversational AI can be done with two approaches as shown below. The first one uses a modular approach with different modules carrying out specific sub tasks while the second uses an end to end model.\n\nFor task oriented conversations, the first is preferred.\nUber also has created a system that combines the strengths of Spark which is CPU driven and Deep Learning frameworks such as Tensorflow that rely on GPUs.\nThe pre-processing of data is done on Spark Clusters and are transferred to GPU clusters where the models are trained.\nFor inference - Apache Spark and Java are used for both batch and real time requests. The tensorflow model is converted into a spark transformer in a Spark pipeline."
  },
  {
    "objectID": "blog/2020-06-06-2019-oreilly-ai-conference/index.html#named-entity-recognition-at-scale-with-dl-twitter",
    "href": "blog/2020-06-06-2019-oreilly-ai-conference/index.html#named-entity-recognition-at-scale-with-dl-twitter",
    "title": "2019 Oreilly AI Conference",
    "section": "37) Named Entity Recognition at Scale with DL | Twitter",
    "text": "37) Named Entity Recognition at Scale with DL | Twitter\nApplications of NER at Twitter include Trends which have to be meaningful words , Event detection, Recommending User Interests.\nTwitter has opted for in-house NER due to the unique linguistic features of Twitter besides other reasons.\n\nGenerating Training Data\n\nTweets were sampled based on tweet engagement\nSampling has to carried out over a longer time span to capture temporal signals. e.g. A soccer game lasts 90 minutes\nNormalization has to be carried out based on countries and spoken language\nCharacter based Labeling is carried out on a crowd sourcing platform\nCharacter labels have then to be processed into token labels to train the model\nDeleted tweets have to expunged to comply with GDPR\n\n\n\nModel\n\nHistorically, Conditional Random Fields were used for NER. Deep Learning and Language models are now the most popular approaches.\nThe Deep Learning approach uses a multi layered approach as shown below.\n\nThe architecture used by Twitter is a Char - BiLSTM -CRF.\n\nA character representation is used to supplement the word representation if a token is unknown.Other features indicate if the token was a hashtag or other twitter specific characteristics.\nTwitter chose not to use the Language Model approach because of the size of the model and latency demands in production.\n\n\nConfidence Estimation\nThe output of the NER model is typically consumed by other models so confidence estimates are also provided along with NER tags. Some downstream applications might require high precision in the NER tags.\nThis confidence estimation also has to be at the entity level rather than token level. Simple softmax decoder gives confidence at the token level i.e. confidence in “San” and “Jose” separately rather than the single entity “San Jose”.\nUsing a CRF based approach proposed in a 2004 paper, the correct confidence can be computed."
  },
  {
    "objectID": "blog/2020-06-06-2019-oreilly-ai-conference/index.html#behavior-analytics-for-enterprise-security-using-nlp-approaches-aruba-networks",
    "href": "blog/2020-06-06-2019-oreilly-ai-conference/index.html#behavior-analytics-for-enterprise-security-using-nlp-approaches-aruba-networks",
    "title": "2019 Oreilly AI Conference",
    "section": "38) Behavior Analytics for Enterprise Security using NLP Approaches | Aruba Networks",
    "text": "38) Behavior Analytics for Enterprise Security using NLP Approaches | Aruba Networks\n\nNeed to identify relevant anomalies in a network. Malware more of a threat than adware\nFor supervised learning to work, you need a list of all possible access to the network tagged as safe or anomalous. However what is available are large volumes of diverse unlabeled data\nNeed to explain to an admin why something has been identified as an anomaly\nMost anomalies tend to be multi-dimensional\nDifferent employees in the network go about differently using network resources in different sequences. A seasoned employee and a first timer also go about accessing network resources in different sequences.\nCapture embeddings capturing the ‘semantic meaning’ of a server. Can you create embeddings for servers hosting Perforce and Git (code repositories) and Jira and Bugzilla (Bug repositories) such that\n\n\\[ Perforce - Git + Jira = Bugzilla \\]\nCapture the likelihood of using a second server given you use first server.\n\nLabel servers as QA/Finance/Dev based on workflows or departments used by them.Evaluating whether servers belonging to a specific group or workflow is being used by someone outside of it can reveal anomalies or breaches.\n\nCorpus Used: Access sequences of servers"
  },
  {
    "objectID": "blog/2020-06-06-2019-oreilly-ai-conference/index.html#interpreting-millions-of-patient-stories-with-deep-learned-ocr-and-nlp-selectdara-john-snow-labs",
    "href": "blog/2020-06-06-2019-oreilly-ai-conference/index.html#interpreting-millions-of-patient-stories-with-deep-learned-ocr-and-nlp-selectdara-john-snow-labs",
    "title": "2019 Oreilly AI Conference",
    "section": "39) Interpreting millions of patient stories with deep learned OCR and NLP | SelectDara & John Snow Labs",
    "text": "39) Interpreting millions of patient stories with deep learned OCR and NLP | SelectDara & John Snow Labs\nHome Health: Healthcare delivered at home primarily to elderly or those with multiple chronic conditions. Industry expected to grow by 6.7% each year as more baby boomers retire. However this is reducing the workforce available to the industry. Most payments to the industry comes from Medicare which is under pressure and reducing amounts of payments.\nGiven the workforce is inexperienced, the goal of the project is to be able to identify assessments (health assessments documents of patients) as Hard,Medium or Easy based on degree of effort and perceived level of difficulty so that a manager can delegate the assessments appropriately.\nFeedback on difficulty was gathered subjectively from workers, while effort was quantified in terms of time spent within a record (which also validates the subjective assessment.)\nChallenges: Different layouts, scales , fonts etc High number of records and pages *Need processing in clusters\nSpark OCR and Spark NLP were used.\n\nSpark NLP is built on Spark ML APIs.Apache project being actively developed. Proven to be better than spacy in accuracy and training time.\nSpark NLP also scales to large clusters of machines and can process many documents in parallel.\n\nThe document assembler (part of Spark NLP) takes the text from the OCR and creates an annotation that represents a document.\n\nLayout Analysis: Identify related regions of text that belong together in the image. Annotations of a document: Contains the text extracted from the image and relevant metadata."
  },
  {
    "objectID": "blog/2020-06-06-2019-oreilly-ai-conference/index.html#building-autonomous-network-operation-using-deep-learning-and-ai-mist",
    "href": "blog/2020-06-06-2019-oreilly-ai-conference/index.html#building-autonomous-network-operation-using-deep-learning-and-ai-mist",
    "title": "2019 Oreilly AI Conference",
    "section": "40) Building autonomous network operation using deep learning and AI | Mist",
    "text": "40) Building autonomous network operation using deep learning and AI | Mist\n\nChallenges facing Enterprise IT Teams\n\nGrowing number of devices with 5G and IOT\nCloud driven dynamic and flexible environments\nWorkload and complexity resulting from public/private/hybrid clouds\n\nIn today’s enterprises, employees and customers want to access resources in company’s data centers/public cloud and private cloud though home offices, branch offices or even from coffee shops.Ideally they should be able to access what they need from wherever or whenever.\nHowever, accessing these resources means navigating a system with several failure points as shown below.The goal is to automatically detect and fix issues with any of these components before someone has to report the issue.\n\nIn an eCommerce company’s warehouse, robots operate through WiFi, a failure in the WiFi system and delays in addressing it can have serious financial repercussions.\nMist developed a self driving network with the following components.\n\nAt the Data stage, data is collected. Each device can send stats and logs to the cloud. User activity is also monitored using stream processing\nAt the event stage, user impacting events and responsible entities are identified.\nAt diagnosis stage, events are correlated to diagnose the problem. E.g. a problem with the I-phone OS or WiFi port. Then you need to identify what changes caused the issue e.g. a config change on i-OS (Temporal event correlation)or a change in an upstream device such as a router (Cross entity correlation)\nFinally you take actions using automated actions if corrective action can be taken within a device controlled by the cloud network or provide information for manual correction.\nMist also has a system to automatically identify issues in firmware. A four step process is used:\n\nCollect logs\nUse NLP to encode these into embeddings\nCluster these and find important issues\nAutomatically open JIRA tickets so that these can be addressed by engineers\n\nSuggestions for building Enterprise AI Solutions:\n\nStart with real business problems\nBuild a step by step process for continuous improvement\nKeep human in the loop"
  },
  {
    "objectID": "blog/2020-06-06-2019-oreilly-ai-conference/index.html#unlocking-the-next-stage-in-computer-vision-with-deep-nn-zillow",
    "href": "blog/2020-06-06-2019-oreilly-ai-conference/index.html#unlocking-the-next-stage-in-computer-vision-with-deep-nn-zillow",
    "title": "2019 Oreilly AI Conference",
    "section": "41) Unlocking the next stage in computer vision with Deep NN | Zillow",
    "text": "41) Unlocking the next stage in computer vision with Deep NN | Zillow\n\nNew Zestimate model will factor in home photos to produce a price estimate\nTypical features such as sq. ft, no of bedrooms don’t say anything about a home being renovated or remodeled\nNew Zestimate understands a home’s unique interiors, high end features and how they impact a home’s overall value\nCurated or retouched photos can bias the model\nView of water increase value but view may not be always relevant e.g. view is accessible only from the order of the balcony\nPhotos may not accurately or comprehensively represent a home\n\n\nBuilding Better models\n\nBetter training and evaluation data\n\n\nIntroduced new app(Zillow 3D Home) to do a 3D scan/tour of the home\nPhotos cannot be retouched or be taken on drones\n\n\nData annotation at scale\n\n\nApp captures 3D images when most models are trained on 2D images\nBuild annotation tool to annotate in 3D space but people generally don’t do this well on 3D data. Video gamers have a better sense of this and were contracted to do the annotation\n\n\nGround truth data\n\n\nZillow Offers buys and sellers home directly\nUse LIDAR to get detailed scans of homes\n\n\nNew techniques\n\n\nAttribute recognition: Identify real estate attributes in listing images using list descriptions as weak supervision\n\n\nRespect people’s privacy\n\n\nTransparency around data collection and sharing\nHome owner can claim or remove photos"
  },
  {
    "objectID": "blog/2020-06-06-2019-oreilly-ai-conference/index.html#introducing-kubeflow-google-ibm",
    "href": "blog/2020-06-06-2019-oreilly-ai-conference/index.html#introducing-kubeflow-google-ibm",
    "title": "2019 Oreilly AI Conference",
    "section": "42) Introducing Kubeflow| Google & IBM",
    "text": "42) Introducing Kubeflow| Google & IBM\nKubeflow has a bunch of components including:\n\nargo: For building pipelines\nkatib: For parallelized hyper parameter tuning\npytorch-job, mxnet-job, tf-serving\n\nModel training on along all frameworks is supported\nEasiest way to deploy: https://www.kubeflow.org/docs/gke/deploy/deploy-ui/\n\nKubeflow Pipelines is a platform for building and deploying portable scalable ML workflows based on Docker containers. They are a directed acyclic graph(DAG) of pipeline components(docker containers) each performing a function.\n\nE.g.:"
  },
  {
    "objectID": "blog/2020-06-06-2019-oreilly-ai-conference/index.html#personalization-at-scale-facebook",
    "href": "blog/2020-06-06-2019-oreilly-ai-conference/index.html#personalization-at-scale-facebook",
    "title": "2019 Oreilly AI Conference",
    "section": "43) Personalization at Scale | Facebook",
    "text": "43) Personalization at Scale | Facebook\nPersonalization using AI is used across Facebook Feed, Instagram Explore, Groups and Marketplace\nFB’s scale is massive. 1.5 Billion daily users sharing billions of posts shared per day, 100 billion + posts seen per day and trillions of posts ranked every day.\n\nPractical Techniques for Personalization:\n1) System Design\nA large scale recommender system typically looks like:\n\nItems to be recommended are very large, so a retrieval state is going to recommend a subset of items with a focus on recall and latency.\nIn the ranking stage a sophisticated model predicts the relevant of an item to the user. Feature store contains features such as no of posts liked or videos shared/viewed by a user.\nFinally some business rules are used so as not to show something in a user’s disliked list or something that is more than a week old.\nFinally the results of the recommender systems are recorded in an impression log. User actions are recorded and fed back to train the model.\nThe whole process needs to complete in less than 1 second.\n2) Candidate Generation\nThe challenge here is identifying $ O(10^2) $ items out of $ O(10^6) $ items quickly. A candidate generator is used which:\n\nOptimized for recall\nLatency bound\nDoes not optimize for accurate rank (which is addressed later)\n\nA two tower neural net architecture is used to create embeddings and calculate similarities for recommendation.\n\n*KNN Offline Indexing**: All item embeddings are populated into a KNN index for fast lookup When a query item comes in, most similar items are looked up using a KNN service look up.\n\n3) Learning from Sparse Data\nChallenge: how to learn from categorical/sparse features and combine with continuous/dense features.\nFor categorical features, an embeddings table is used to convert each category into a dense vector. For some features that include multiple IDs, some kind of pooling(average or max) is used. Finally pairwise dot products are taken and concatenated.\n\nThe complete architecture for this Deep Learning Recommendation Model DLRM is as follows:\n\n\nFeature interaction in simplest from can be concatenating the two vectors.\nEmbeddings tables get really large and may not fit into GPU memory. So data parallelism does not work and requires model parallelism.\n\n4) Keeping Models Fresh\n\nData distributions keep changing as items(videos,posts etc.) change continuously.\nOnline training: Update model every few minutes after training batches of data collected every few minutes. Fine tune from previous baseline given you don’t have to train from scratch\nRecurring Training: Deploy updated model every few weeks or days."
  },
  {
    "objectID": "blog/2020-06-06-2019-oreilly-ai-conference/index.html#os-for-ai-serverless-productionized-ml",
    "href": "blog/2020-06-06-2019-oreilly-ai-conference/index.html#os-for-ai-serverless-productionized-ml",
    "title": "2019 Oreilly AI Conference",
    "section": "44) OS for AI: Serverless, Productionized ML",
    "text": "44) OS for AI: Serverless, Productionized ML\nSee full presentation here\n\nAlgorithmia has 9,5000 algorithms on multiple frameworks developed by over 100,000 developers The main challenges of deploying ML models in enterprise are as follows\n\n\n\nMachine Learning != Production Machine Learning\n\nAn Operating System :\n\nprovides common functionality needed by many programs\nStandardizes conventions to make systems easier to work with\nPresents a higher level of abstraction of the underlying hardware.\n\nOperating systems evolved from punch cards that was suitable for only one job to to Unix that supported multi-tenancy and composability to DOS that allowed hardware abstraction i.e. you could access the hard disk or floppy disk through the same set of commands. Windows and Mac allowed GUI that democratized computing. Finally we have app stores made available through IOS and Android that made software installation extremely easily.\nThis is where we want ML to ultimately be - a user should be able to search for an algorithm and use it without having to do cumbersome installations and devops.\n\nThe most standard way of building and deploying a model involves building a model and putting it on a server while exposing a REST endpoint that can be accessed from any device,building such a system from scratch can be quite laborious involving the following steps:\n\nSet Up Server\n\nThis requires properly balancing CPU,GPU,memory and cost.\n\nCreate microservice\n\nYou can write an API wrapper using Flask but securing, metering and disseminating this can be challenging.\n\nAdd scaling\n\nYou have to do automation to predict increase in loads and automatically configure a cloud VM to scale to meet the increased loads\n\nRepeat for each unique environment\n\nThis can be challenging if you are using multiple languages and frameworks.\nDeploying models using serverless functions help resolve these challenges to a certain extent as shown below.\n\nEven so, all dependencies are supported and you often have to build a container with the dependencies locally and then move it to the cloud service provider.\nAn ideal OS for AI should do the following:\n\nAlgorithmia has built a solution that seeks to do all this. Refer to the deck linked to earlier for more details."
  },
  {
    "objectID": "blog/2020-06-06-2019-oreilly-ai-conference/index.html#towards-universal-semantic-understanding-of-natural-languages-ibm-research",
    "href": "blog/2020-06-06-2019-oreilly-ai-conference/index.html#towards-universal-semantic-understanding-of-natural-languages-ibm-research",
    "title": "2019 Oreilly AI Conference",
    "section": "45) Towards universal semantic understanding of natural languages | IBM Research",
    "text": "45) Towards universal semantic understanding of natural languages | IBM Research\nTypically, each language needs separate parsers and separate text analytics to be developed. This means work has to be replicated in each language.\nThe goal is to come with a unified representation applicable across all languages, so this work does not have to be replicated.\nThe major challenges being faced here include:\n\nAnnotation: Different annotation schemes are used for different languages and sometimes for the same language.\nTraining Data: High quality labeled data is required\nModels are built for one task at a time\n\nIBM is addressing these challenges by creating an automated annotation scheme combined with smart crowdsourcing coupled with programmable abstractions so that work does not have to be repeated.\nWith semantic parsing we want to identify the key semantic parts of a sentence no matter in which way it is written, these semantic labels are largely stable.\n\nFramenet and PropBank are commonly used resources for Semantic Role Labeling in English.\nThe challenge is that labels across languages typically do not match even if it conveys the same meaning. For example the same subject in a given sentence can be labelled as an ‘orderer’ in English and ‘buyer’ in Chinese.\nWe want sentences across languages to share the semantic labels as shown below.\n\n\nCreating cross lingual training data\nTypically you find annotators to annotate corpora in each language separately that takes months. The proposed solution is to do annotation in parallel corpora on datasets that are readily available across multiple languages e.g. subtitles of movies, the Bible etc.\n\nHowever these projections are not always accurate due to translation shifts or error in the source languages that get magnified by projecting it.\nOne way to solve this is the below method where only selected sentences where the projections from the source language (EN) to the target language (TL) are complete and correct. This training data is used to train the SRL model which is then used to bootstrap more labelled data and train iteratively.\n\nThere still can be cases where there is a one to many mapping between a single label in one language to multiple labels in another or cases where no mapping exists. Human intervention is required in this case.\n\n\nCrowd in the Loop Learning\nLabeling tasks are classified as hard and easy by a routing model and assigned to a crowd or an expert accordingly.\n\nA query strategy model can also be used to determine if the labels predicted by an SRL model are correct or not. If this model is not confident that the labels are correct, it can be assigned to a human.\n\nThese techniques were used to create a universal propostion banks for 8 languages.\n\n\nDeveloping Models for Semantic Annotation\nThe data is characterized by a heavy tail of labels that occur very infrequently. So instance based learning with KNN is used in conjunction with deep learning.\nA demo of the final model is available here"
  },
  {
    "objectID": "blog/2020-06-06-2019-oreilly-ai-conference/index.html#the-holy-grail-of-data-science-rapid-model-development-and-deployment-zepl",
    "href": "blog/2020-06-06-2019-oreilly-ai-conference/index.html#the-holy-grail-of-data-science-rapid-model-development-and-deployment-zepl",
    "title": "2019 Oreilly AI Conference",
    "section": "46) The holy grail of data science : Rapid model development and deployment | Zepl",
    "text": "46) The holy grail of data science : Rapid model development and deployment | Zepl\n\n87% of ML projects don’t make it into production\nZepl data science and analytics platform makes Apache Zepplin enterprise grade\n\n\nData Dependencies\n\nModel consumes different data sources as well as outputs of other models. Data is subject to input distribution changes, data label mapping changes etc. So keeping a versioned copy of the data is important.\nCreating a data catalog which is searchable, accessible and annotatable is important. It can capture basic information such as location of data, schema and additional notes. This can be accomplished in a data science notebook.\n\n\n\nReproducibility\n\nContainers deployed on the server side so that every one can access the same resources can help solve reproducibility issues.\n\n\nPrototyping vs Production\nReplicating a model in a different language as is typically required in corporate settings is challenging.\n\n\nMonitoring\nPrediction Bias:\nDistributions of predicted labels should be equal to distribution of observed labels. E.g. if only 1% of e-mails are spam, only 1 % of e-mails should be predicted as spam\nAction Limits:\nTrigger alarms when model performance deteriorates/drifts significantly\n\n\nModel Ownership in Production\nDeveloper != Owner\nHand offs between departments tend to result in loss of information. Need thorough documentation to prevent this.\n\n\nTesting & Deploying\nUse pipelines with Kubeflow or on Sagemaker.\nZepl allows you run tests on developed models. Clicking a test button, it packages a notebook and spins up a container where there tests are carried out."
  },
  {
    "objectID": "blog/2020-06-06-2019-oreilly-ai-conference/index.html#deep-reinforcement-learning-for-industrial-robotics-osaro",
    "href": "blog/2020-06-06-2019-oreilly-ai-conference/index.html#deep-reinforcement-learning-for-industrial-robotics-osaro",
    "title": "2019 Oreilly AI Conference",
    "section": "47) Deep reinforcement learning for industrial robotics | OSARO",
    "text": "47) Deep reinforcement learning for industrial robotics | OSARO\n\nOSARO builds edge deployed perception and control software for robots.\n\nLevels of Autonomy:\n\nMost robots used in warehouses are at Level 1, do not take sensory feedback and cannot react to the environment.\nPartial autonomy: At this level, robots can take sensory feedback and react to the environment but cannot react to surprises or new objects.\nDeep RL enables level 4 and level 5 automation. It allows:\n\nMoving away from programming robots - from static to dynamic robotic systems\nAddressing questions that are “hard to describe to computers”\nHandling variabilities and react to changes\nLearning at scale\n\nThis is necessary as warehouse robots often have to pick objects that are transparent or reflective which poses challenge to computer vision systems as well as objects with deformable surfaces.\nThis can also be important in countries like Japan with a shrinking labor force.\nOSARO has built ‘object picking’ robots with level 4 automation to staff e-commerce warehouses.\n\nIn robotics industry , each vendor has a proprietary platform and Domain Specific programming language, industry is thus highly fragmented."
  },
  {
    "objectID": "blog/2020-06-06-2019-oreilly-ai-conference/index.html#imitation-learning",
    "href": "blog/2020-06-06-2019-oreilly-ai-conference/index.html#imitation-learning",
    "title": "2019 Oreilly AI Conference",
    "section": "Imitation Learning",
    "text": "Imitation Learning\nLearning from an expert’s demonstration.\n\nCopies behavior, does not necessarily learn problem objective.\nTrained policy only as good as the demonstration."
  },
  {
    "objectID": "blog/2020-06-06-2019-oreilly-ai-conference/index.html#meta-learning",
    "href": "blog/2020-06-06-2019-oreilly-ai-conference/index.html#meta-learning",
    "title": "2019 Oreilly AI Conference",
    "section": "Meta Learning",
    "text": "Meta Learning\n\nModels that can learn new skills or adapt to new environments with few training examples\n\nA meta learner(agent) will train a Learner(model) on a data set with a large number of tasks, and learn common representations across these tasks. The Learner can now adapt to new tasks with very little"
  },
  {
    "objectID": "blog/2020-06-06-2019-oreilly-ai-conference/index.html#challenges-and-future-directions-in-deploying-nlp-in-commercial-environments-intel",
    "href": "blog/2020-06-06-2019-oreilly-ai-conference/index.html#challenges-and-future-directions-in-deploying-nlp-in-commercial-environments-intel",
    "title": "2019 Oreilly AI Conference",
    "section": "48) Challenges and future directions in deploying NLP in commercial environments | Intel",
    "text": "48) Challenges and future directions in deploying NLP in commercial environments | Intel\nImplications of latest developments in NLP include:\n\nPre-training of large Language models is very costly\nShift of focus from lower level training to more application specific tweaking\nLoading very large model and heavy fast forward computing during inference\n\nNLP Architect is Intel’s NLP Library for developing DL based NLP applications. It also produces optimized NLP models ready for inference deployment.\n\nKnowledge distillation from BERT/XLNet to small models is supported\nQ8BERT - Produced by quantization aware training. Quantized 8 bit version of Bert(int) as against 32 bit(floating point). Leads to 4x memory compression. This can be done in just the fine tuning step.\nSparse version of Google’s NMT model is also available\n\n\nModel Distillation\nTrain a small network using the output of a larger model. See details here\nUsing this approach, BERT models with 300 million parameters can be distilled to an LSTM+CRF model with only 3 million parameters.\nAs seen in the figure below, the performance of these distilled models is on par with the original BERT model.\n\nQuantized BERT’s performance on various tasks is comparable to that of BERT.\n\nNLP Architect also supports Aspect based sentiment analysis which identifies the target word a sentiment term qualifies.\nE.g. for the sentence: ‘service is poor but pizza was great’. ‘poor’ is a sentiment term which qualifies the target word ‘service’ while ‘great’ qualifies the word ‘pizza’. There are also semi supervised techniques that can learn domain specific terms or lexicon e.g. movie reviews vs food reviews\nNER models that can learn to tag words from any domain when seeded with a few examples is also supported. Example, if seeded with grape fruit and lemon as examples of citrus fruits, the model will recommend a list of words to the user who can modify/accept it."
  },
  {
    "objectID": "blog/2020-06-06-2019-oreilly-ai-conference/index.html#what-you-must-know-to-build-ai-systems-that-understand-natural-language-pacific-ai",
    "href": "blog/2020-06-06-2019-oreilly-ai-conference/index.html#what-you-must-know-to-build-ai-systems-that-understand-natural-language-pacific-ai",
    "title": "2019 Oreilly AI Conference",
    "section": "49) What you must know to build AI systems that understand natural language | Pacific AI",
    "text": "49) What you must know to build AI systems that understand natural language | Pacific AI\nSolving Natural Language Understanding would effectively lead to solving hard AGI."
  },
  {
    "objectID": "blog/2020-06-06-2019-oreilly-ai-conference/index.html#many-languages-spoken.",
    "href": "blog/2020-06-06-2019-oreilly-ai-conference/index.html#many-languages-spoken.",
    "title": "2019 Oreilly AI Conference",
    "section": "Many languages spoken.",
    "text": "Many languages spoken.\nEven the type of language spoken at home or in an academic paper are very different. Different domains have different jargon.\nSMS Language will have emojis and use a lot of slang or profanities. Auto correct can often be a problem with informal language.\nLegal transcriptions follow a strict procedure that needs to be understood to make sense of it. SEC filings will have a lot of boiler plate that can be ignored.\nGiven the varieties of languages, NLP models have to be tailored to the type of language."
  },
  {
    "objectID": "blog/2020-06-06-2019-oreilly-ai-conference/index.html#domain-specificity",
    "href": "blog/2020-06-06-2019-oreilly-ai-conference/index.html#domain-specificity",
    "title": "2019 Oreilly AI Conference",
    "section": "Domain specificity",
    "text": "Domain specificity\nOff the shelf models cannot work on all domains as they are typically trained on content like Wikipedia articles.This problem is particularly acute in health care.\nE.g. “states started last night, upper abd, took alka seltzer approx 0500, no relief. nausea no vomiting”\nIn finance, the sentiment towards a stock is not quite the same as sentiment towards a restaurant in a Yelp review.\nIt is important to train domain specific models. You might have to pair your NLP data scientist with a domain expert who is an expert in the language the model has to understand.\n\nUse both structured and unstructured data\nIn a hospital demand forecasting problem, you need to look at medical notes from doctors and nurses as well as available structured data such as age, reason for visit, wait time etc.\nBest practice: Unify NLP and ML pipeline as enabled by Spark NLP and Spark ML\nWhat is good first NLP project?\nStart with an existing model based on structured data that human domain experts usually solve with free text data\n\nIt is interesting to note that with the advent of advanced language models such as GPT3, we may not need to build domain specific models from scratch but only fine tune a language model. Even so, given GPT3 has been licensed exclusively to Microsoft, it may be out fo reach for most organizations in the world."
  },
  {
    "objectID": "blog/2020-06-06-2019-oreilly-ai-conference/index.html#language-inference-in-medicine-ibm-research",
    "href": "blog/2020-06-06-2019-oreilly-ai-conference/index.html#language-inference-in-medicine-ibm-research",
    "title": "2019 Oreilly AI Conference",
    "section": "50) Language Inference in medicine | IBM Research",
    "text": "50) Language Inference in medicine | IBM Research\n\nNeed to determine if a patient meets eligibility criteria for clinical trials by comparing against doctor and nurse notes.\n\nAn example of clinical trial eligibility criteria is shown below\n\n\nNeed to compare text between medical records and guidelines definitions\nNatural Language Inference is a three way classification problem\n\nGiven a premise and a hypothesis, classify the latter as\n\nDefinitely true (entailment) \nDefinitely false (contradiction) \nMight be true, might be false (neutral) \n\nE.g.\nPremise: A person on a horse jumps over a broken down airplane\nHypothesis 1: A person is outdoors, on a horse\nThis hypothesis is true(hence an entailment)\nHypothesis 2: A person is at a diner, ordering an omelet\nThis hypothesis is a contradiction.\nHypothesis 3: A person is training his horse for a competition\nThis hypothesis is neutral.\nThis can be more complicated in the following example.\nPremise: The campaigns seem to reach a new pool of contributors.\nHypothesis: The campaign drew a new crowd of funders.\nAn understanding of the meaning of words (pool = crowd, campaigns is a plural of campaign, funders = contributors) is necessary here.\nIn medicine this can be even more complicated.\nPremise: During the admission he had a paracentesis for ~ 5 liters and started on spironolactone, lasix and protonix\nHypothesis 1: The patient has cirrhosis(Entailment)\nHypothesis 2: The patient does not have fluid in the abdomen (Contradiction)\nHypothesis 3: The patient would benefit from a liver transplant (Neutral)"
  },
  {
    "objectID": "blog/2020-06-06-2019-oreilly-ai-conference/index.html#datasets",
    "href": "blog/2020-06-06-2019-oreilly-ai-conference/index.html#datasets",
    "title": "2019 Oreilly AI Conference",
    "section": "Datasets",
    "text": "Datasets\n\nStanford Natural Language Inference Dataset (SNLI). Premise here are captions from Flicker photographs, and humans were asked to write a statement that is definitely true, definitely false and might be true/false.\nAbout 500,000 training pairs\nMultiNLI from NYU: Covers more variety of genres beyond photographs.\nClinical NLI Corpus was created by IBM Research. Premise is a sentence from a patient’s history. Clinicians were then asked to write sentences for each of the three classes."
  },
  {
    "objectID": "blog/2020-06-06-2019-oreilly-ai-conference/index.html#models",
    "href": "blog/2020-06-06-2019-oreilly-ai-conference/index.html#models",
    "title": "2019 Oreilly AI Conference",
    "section": "Models",
    "text": "Models\n\nGBM classifier performance was limited.\nA stronger deep learning baseline using Bag of Words was created. Embeddings vectors for premise and hypothesis were created by summing the word embeddings for the constituent words.These were concatenated and passed through a fully connected Neural Net.\nOther models considered include:\n\nEnhanced Sequential Inference ModelESIM\nInferSent\n\nFollowing variations of transfer learning were also explored\n\nSequential Transfer: Pre train on SNLI and then train on Medical NLI, followed by testing on Medical NLI\nMulti target transfer:Some layers were shared between open and medical domains, while other layers were trained exclusively on the open domain data. Testing was carried out on the medical domain.\n\nApproach was also designed to leverage existing knowledge graph compiled by domain experts. This relied on developing a matrix which captured the shortest path distance between concepts in the knowledge graph which mirrored the attention matrix developed by the ESIM model from data."
  },
  {
    "objectID": "blog/2019-08-11-introduction-to-gaussian-processes/index.html",
    "href": "blog/2019-08-11-introduction-to-gaussian-processes/index.html",
    "title": "Introduction to Gaussian Processes",
    "section": "",
    "text": "Fundamental to understanding a Gaussian process is a Gaussian distribution.The 1 dimensional version of the Gaussian distribution is the Normal distribution or the Bell Curve.\nThe plot of a standard normal distribution with mean 0 and variance 1 is shown below. Note that the curve doesn’t look like a perfect bell as the data has been simulated\nset.seed(0)\nplot(density(rnorm(100000)),main = bquote(\"N(\"~mu~\"=0,\"~sigma^2~\"=1)\"))\n\nA bi-variate Gaussian distribution is simply a Gaussian distribution in 2 dimensions. Whereas a uni-variate Gaussian distribution is parameterized by two scalar quantities (i.e. the mean and variance),the bi-variate Gaussian distribution and its higher dimensional counterparts are parameterized by a vector( the mean) and a matrix (the covariance matrix)\n\\[ \\begin{bmatrix} x_1 \\\\\\\\ x_2 \\end{bmatrix} = N\\Big( \\begin{bmatrix} \\mu_1 \\\\\\\\ \\mu_2 \\end{bmatrix},\\begin{bmatrix} \\Sigma_{11} & \\Sigma_{12} \\\\\\\\ \\Sigma_{21} & \\Sigma_{22} \\end{bmatrix} \\Big) \\]\nA bi-variate Gaussian can be represented using a simple scatter plot as shown below.\nlibrary(ggplot2)\ndf = data.frame(x= rnorm(10000),y =rnorm(10000))\nggplot(df,aes(x=x,y=y)) + geom_point()+geom_density_2d()+labs(x='x1',y='x2')\n\nNote that the bi-variate Gaussian above is constructed using two independent 1D Gaussians, meaning the two variables being plotted here are independent i.e. information about one doesn’t tell us anything about the other. This is further evidenced by the circular, symmetrical distribution of the points.\nSuch a standard normal bi-variate Gaussian has mean \\(\\mu = \\begin{bmatrix} 0 \\\\\\\\ 0 \\end{bmatrix}\\) and a covariance matrix \\(\\begin{bmatrix} 1 & 0 \\\\\\\\ 0 & 1 \\end{bmatrix}\\)\nNote the off-diagonal elements of the covariance matrix are 0.\nThe plot below shows a bi-variate Gaussian with \\(\\mu = \\begin{bmatrix} 0 \\\\\\\\ 0 \\end{bmatrix}\\) and a covariance matrix \\(\\begin{bmatrix} 1 & 0.7 \\\\\\\\ 0.7 & 1 \\end{bmatrix}\\).\nlibrary(mvtnorm)\ngaussian_2d &lt;- data.frame(rmvnorm(10000,mean= c(0,0),sigma = matrix(c(1,0.7,0.7,1),ncol=2)))\nggplot(gaussian_2d,aes(x=X1,y=X2)) + geom_point()+geom_density_2d() \n\nHere the two random variables are positively correlated and are not independent,because if one is positive, we know that the second is also far more likely to be positive rather than negative\nSimilarly,the plot below shows a bi-variate Gaussian with \\(\\mu = \\begin{bmatrix} 0 \\\\\\\\ 0 \\end{bmatrix}\\) and a covariance matrix \\(\\begin{bmatrix} 1 & -0.7 \\\\\\\\ -0.7 & 1 \\end{bmatrix}\\)\ngaussian_2d &lt;- data.frame(rmvnorm(10000,mean= c(0,0),sigma = matrix(c(1,-0.7,-0.7,1),ncol=2)))\nggplot(gaussian_2d,aes(x=X1,y=X2)) + geom_point()+geom_density_2d() \n\nHere the two variables are negatively correlated.\nNow consider a different visualization of the same data in the figure above.Only 10 of the 10,000 points plotted above is visualized here.\nlibrary(gganimate)\nlibrary(tidyr)\nlibrary(dplyr)\nlibrary(magrittr)\n\n#Select top 10 rows for visualization\nviz_data &lt;- head(gaussian_2d,10)\nviz_data$set &lt;- as.integer(c(1:10))\n#Convert to long format\nviz_data_long &lt;- gather(viz_data,key='datapoint',value='y',-set) %&gt;% arrange(set) %&gt;% \n                    select(-datapoint) %&gt;% \n                      mutate(x = rep(c(1,2),length(unique(viz_data$set)))) %&gt;%\n                        select(set,x,y)\nggplot(viz_data_long,aes(x=x,y=y,group=set)) + geom_point() + \n  geom_line(col='cornflowerblue')+ transition_time(set)\n## Warning: No renderer available. Please install the gifski, av, or magick package\n## to create animated output\n## NULL\nThe 2D Gaussian distribution can be used to model a line between two given points. A Gaussian distribution with N dimensions can be used to model a line through N different points as shown below.\nA 10 D Gaussian with a covariance matrix shown below can be constructed.\nN  &lt;- 10 # dimension of required cov matrix\ncov_mat &lt;- diag(N)\n\nvec &lt;- seq(1,0.1,by = -1/N)[2:N]\n\nfor(i in 1:(N-1)){\n  cov_mat[i,(i+1):N] &lt;- cov_mat[(i+1):N,i] &lt;- vec[1:(N-i)]\n}\n\nrownames(cov_mat) &lt;- colnames(cov_mat)&lt;- paste0('X',c(1:10))\nprint(cov_mat)\n##      X1  X2  X3  X4  X5  X6  X7  X8  X9 X10\n## X1  1.0 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1\n## X2  0.9 1.0 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2\n## X3  0.8 0.9 1.0 0.9 0.8 0.7 0.6 0.5 0.4 0.3\n## X4  0.7 0.8 0.9 1.0 0.9 0.8 0.7 0.6 0.5 0.4\n## X5  0.6 0.7 0.8 0.9 1.0 0.9 0.8 0.7 0.6 0.5\n## X6  0.5 0.6 0.7 0.8 0.9 1.0 0.9 0.8 0.7 0.6\n## X7  0.4 0.5 0.6 0.7 0.8 0.9 1.0 0.9 0.8 0.7\n## X8  0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 0.9 0.8\n## X9  0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 0.9\n## X10 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0\nThe plot below shows a gaussian distribution in 10 dimensions, which can potentially be used to model a line passing through 10 points.\nggplot(viz_data_long,aes(x=x,y=y,group=set)) + geom_point() + \n  geom_line(col='cornflowerblue')+ transition_time(set)\n## Warning: No renderer available. Please install the gifski, av, or magick package\n## to create animated output\n## NULL\nThis should give you the intuition that using an N - dimensional Gaussian can be used to model a line through N number of points and an infinite dimensional Gaussian can model a continuous curve. This leads to the idea for Gaussian process regression.\nAlso compare the covariance between any two points in the covariance matrix above and how it manifests in the graph. X1 and X2 have a high covariance, hence the points at 1 and 2 on the x-axis are close to each other along the y-axis. X1 and X10 on the other hand have a low covariance,hence the points 1 and 10 on the x-axis can vary widely.\nConversely if two points lie close to each other on the y-axis, they are likely to have a high covariance whereas if 2 points lie far apart from each other on the y-axis, they are likely to have a low covariance."
  },
  {
    "objectID": "blog/2019-08-11-introduction-to-gaussian-processes/index.html#gaussian-distributions",
    "href": "blog/2019-08-11-introduction-to-gaussian-processes/index.html#gaussian-distributions",
    "title": "Introduction to Gaussian Processes",
    "section": "",
    "text": "Fundamental to understanding a Gaussian process is a Gaussian distribution.The 1 dimensional version of the Gaussian distribution is the Normal distribution or the Bell Curve.\nThe plot of a standard normal distribution with mean 0 and variance 1 is shown below. Note that the curve doesn’t look like a perfect bell as the data has been simulated\nset.seed(0)\nplot(density(rnorm(100000)),main = bquote(\"N(\"~mu~\"=0,\"~sigma^2~\"=1)\"))\n\nA bi-variate Gaussian distribution is simply a Gaussian distribution in 2 dimensions. Whereas a uni-variate Gaussian distribution is parameterized by two scalar quantities (i.e. the mean and variance),the bi-variate Gaussian distribution and its higher dimensional counterparts are parameterized by a vector( the mean) and a matrix (the covariance matrix)\n\\[ \\begin{bmatrix} x_1 \\\\\\\\ x_2 \\end{bmatrix} = N\\Big( \\begin{bmatrix} \\mu_1 \\\\\\\\ \\mu_2 \\end{bmatrix},\\begin{bmatrix} \\Sigma_{11} & \\Sigma_{12} \\\\\\\\ \\Sigma_{21} & \\Sigma_{22} \\end{bmatrix} \\Big) \\]\nA bi-variate Gaussian can be represented using a simple scatter plot as shown below.\nlibrary(ggplot2)\ndf = data.frame(x= rnorm(10000),y =rnorm(10000))\nggplot(df,aes(x=x,y=y)) + geom_point()+geom_density_2d()+labs(x='x1',y='x2')\n\nNote that the bi-variate Gaussian above is constructed using two independent 1D Gaussians, meaning the two variables being plotted here are independent i.e. information about one doesn’t tell us anything about the other. This is further evidenced by the circular, symmetrical distribution of the points.\nSuch a standard normal bi-variate Gaussian has mean \\(\\mu = \\begin{bmatrix} 0 \\\\\\\\ 0 \\end{bmatrix}\\) and a covariance matrix \\(\\begin{bmatrix} 1 & 0 \\\\\\\\ 0 & 1 \\end{bmatrix}\\)\nNote the off-diagonal elements of the covariance matrix are 0.\nThe plot below shows a bi-variate Gaussian with \\(\\mu = \\begin{bmatrix} 0 \\\\\\\\ 0 \\end{bmatrix}\\) and a covariance matrix \\(\\begin{bmatrix} 1 & 0.7 \\\\\\\\ 0.7 & 1 \\end{bmatrix}\\).\nlibrary(mvtnorm)\ngaussian_2d &lt;- data.frame(rmvnorm(10000,mean= c(0,0),sigma = matrix(c(1,0.7,0.7,1),ncol=2)))\nggplot(gaussian_2d,aes(x=X1,y=X2)) + geom_point()+geom_density_2d() \n\nHere the two random variables are positively correlated and are not independent,because if one is positive, we know that the second is also far more likely to be positive rather than negative\nSimilarly,the plot below shows a bi-variate Gaussian with \\(\\mu = \\begin{bmatrix} 0 \\\\\\\\ 0 \\end{bmatrix}\\) and a covariance matrix \\(\\begin{bmatrix} 1 & -0.7 \\\\\\\\ -0.7 & 1 \\end{bmatrix}\\)\ngaussian_2d &lt;- data.frame(rmvnorm(10000,mean= c(0,0),sigma = matrix(c(1,-0.7,-0.7,1),ncol=2)))\nggplot(gaussian_2d,aes(x=X1,y=X2)) + geom_point()+geom_density_2d() \n\nHere the two variables are negatively correlated.\nNow consider a different visualization of the same data in the figure above.Only 10 of the 10,000 points plotted above is visualized here.\nlibrary(gganimate)\nlibrary(tidyr)\nlibrary(dplyr)\nlibrary(magrittr)\n\n#Select top 10 rows for visualization\nviz_data &lt;- head(gaussian_2d,10)\nviz_data$set &lt;- as.integer(c(1:10))\n#Convert to long format\nviz_data_long &lt;- gather(viz_data,key='datapoint',value='y',-set) %&gt;% arrange(set) %&gt;% \n                    select(-datapoint) %&gt;% \n                      mutate(x = rep(c(1,2),length(unique(viz_data$set)))) %&gt;%\n                        select(set,x,y)\nggplot(viz_data_long,aes(x=x,y=y,group=set)) + geom_point() + \n  geom_line(col='cornflowerblue')+ transition_time(set)\n## Warning: No renderer available. Please install the gifski, av, or magick package\n## to create animated output\n## NULL\nThe 2D Gaussian distribution can be used to model a line between two given points. A Gaussian distribution with N dimensions can be used to model a line through N different points as shown below.\nA 10 D Gaussian with a covariance matrix shown below can be constructed.\nN  &lt;- 10 # dimension of required cov matrix\ncov_mat &lt;- diag(N)\n\nvec &lt;- seq(1,0.1,by = -1/N)[2:N]\n\nfor(i in 1:(N-1)){\n  cov_mat[i,(i+1):N] &lt;- cov_mat[(i+1):N,i] &lt;- vec[1:(N-i)]\n}\n\nrownames(cov_mat) &lt;- colnames(cov_mat)&lt;- paste0('X',c(1:10))\nprint(cov_mat)\n##      X1  X2  X3  X4  X5  X6  X7  X8  X9 X10\n## X1  1.0 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1\n## X2  0.9 1.0 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2\n## X3  0.8 0.9 1.0 0.9 0.8 0.7 0.6 0.5 0.4 0.3\n## X4  0.7 0.8 0.9 1.0 0.9 0.8 0.7 0.6 0.5 0.4\n## X5  0.6 0.7 0.8 0.9 1.0 0.9 0.8 0.7 0.6 0.5\n## X6  0.5 0.6 0.7 0.8 0.9 1.0 0.9 0.8 0.7 0.6\n## X7  0.4 0.5 0.6 0.7 0.8 0.9 1.0 0.9 0.8 0.7\n## X8  0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 0.9 0.8\n## X9  0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 0.9\n## X10 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0\nThe plot below shows a gaussian distribution in 10 dimensions, which can potentially be used to model a line passing through 10 points.\nggplot(viz_data_long,aes(x=x,y=y,group=set)) + geom_point() + \n  geom_line(col='cornflowerblue')+ transition_time(set)\n## Warning: No renderer available. Please install the gifski, av, or magick package\n## to create animated output\n## NULL\nThis should give you the intuition that using an N - dimensional Gaussian can be used to model a line through N number of points and an infinite dimensional Gaussian can model a continuous curve. This leads to the idea for Gaussian process regression.\nAlso compare the covariance between any two points in the covariance matrix above and how it manifests in the graph. X1 and X2 have a high covariance, hence the points at 1 and 2 on the x-axis are close to each other along the y-axis. X1 and X10 on the other hand have a low covariance,hence the points 1 and 10 on the x-axis can vary widely.\nConversely if two points lie close to each other on the y-axis, they are likely to have a high covariance whereas if 2 points lie far apart from each other on the y-axis, they are likely to have a low covariance."
  },
  {
    "objectID": "blog/2019-08-11-introduction-to-gaussian-processes/index.html#marginal-distributions-and-conditional-distributions",
    "href": "blog/2019-08-11-introduction-to-gaussian-processes/index.html#marginal-distributions-and-conditional-distributions",
    "title": "Introduction to Gaussian Processes",
    "section": "Marginal Distributions and Conditional Distributions",
    "text": "Marginal Distributions and Conditional Distributions\nGiven a jointly Gaussian distribution\n\\[ \\begin{bmatrix} x_1 \\\\\\\\ x_2 \\end{bmatrix} = N\\Big( \\begin{bmatrix} \\mu_1 \\\\\\\\ \\mu_2 \\end{bmatrix},\\begin{bmatrix} \\Sigma_{11} & \\Sigma_{12}\\\\\\\\ \\Sigma_{21} & \\Sigma_{22} \\end{bmatrix} \\Big) \\]\naccording to the Multivariate Gaussian Theorem, the marginal distributions of the two component variables are given by.\n\\[ p(x_1) = N (x_1 | \\mu_1.\\Sigma_{12}) \\]\n\\[ p(x_2) = N (x_2 | \\mu_2.\\Sigma_{22}) \\]\nThe conditional posterior distribution is given by:\n\\[ p(x_2|x_1) = N (x_2 | \\mu_{2|1},\\Sigma_{2|1}) \\] where:\n\\[ \\mu_{2|1} = \\mu_2 + \\Sigma_{21}\\Sigma_{11}^{-1} (x_1 - \\mu_1) \\]\nand\n\\[ \\Sigma_{2|1} = \\Sigma_{22} - \\Sigma_{21}\\Sigma_{11}^{-1}\\Sigma_{12} \\]\nThese expression are critical to do inference with Gaussian Process Regression."
  },
  {
    "objectID": "blog/2019-08-11-introduction-to-gaussian-processes/index.html#gaussian-process-regression.",
    "href": "blog/2019-08-11-introduction-to-gaussian-processes/index.html#gaussian-process-regression.",
    "title": "Introduction to Gaussian Processes",
    "section": "Gaussian Process Regression.",
    "text": "Gaussian Process Regression.\nConsider three points shown below which you want to model using a Gaussian distribution.\nplot(data.frame(x = c(1,1.5,3) , f = c(0.2,0.25,0.8)),xlim=c(0,5),ylim = c(0,1),pch=16)\ntext(x=c(1,1.5,3),y = c(0.25,0.3,0.85),labels=c('(x1,f1)','(x2,f2)','(x3,f3)'))\n\nThe three points above can be modeled using a Gaussian distribution of the form below. Note that a zero vector can be used for the mean after standardizing the data.\n\\[ \\begin{bmatrix}\n    f_1 \\\\\\\\\n    f_2 \\\\\\\\\n    f_3 \\end{bmatrix} = N\\Bigg( \\begin{bmatrix} 0 \\\\\\\\ 0 \\\\\\\\ 0 \\end{bmatrix},\\begin{bmatrix} K_{11} & K_{12} & K_{13} \\\\\\\\ K_{21} & K_{22} & K_{23} \\\\\\\\ K_{31} & K_{32} & K_{33} \\end{bmatrix} \\Bigg) = N(\\textbf{0},\\textbf{K}) \\]\nTo model these data points using a Gaussian distribution , we need to construct a covariance matrix such that the terms \\(K_{ij}\\) captures the similarity or covariance between points i and j.\nThe similarity between two points can be captured using kernel functions.Consider a squared exponential kernel(also referred to as a radial basis function kernel) that captures the similarity between two points \\(X_i\\) and \\(X_j\\)\n\\[ K_{ij} =  e^{ -\\sigma || X_i - X_j ||^2} \\]\n\\[ K_{ij} = \\begin{cases}\n                     0, \\text{if}\\ ||X_i -X_j|| \\to \\infty \\\\\n                     1, \\text{if}\\ X_i = X_j\n          \\end{cases} \\]\nThe covariance matrix for the three points can be calculated as shown below\nlibrary(kernlab)\nrbfkernel &lt;- rbfdot(sigma = 0.1) # Set value of hyperparameter\npoint1 &lt;- c(1)\npoint2 &lt;- c(1.5)\npoint3 &lt;- c(3)\n\nK12 &lt;- K21 &lt;- rbfkernel(point1,point2)\nK23 &lt;- K32 &lt;- rbfkernel(point2,point3)\nK13 &lt;- K31 &lt;-rbfkernel(point1,point3)\nK11 &lt;- rbfkernel(point1,point1)\nK22 &lt;- rbfkernel(point2,point2)\nK33 &lt;- rbfkernel(point3,point3)\n\n\nK &lt;- matrix(c(K11,K12,K13,K21,K22,K23,K31,K32,K33),byrow=TRUE,nr=3)\nprint(K)\n##           [,1]      [,2]      [,3]\n## [1,] 1.0000000 0.9753099 0.6703200\n## [2,] 0.9753099 1.0000000 0.7985162\n## [3,] 0.6703200 0.7985162 1.0000000\nNow that we have learned a model to fit the three points, we want to predict the value of a fourth point \\(x^*\\), the predicted value for this point \\(f^*\\) can lie anywhere along the red vertical lines.\nplot(data.frame(x = c(1,1.5,3) , f = c(0.2,0.25,0.8)),xlim=c(0,5),ylim = c(0,1),pch=16)\nabline(v= 2.5,col='red')\ntext(x=c(1,1.5,3,2.15),y = c(0.25,0.3,0.85,0.5),labels=c('(x1,f1)','(x2,f2)','(x3,f3)','(x*=2.5,f*)'))\n\nIn order to capture the relationship between the four points, the four points can be modeled as a 4D Gaussian defined as follows:\n\\[ \\begin{bmatrix} \\textbf{f} \\\\\\\\ f^* \\end{bmatrix} = N\\Bigg( \\begin{bmatrix} \\textbf{0} \\\\\\\\ 0 \\end{bmatrix},\\begin{bmatrix} \\textbf{K} & \\textbf{K}_* \\\\\\\\ \\textbf{K}_*^T & K_{**} \\end{bmatrix} \\Bigg) \\]\nwhere:\n\\[ \\textbf{f} = \\begin{bmatrix} f_1 \\\\\\\\ f_2 \\\\\\\\ f_3 \\end{bmatrix} \\] \\[ \\textbf{0} = \\begin{bmatrix} 0 \\\\\\\\ 0 \\\\\\\\ 0 \\end{bmatrix} \\] \\[ \\textbf{K} =\\begin{bmatrix} K_{11} & K_{12} & K_{13} \\\\\\\\\n                               K_{21} & K_{22} & K_{23} \\\\\\\\\n                               K_{31} & K_{32} & K_{33}\n                \\end{bmatrix} \\] \\[ \\textbf{K}_* = \\begin{bmatrix} K_{1*} \\\\\\\\ K_{2*} \\\\\\\\ K_{3*} \\end{bmatrix} \\]\nThis is a joint distribution over \\(\\textbf{f}\\) and \\(f^*\\). Given \\(\\textbf{f}\\) which we already know from the three available data points, we can use the Multivariate Gaussian Theorem to estimate the distribution over \\(f_*\\). The mean and variance of this distribution is given by:\n\\[ \\mu^* = \\textbf{K}_*^{T}\\textbf{K}^{-1}f \\]\n\\[ \\sigma^* = K_{**} - \\textbf{K}_*^{T} \\textbf{K}^{-1} \\textbf{K}_* \\]\nWe can get mean and variance of the distribution over the unknown value at x = 2.5 as follows\nf &lt;- c(0.2,0.25,0.8)\n\npoint_star &lt;- c(2.5)\n\nK1star &lt;- Kstar1 &lt;- rbfkernel(point1,point_star)\nK2star &lt;- Kstar2 &lt;- rbfkernel(point2,point_star)\nK3star &lt;- Kstar3 &lt;-rbfkernel(point3,point_star)\nKstarstar &lt;- rbfkernel(point_star,point_star)\n\nKstar = c(K1star,K2star,K3star)\nKstar_t = t(Kstar)\nK_inv = solve(K)\n\n##The mean##\nmu_star = Kstar_t%*%K_inv%*%f\n\n##The  standard deviation##\nsigma_star = sqrt(Kstarstar - Kstar_t%*%K_inv%*%Kstar)\n\nprint(paste0('Mean of f* = ',mu_star))\n## [1] \"Mean of f* = 0.575736376567534\"\nprint(paste0('Standard deviation of f* = ',sigma_star))\n## [1] \"Standard deviation of f* = 0.0264409370474204\"\nThe prediction is shown below in green using an error bar.\nplot(data.frame(x = c(1,1.5,3,point_star) , f = c(0.2,0.25,0.8,mu_star)),xlim=c(0,5),ylim = c(0,1),pch=16, col = c(rep('black',3),'green'))\narrows(point_star,mu_star - 1.96*sigma_star,point_star,mu_star + 1.96*sigma_star,length = 0.05,angle=90,\n       code=3,col='green')\n\nSimilarly the prediction of any point along the x-axis can be found. Below the predictions for a sequence of points along the x-axis have been generated and plotted along with the their 95% confidence intervals.\nFor any input value, the Gaussian process model gives us a mean and a variance for the prediction.It is in effect a function.\npredict &lt;- function(input){\n  \n  point_star &lt;- input\n\n  K1star &lt;- Kstar1 &lt;- rbfkernel(point1,point_star)\n  K2star &lt;- Kstar2 &lt;- rbfkernel(point2,point_star)\n  K3star &lt;- Kstar3 &lt;-rbfkernel(point3,point_star)\n  Kstarstar &lt;- rbfkernel(point_star,point_star)\n\n  Kstar = c(K1star,K2star,K3star)\n  Kstar_t = t(Kstar)\n  K_inv = solve(K)\n\n  ##The mean##\n  mu_star = Kstar_t%*%K_inv%*%f\n\n  ##The standard deviation##\n  sigma_star = tryCatch(sqrt(Kstarstar - Kstar_t%*%K_inv%*%Kstar),\n                        warning = function(e){0})\n  \n  return(c(mu_star,sigma_star))\n}\n\n##Vector of points to generate predictions on\nx &lt;- seq(0,5,by=0.1)\n\npredictions &lt;- sapply(x,predict)\nmean &lt;- predictions[1,]\n#lower bound\nlb &lt;- predictions[1,] - 1.96*predictions[2,]\n#upper bound\nub &lt;-predictions[1,] + 1.96*predictions[2,]\n#Bind together in a dataframe\npreds &lt;- data.frame(cbind(x,mean,lb,ub))\n#Inputs\ninputs &lt;-  data.frame(x=c(point1,point2,point3),f=f)\n\n\nggplot(preds,aes(x=x,y=mean)) + \n  geom_ribbon(aes(x=x,ymin=lb,ymax=ub),fill='grey70')+\n  geom_line(col='green')+\n  geom_point(data=inputs,mapping=aes(x=x,y=f))+labs(y='y')\n\nWe can clearly see that the uncertainty near the training data points is low, as we move away from the training data points, the level of uncertainty increases. This aligns very well with our intuition."
  },
  {
    "objectID": "blog/2019-08-11-introduction-to-gaussian-processes/index.html#gaussian-processes",
    "href": "blog/2019-08-11-introduction-to-gaussian-processes/index.html#gaussian-processes",
    "title": "Introduction to Gaussian Processes",
    "section": "Gaussian Processes",
    "text": "Gaussian Processes\nThe Gaussian process is thus the generalization of a multi-variate normal distribution to infinitely many variables.It can be thought of as a Gaussian distribution over functions( thinking of functions as infinitely long vectors containing the value of the function at every input)\nThe choice of a kernel function encodes a prior belief that the data points that lie close to each other should result in similar function outputs.The value of the hyper parameter sigma encodes how smooth the functions are.\nConsider a sequence of test points from -5 to 5. The GP prior, or the distribution over the possible space of functions when no training data is available is as follows. Only three functions have been sampled here.\nx_vec &lt;- seq(-5,5,by=0.1)\nrbfkernel &lt;- rbfdot(sigma = 1) # Set value of hyperparameter\ncov_mat &lt;- kernelMatrix(rbfkernel,x_vec,x_vec)\n\ngp_prior &lt;- data.frame(t(rmvnorm(5,mean= rep(0,length(x_vec)),sigma = cov_mat)))\n\n\nmatplot(x=x_vec,y = gp_prior,lty=1,type='l',xlab ='x',ylab = 'y',col=c(1:10))\n\nAssume the data generating process is a sine function, and the value of the sine function at three points 1,1.5 and 3 were observed as shown below.\nplot(x=x_vec,y=sin(x_vec),type='l',xlab='x',ylab='y')\npoints(x=c(0.5,2,3),y=sin(c(0.5,2,3)),pch =16)\n\ntrain_x &lt;- c(0.5,2,3)\ntest_x &lt;- x_vec\nf &lt;- sin(train_x)\nK &lt;- kernelMatrix(rbfkernel,train_x,train_x)\nK_star &lt;- kernelMatrix(rbfkernel,train_x,test_x)\nK_starstar &lt;- kernelMatrix(rbfkernel,test_x,test_x)\nK_inv &lt;- solve(K)\nThe mean and variance of the test points are calculated as follows. Note that there are more efficient ways of doing the linear algebra calculations shown below, specifically be reducing the matrix inverse to linear system of equations. References for doing this have been provided in the appendix.\nmu_star &lt;- t(K_star)%*%K_inv%*%f\nSigma_star &lt;- K_starstar - t(K_star)%*%K_inv%*%K_star\nSampling from this GP posterior produces the following posterior distribution of functions.\ngp_posterior &lt;- data.frame(t(rmvnorm(5,mean= rep(0,length(mu_star)),sigma = Sigma_star)) + as.vector(mu_star))\nmatplot(x=test_x,y = gp_posterior,lty=1,type='l',xlab ='x',ylab = 'y',col=c(1:10))\nlines(x=x_vec,y=sin(x_vec),lty=2,lwd=2)\npoints(x=train_x,y=f,pch=16)\n\nIt can be seen that the presence of data squishes all three functions to a single point.The variance across the three functions goes to 0 at these points and increases with distance from these points."
  },
  {
    "objectID": "blog/2019-08-11-introduction-to-gaussian-processes/index.html#conclusion",
    "href": "blog/2019-08-11-introduction-to-gaussian-processes/index.html#conclusion",
    "title": "Introduction to Gaussian Processes",
    "section": "Conclusion",
    "text": "Conclusion\nGaussian Processes provide an interesting alternative to generalized linear models. It comes with a minimal set of assumptions and provides a quantitative estimate of uncertainty in predictions. This also makes it applicable in areas like active learning.\nGaussian Process are very computationally intensive and it’s application to large data sets have therefore been limited. Research in Gaussian Processes have been focused on making these computations more tractable."
  },
  {
    "objectID": "blog/2019-08-11-introduction-to-gaussian-processes/index.html#references-and-additional-resources",
    "href": "blog/2019-08-11-introduction-to-gaussian-processes/index.html#references-and-additional-resources",
    "title": "Introduction to Gaussian Processes",
    "section": "References and Additional Resources",
    "text": "References and Additional Resources\n\nAlmost all the content in this write up comes from Nando de Freitas’ UBC lecture\nA second lecture I referenced is available here\nGPyTorch: An open source project leveraging GPs for machine learning using PyTorch.\nkernlab: A package in R for Kernel Methods\nA high quality intro to GPs from distill.pub\nLargely the same content in python"
  },
  {
    "objectID": "blog/2021-12-24-how-not-to-be-unhappy/index.html",
    "href": "blog/2021-12-24-how-not-to-be-unhappy/index.html",
    "title": "How not to be unhappy",
    "section": "",
    "text": "The famous opening sentence of Anna Karenina is\n\n“Happy families are all alike; every unhappy family is unhappy in its own way”\n\nMy own version of this is as follows\n\n“Unhappy individuals are alike; every happy individual is happy in his or her own way”.\n\nThis does assume that the basic needs of a human being – food, shelter and security have been met.\nFrom my experience, there is no simple answer to what makes one happy – it could be career, money, fame, family, adventure or something else entirely. Unhappiness on the other hand tends to stem from a few identifiable sources.\nThis leads me to be believe that if we are able to identify these sources of unhappiness and remedy them, we have a great chance of being happy. Rather than chase happiness, resist the habits and impulses that make you unhappy.\nAs Charlie Munger says “Invert, always invert”.\nI think the roots of unhappiness can be boiled down to the following 1. An unhealthy body and an unhealthy mind  2. Focusing on things you cannot control  3. Comparing yourself to others\nAddressing these are simple but simple things are often very hard to do.\nThe first of the three is perhaps the easiest. Regular exercise, mindfulness meditation and keeping a gratitude journal can help immensely in this regard. Therapy seems to work wonders for some people. Prioritizing physical and mental health is the biggest favor you can do to our future self.\nThe second is harder. You need to build a filter for your thoughts – a debugger of sorts. When a thought pops into your head, ask yourself - is this something I can do anything about? if yes – act; if not, ignore it. I do this for issues ranging from politics to the fluctuating fortunes of my favorite sports team. This is the central principle of stoicism.\nAny thought can pop into your head, you need to train your mind so that you run this thought through a filter next rather than letting it grow and morph into a runaway train. I have a long way to go to master this but I have made enough progress to know this is possible.\nThe third is perhaps the hardest. We have been brought up in a system where success seems to be a zero-sum game. It takes some effort to unlearn these habits and come to the realization that life is a single player game. Your success does not preclude mine.\nWe all have our own clocks and yard sticks to measure ourselves. Life is not a race where you get a prize for being better than someone. The goal is to become the best we can be, not to outdo others.\nWhat has helped me here is writing down some of these principles that I aspire to live by. I read them everyday and this helps me internalize these ideas better. I do fall off the wagon once in a while, but reading these lets me reaffirm the values I care about and helps me get back on track.\nThere is still a long way to go but I am taking baby steps every day towards becoming a better, more content person."
  },
  {
    "objectID": "blog/2021-04-11-a-crisis-is-a-terrible-thing-to-waste.en/index.html",
    "href": "blog/2021-04-11-a-crisis-is-a-terrible-thing-to-waste.en/index.html",
    "title": "A Crisis is a terrible thing to waste",
    "section": "",
    "text": "I accompanied my wife to Manhattan yesterday for her scheduled vaccine appointment. This was my first visit to the city in almost 6 months.\nNYC is certainly back on its feet and humming along. From what I saw, almost 95% of people were wearing masks and commuters were socially distancing on the subway.\nOne thing I noticed was how the restaurant industry had responded to servicing customers in the winter in the middle of a pandemic. Outdoor seating was a lifeline for the restaurant industry in the summer, an option that was unfeasible during the harsh winter.\nCreative entrepreneurs of course found a way. Enter – heated tents, wooden structures and igloos.\n\n\n\n\n\nFigure 1: Image from Will Feuer, CNBC\n\n\nAs temperatures rise and the pandemic abates, these structures will go down but they are certain to re-appear the next winter. In a way, crisis has forced the industry to respond creatively that will pay off over the long run.\nTypically, during the winter, restaurants roll back outdoor dining significantly reducing capacity. Now restaurants will be able to continue offering outdoor seating during the winters, a measure that should help boost revenue.\nThe economist Paul Romer said a crisis is a terrible thing to waste. The restaurant industry certainly has taken a beating in this crisis, but they certainly haven’t wasted it."
  },
  {
    "objectID": "blog/2024-03-16-meta-s-anti-fragility/index.html",
    "href": "blog/2024-03-16-meta-s-anti-fragility/index.html",
    "title": "Meta’s Anti-fragility",
    "section": "",
    "text": "As Nassim Nicholas Taleb defines it, anti-fragility is the quality that allows a system not only to resist shocks but also to become stronger from them.\nCan companies in a market economy be anti-fragile? In the modern, market economy, where the life span of companies keeps getting shorter and the pace of technological change keeps accelerating, perhaps this is an impossible standard.\nHowever, I believe Meta is one company that could be considered worthy of this tag.\nI can think of at least four events which posed an existential or serious risk to Meta (or Facebook at the time). The company negotiated and emerged stronger from each of these challenges.\nIs there another company that has navigated so many challenges and emerged stronger? What is certain is that overcoming these challenges has likely allowed Meta to develop the muscle to potentially handle challengers or threats better than any other company.\nCouple this with a visionary, war time, founder CEO, and you have a company that only a brave person will bet against.\nEven so, it remains to be seen whether Meta’s anti-fragility is just a function of Mark Zuckerburg’s vision and leadership. Great companies can outlast its founders, whether Meta can pass that test remains to be seen."
  },
  {
    "objectID": "blog/2024-03-16-meta-s-anti-fragility/index.html#footnotes",
    "href": "blog/2024-03-16-meta-s-anti-fragility/index.html#footnotes",
    "title": "Meta’s Anti-fragility",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nhttps://www.cnbc.com/2017/06/21/when-zuckerberg-said-facebook-must-focus-on-mobile-nothing-happened.html↩︎\nhttps://www.vanityfair.com/news/2016/06/how-mark-zuckerberg-led-facebooks-war-to-crush-google-plus↩︎\nhttps://stratechery.com/2024/meta-earnings-meta-ai-and-att-metas-new-playbook/↩︎\nhttps://www.eugenewei.com/blog/2020/9/18/seeing-like-an-algorithm↩︎"
  },
  {
    "objectID": "blog/2020-05-24-learning-with-noisy-labels/index.html",
    "href": "blog/2020-05-24-learning-with-noisy-labels/index.html",
    "title": "Learning with Noisy Labels",
    "section": "",
    "text": "One of the challeges in AML is that labels are not clean and prone to human error. Sometimes, decisioning happens at one level e.g. Case, while modelling happens at a different level. e.g. Alert. This often requires inferring the alert labels from case labels that introduces noise or it requires analysts to manually go and decision alerts which can be time consuming.\nBesides the fact that incidence of money laundering is not often clear cut or very evident means that there is some degree of noise in the labels provided by any investigator.\nThere has been extensive reasearch into learning with Noisy labels. One of the most usable approaches has been described in this paper and implemented in this open source python package"
  },
  {
    "objectID": "blog/2020-05-24-learning-with-noisy-labels/index.html#introduction",
    "href": "blog/2020-05-24-learning-with-noisy-labels/index.html#introduction",
    "title": "Learning with Noisy Labels",
    "section": "",
    "text": "One of the challeges in AML is that labels are not clean and prone to human error. Sometimes, decisioning happens at one level e.g. Case, while modelling happens at a different level. e.g. Alert. This often requires inferring the alert labels from case labels that introduces noise or it requires analysts to manually go and decision alerts which can be time consuming.\nBesides the fact that incidence of money laundering is not often clear cut or very evident means that there is some degree of noise in the labels provided by any investigator.\nThere has been extensive reasearch into learning with Noisy labels. One of the most usable approaches has been described in this paper and implemented in this open source python package"
  },
  {
    "objectID": "blog/2020-05-24-learning-with-noisy-labels/index.html#overview-of-approach",
    "href": "blog/2020-05-24-learning-with-noisy-labels/index.html#overview-of-approach",
    "title": "Learning with Noisy Labels",
    "section": "Overview of Approach",
    "text": "Overview of Approach\n\nAssumptions\n\nClass Conditional Noise Process\nIt is assumed that labels follow a class conditional noise process i.e. the label errors are dependant on the true latent class of an observation. This means that an oracle can draw a clean alert i.e. non suspicious from a distribution of clean alerts , but while reporting whether it is suspicious or not, makes independant ,random mistakes with some unkown probabiltiy \\(\\eta\\)&lt; 0.5 ; assume\\(\\eta\\) = 0.4\nThis means that after drawing a clean example, the oracle flips a coin with probability of showing head of \\(1 - \\eta\\) (0.6). If the coin comes up as head, the example is reported to be clean else, it is reported as suspicious.\nSimilarly for suspicious alerts, mistakes are made with some specified proabability.\n\n\n\nMethodology\n\nEstimate the joint distribution of given, noisy labels and latent (unknown) uncorrupted labels to fully characterize class-conditional label noise.\nFind and prune noisy examples with label issues.\nTrain with errors removed, re-weighting examples by the estimated latent prior.\n\nThe central idea of the approach is ‘self confidence’. Self confidence is the predicted probability that an example \\(x\\)belongs to its given label\\(\\tilde{y}\\) i.e. there is no label error.If the predicted probability of a class is greater than a threshold given by the average predicted proababilties of examples in that class i.e if there is high self confidence, the example is confidently counted as belonging to the specified class.\nThis idea of ‘self confidence’ is used to estimate the joint distribution between true and noisy labels.Once this is estimated, the number of label issues in a data set i.e. clean alerts being labelled suspicious(Type I error) and suspicious alerts being labelled clean(Type II error)-can be estimated. For each type of error, the most likely mislabelled examples are identifed.\nThe model is then retrained after removing the errors and appropriately reweighting the examples.\nRefer to the original paper for extra details."
  },
  {
    "objectID": "blog/2020-05-24-learning-with-noisy-labels/index.html#experiment",
    "href": "blog/2020-05-24-learning-with-noisy-labels/index.html#experiment",
    "title": "Learning with Noisy Labels",
    "section": "Experiment",
    "text": "Experiment\nNow we will examine how effective this method is using some simulated data that is representative of the type of labellign errors we see in AML use cases\n\nData\nConsider the HRCP Scenario being used at an institutions. The scenario generates alerts when the Very High Risk Amount is greater than $5000 and the Very High Risk Transaction Count is greater than 10. It is assumed that these two parameters alone are sufficient to detect the truly effective alerts and the decision boundary is linear. Effectiveness on one side of the decision boundary is considered to be 90% while it is 5% on the other side.\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nnp.random.seed(10)\n#Simulate alert parameters\nN = 5000 # no of samples\nAmt = np.asarray(np.random.uniform(low=5000,high=20000,size=N))\nPerc = np.asarray(np.random.uniform(low=10,high=100,size=N))\nalerts = np.column_stack((Amt,Perc))\ninside_region_indices = np.ravel(np.argwhere(( 125 * Perc +  Amt &gt;= 27400)))\noutside_region_indices = np.setdiff1d(np.arange(5000),inside_region_indices)\n#90% of observations inside the region are effective\neffective_indices_i = np.random.choice(inside_region_indices,size = int(len(inside_region_indices)*0.9),replace= False)\n#5% of observations outside the region are effective\neffective_indices_o = np.random.choice(outside_region_indices,size = int(len(outside_region_indices)*0.05),replace=False)\n#Combone both\neffective_indices =  np.append(effective_indices_i,effective_indices_o)\neffective_flag1 = np.zeros(N,dtype=int)\neffective_flag1[effective_indices] = 1\nCreate a pandas dataframe as shown below\nalerts_df = pd.DataFrame([Amt,Perc,effective_flag1]).T\nalerts_df.columns = ['Amt','Perc','effective_flag']\nalerts_df['effective_flag'] = alerts_df['effective_flag'].astype('category')\nalerts_df.head()\n\n\n\n\n\n\nAmt\nPerc\neffective_flag\n\n\n\n\n0\n16569.809649\n23.055201\n0.0\n\n\n1\n5311.279240\n32.199579\n0.0\n\n\n2\n14504.723524\n17.939790\n0.0\n\n\n3\n16232.058238\n52.469434\n0.0\n\n\n4\n12477.605185\n66.851593\n0.0\n\n\n\n\nThe frequency count of the effective flag is as follows\nalerts_df['effective_flag'].value_counts()\n0.0    4444\n1.0     556\nName: effective_flag, dtype: int64\nsns.set(style='whitegrid')\nsns.scatterplot(x=\"Amt\",y=\"Perc\",hue=\"effective_flag\",data=alerts_df,legend=False)\nplt.show()\n\nNow consider that there have been labelling errors due to inferring the disposition of alerts from cases. This causes the disposition of some number of alerts across the alert region to be flipped from non effective to effective. We will assume the disposition of 5% of the non effective alerts have been flipped from effective to non effective\nnon_effective_indices = np.setdiff1d(np.arange(5000),effective_indices)\n#The mislabelled indices are pre selected\nmislabelled_indices = np.random.choice(non_effective_indices,size = int(len(non_effective_indices)*0.05),replace=False)\neffective_flag2 = np.copy(effective_flag1)\neffective_flag2[mislabelled_indices] = 2\nalerts_df['effective_flag'] =pd.Series(effective_flag2).astype('category')\nThe frequency count of the updated effective flag is as follows\nalerts_df['effective_flag'].value_counts()\n0    4222\n1     556\n2     222\nName: effective_flag, dtype: int64\nThe udpated dataset looks as follows.\nsns.set(style='whitegrid')\nsns.scatterplot(x=\"Amt\",y=\"Perc\",hue=\"effective_flag\",data=alerts_df,legend=False)\nplt.show()\n\nCreate a new dataset with observed and true labels.\ntrue_labels = pd.Series(effective_flag1)\nobserved_labels = pd.Series((effective_flag2 &gt;0).astype(int))\nalerts_df_final = alerts_df.iloc[:,0:2]\nalerts_df_final['true_labels'] = true_labels\nalerts_df_final['observed_labels'] = observed_labels\nalerts_df_final.head()\n\n\n\n\n\n\nAmt\nPerc\ntrue_labels\nobserved_labels\n\n\n\n\n0\n16569.809649\n23.055201\n0\n0\n\n\n1\n5311.279240\n32.199579\n0\n0\n\n\n2\n14504.723524\n17.939790\n0\n0\n\n\n3\n16232.058238\n52.469434\n0\n0\n\n\n4\n12477.605185\n66.851593\n0\n0\n\n\n\n\ntrue_labels.value_counts()\n0    4444\n1     556\ndtype: int64\nobserved_labels.value_counts()\n0    4222\n1     778\ndtype: int64"
  },
  {
    "objectID": "blog/2020-05-24-learning-with-noisy-labels/index.html#using-cleanlab",
    "href": "blog/2020-05-24-learning-with-noisy-labels/index.html#using-cleanlab",
    "title": "Learning with Noisy Labels",
    "section": "Using cleanlab",
    "text": "Using cleanlab\nFirst and foremost cleanlab can be used to identify the noisy labels\n#Create dataset compatible with sklearn\nX = alerts_df_final[['Amt','Perc']].values\ny_observed = observed_labels.values\ny_true = true_labels.values\nImport required packages and functions\nimport cleanlab\nfrom cleanlab.latent_estimation import estimate_py_noise_matrices_and_cv_pred_proba\nfrom sklearn.linear_model import LogisticRegression\nfrom cleanlab.pruning import get_noise_indices\nclf1 = LogisticRegression(multi_class = 'ovr', max_iter =1000,solver = 'lbfgs')\nest_py , est_nm, est_inv,confident_joint, my_psx = estimate_py_noise_matrices_and_cv_pred_proba(X = X,\n                                                                                                s= y_observed,\n                                                                                               clf = clf1)\npredicted_label_errors = get_noise_indices(y_observed,my_psx,n_jobs=1)\npredicted_label_error_indices = np.argwhere(predicted_label_errors==True)\nprint('The dataset had a total of {} label errors'.format(len(mislabelled_indices)))\nprint('Clean lab has predicted a total of {} label errors'.format(len(predicted_label_error_indices)))\nprint('Out of the {} true label errors, clean lab has correctly identified {}'.format(len(mislabelled_indices),\n                                                                                     len(np.intersect1d(predicted_label_error_indices,\n                                                                                                       mislabelled_indices))))\nThe dataset had a total of 222 label errors\nClean lab has predicted a total of 424 label errors\nOut of the 222 true label errors, clean lab has correctly identified 139\nWe an also evaluate the performance of the classfier after pruning the examples identified as noisy by cleanlab. First we will train a classifier without using cleanlab. For training we will use the observed labels while for testing we will use the true labels.\nfrom sklearn.model_selection import train_test_split\nX_train,_,y_train,_ = train_test_split(X,y_observed,test_size = 0.3,shuffle=False)\n_,X_test,_,y_test = train_test_split(X,y_true,test_size = 0.3,shuffle=False)\nbincount = np.bincount(y_train)\nthreshold = bincount[1]/sum(bincount)\nprint('The threshold used for classification is the prior probability of effectiveness \\\nin the training data {0:.3f}'.format(threshold))\nThe threshold used for classification is the prior probability of effectiveness in the training data 0.158\nfrom sklearn.metrics import f1_score,precision_score,recall_score\n\nclf = LogisticRegression(solver='lbfgs', multi_class='ovr', max_iter=1000)\n_ = clf.fit(X_train,y_train)\npred_prob = clf.predict_proba(X_test)[:,1]\npred = (pred_prob &gt;threshold).astype(int)\nprint('Without confident learning, the f1_score produced by the model is {:.3f}'.format(f1_score(y_test,pred)) )\nprint('Without confident learning, the precision_score produced by the model is {:.3f}'.format(precision_score(y_test,pred)))\nprint('Without confident learning, the recall_score produced by the model is {:.3f}'.format(recall_score(y_test,pred)))\nWithout confident learning, the f1_score produced by the model is 0.099\nWithout confident learning, the precision_score produced by the model is 0.057\nWithout confident learning, the recall_score produced by the model is 0.389\nNow we will see what model performance looks like when using confident learning\nfrom cleanlab.classification import LearningWithNoisyLabels\nclf = LogisticRegression(solver='lbfgs', multi_class='ovr', max_iter=1000)\nrp = LearningWithNoisyLabels(clf =clf,seed=2)\n_ = rp.fit(X_train,y_train)\npred_prob = rp.predict_proba(X_test)[:,1]\npred = (pred_prob &gt;threshold).astype(int)\nprint('With confident learning, the f1_score produced by the model is {:.3f}'.format(f1_score(y_test,pred)) )\nprint('With confident learning, the precision_score produced by the model is {:.3f}'.format(precision_score(y_test,pred)))\nprint('With confident learning, the recall_score produced by the model is {:.3f}'.format(recall_score(y_test,pred)))\nWith confident learning, the f1_score produced by the model is 0.316\nWith confident learning, the precision_score produced by the model is 0.208\nWith confident learning, the recall_score produced by the model is 0.654"
  },
  {
    "objectID": "blog/2021-10-23-learning-julia/index.html",
    "href": "blog/2021-10-23-learning-julia/index.html",
    "title": "Learning Julia",
    "section": "",
    "text": "It has been a while since my last blog post and this is not because I have been idling. I have been flirting with Julia (no pun intended) – a high performance programming language for scientific computing.\nI have been working through the MOOC – Introduction to Computational Thinking with Julia - to pick up the language. The course is really good and besides teaching you Julia, introduces several foundational ideas in data science and statistical thinking.\nIn fact, I have been coaxing a couple of my cousins and my nephew; all in their late teens; to work through this course to pick up some computational thinking skills.\nSome early reactions to what I have learned so far.\n1.Julia brings together the best of Python and R. The package management in Julia is hassle free like in R, yet the language is incredibly easy to use and general purpose like Python.  2.The multiple dispatch system lives up to the hype. 3.The Pluto notebook project despite being in its infancy seems really promising and sleek. 4.Seam Carving, which I learned about in this MOOC is perhaps the most simple, elegant, practically useful algorithm I have come across.\nI have also been exploring the JuMP package as I have also been diving into Discrete Optimization on the side.\nI packaged some of the things I learned and did a presentation to my team as part of the “Monthly Learning Hour”. See the video below you if you are interested. The code used can be found here: https://github.com/govindgnair23/Intro_to_Julia"
  },
  {
    "objectID": "blog/2024-07-09-explanations-causality-and-theories-an-aml-perspective/index.html",
    "href": "blog/2024-07-09-explanations-causality-and-theories-an-aml-perspective/index.html",
    "title": "Explanations, Causality and Theories - An AML Perspective",
    "section": "",
    "text": "Science is the sum total of human knowledge gained by man through experimentation and observation\nThis is the definition of science I recall from my days in middle school. Alas, it is incomplete. It omits perhaps the most important step of the scientific method - conjecture."
  },
  {
    "objectID": "blog/2024-07-09-explanations-causality-and-theories-an-aml-perspective/index.html#hard-to-vary.",
    "href": "blog/2024-07-09-explanations-causality-and-theories-an-aml-perspective/index.html#hard-to-vary.",
    "title": "Explanations, Causality and Theories - An AML Perspective",
    "section": "Hard to vary.",
    "text": "Hard to vary.\nIn his book “The Beginning of Infinity”,Deutch uses Norse mythology as an example of a theory that is easy to vary.\nAccording to Norse mythology,the seasons change according to the fortunes of Freyr, the god of spring, in his eternal war with the forces of cold and darkness. When Freyr wins, the earth experiences summer while when he loses, winter sets in. This is a poor explanation because any variation in the seasons can be attributed to Freyr’s vicissitudes.It is extremely easy to vary to fit any observation.Such explanations are not even worth testing.\nIn contrast,the Tilt theory of seasons correctly explains that seasons are caused by the tilt of the earth’s axis of rotation relative to the plane of orbit.The hemisphere in which the sun’s rays fall vertically have summer while the hemisphere where the rays strike obliquely have winter. This theory, constrained by what we know about geometry,heat and mechanics of the earth’s motion is much harder to vary and is thus a much better explanation."
  },
  {
    "objectID": "blog/2024-07-09-explanations-causality-and-theories-an-aml-perspective/index.html#falsifiable",
    "href": "blog/2024-07-09-explanations-causality-and-theories-an-aml-perspective/index.html#falsifiable",
    "title": "Explanations, Causality and Theories - An AML Perspective",
    "section": "Falsifiable",
    "text": "Falsifiable\nFalsifiability, introduced by Karl Popper, is a necessary criterion which makes a theory scientific. A theory has to make testable predictions which can be validated through observations. A scientific theory should thus be falsifiable.\nThe tilt theory of seasons, for example can make predictions for seasons in both hemispheres. If these predictions are refuted, the theory can be discarded.\nInterestingly, the Norse myth is also falsifiable. As per the myth, if Freyr is winning, the whole earth will enjoy summer which is easily disproved by observing different seasons in the northern and southern hemispheres. Falsifiability is thus a necessary not a sufficient criterion for a scientific theory."
  },
  {
    "objectID": "blog/2024-07-09-explanations-causality-and-theories-an-aml-perspective/index.html#reach",
    "href": "blog/2024-07-09-explanations-causality-and-theories-an-aml-perspective/index.html#reach",
    "title": "Explanations, Causality and Theories - An AML Perspective",
    "section": "Reach",
    "text": "Reach\nA powerful explanatory theory will also have reach. A theory with reach will be applicable to a wide variety of cases and phenomenon beyond its original scope.\nEinstein’s theory of general relativity is perhaps the finest example of a theory with reach. Einstein’s field equations predicted the existence of black holes and gravitational waves, something that never occurred to Einstein when he originally formulated the theory."
  },
  {
    "objectID": "blog/2024-07-09-explanations-causality-and-theories-an-aml-perspective/index.html#why-is-an-episode-of-transactional-activity-suspicious",
    "href": "blog/2024-07-09-explanations-causality-and-theories-an-aml-perspective/index.html#why-is-an-episode-of-transactional-activity-suspicious",
    "title": "Explanations, Causality and Theories - An AML Perspective",
    "section": "Why is an episode of transactional activity suspicious ?",
    "text": "Why is an episode of transactional activity suspicious ?\nAs the Fincen guidance lays out, an episode is suspicious if it conforms to some pattern of suspicious activity or red flag, such as:\n\nUnusually complex series of transactions indicative of layering activity involving multiple accounts, banks, parties, jurisdictions;\n\nAn episode of activity is suspicious because it is consistent with a red flag. This is the answer to “Why?”\n“How” describes how the episode of activity fits into the aforementioned pattern.\nIn other words, the “Why” is the cause of suspicion. As per the counterfactual definition of causality,this also means that if the transactions did not fit in with the pattern, there would be no reason to consider the transaction suspicious. We are interested in identifying “necessary” causation. Typically, none of these red flags or patterns in isolation guarantees the activity in question is suspicious; it is not a sufficient cause.\nThis suggests a very scientific approach to investigations.\nEvery alert or case begins with a null hypothesis (H0) and an alternate hypothesis (H1)\nH0: The entity did not carry out transactions consistent with a pattern of interest.  H1: The entity carried out transactions with the pattern of interest.\nAn investigator is trying to collect evidence to reject the null hypothesis. If sufficient evidence; which is determined by the risk tolerance of the institution; is available, then a SAR has to be filed outlining the “Why” and the “How”—the hypothesis tested and the evidence gathered."
  },
  {
    "objectID": "blog/2024-07-09-explanations-causality-and-theories-an-aml-perspective/index.html#transaction-monitoring-systems-need-to-evolve",
    "href": "blog/2024-07-09-explanations-causality-and-theories-an-aml-perspective/index.html#transaction-monitoring-systems-need-to-evolve",
    "title": "Explanations, Causality and Theories - An AML Perspective",
    "section": "Transaction Monitoring Systems need to evolve",
    "text": "Transaction Monitoring Systems need to evolve\nToday, transaction monitoring systems generate a case(a collection of alerts) and leave investigators to come up with their own hypotheses. This is equivalent to presenting the results of an experiment without stating the hypothesis or theory being tested.\nTo make investigators productive, transaction monitoring systems should evolve from just presenting a collection of alerts to also presenting the hypothesis being evaluated.\nA hypothesis is an explanation for potentially nefarious activity - a red flag. A good hypothesis or explanations should adhere to at least the first two criteria laid out by Deutch.\n\nIt should be hard to vary\nIt should be falsifiable\n\nThe more specific a pattern or red flag is, the harder it is to vary. For example, “the behavior of the entity changed” is a poor explanation for suspicious activity. A change in behavior could also be because someone sold their car or their house.\nA potentially good explanation for suspicious activity is:\n\nTransactions that are not commensurate with the stated business type and/or that are unusual and unexpected in comparison with the volumes of similar businesses operating in the same locale.\n\nGiven the hypothesis is specific, it is also eminently falsifiable.\nIn summary, a good transaction monitoring system doesn’t just generate a case or collection of alerts, it will also suggest good hypotheses that explains why the entity in question is suspicious."
  },
  {
    "objectID": "blog/2020-06-14-should-you-rebalance-an-imbalanced-dataset.en/index.html",
    "href": "blog/2020-06-14-should-you-rebalance-an-imbalanced-dataset.en/index.html",
    "title": "Should you rebalance an imbalanced dataset?",
    "section": "",
    "text": "This post looks at the impact of rebalancing an imbalanced dataset and seeks to identify some heuristics/rules of thumb to determine when it is appropriate to rebalance an imbalanced dataset through oversampling/undersampling."
  },
  {
    "objectID": "blog/2020-06-14-should-you-rebalance-an-imbalanced-dataset.en/index.html#introduction",
    "href": "blog/2020-06-14-should-you-rebalance-an-imbalanced-dataset.en/index.html#introduction",
    "title": "Should you rebalance an imbalanced dataset?",
    "section": "",
    "text": "This post looks at the impact of rebalancing an imbalanced dataset and seeks to identify some heuristics/rules of thumb to determine when it is appropriate to rebalance an imbalanced dataset through oversampling/undersampling."
  },
  {
    "objectID": "blog/2020-06-14-should-you-rebalance-an-imbalanced-dataset.en/index.html#case-1-when-your-model-is-a-good-fit-for-your-data",
    "href": "blog/2020-06-14-should-you-rebalance-an-imbalanced-dataset.en/index.html#case-1-when-your-model-is-a-good-fit-for-your-data",
    "title": "Should you rebalance an imbalanced dataset?",
    "section": "Case 1: When your model is a good fit for your data",
    "text": "Case 1: When your model is a good fit for your data\nThis is the condition where your model has low bias and is an appropriate fit for the data on hand.\n\nOriginal Data\nConsider the true data you are working with below which is linearly separable. The data is imbalanced with a ratio of 1:4. 20% of the data is held out as a test set.\nset.seed(0)\nc1 &lt;- cbind('x1'=rnorm(200,0.6,0.12),'x2'=rnorm(200,0.4,0.15))\nc2 &lt;- cbind('x1'=rnorm(800,0.4,0.12),'x2'=rnorm(800,0.6,0.15))\noriginal_data &lt;- data.frame(rbind(c1,c2))\noriginal_data$y &lt;- as.factor(c(rep(1,200),rep(0,800)))\n\n#Split to train and test sets\ntrain_index &lt;- sample(c(1:nrow(original_data)),size = 0.8*nrow(original_data),replace=FALSE)\ntrain_data &lt;- original_data[train_index,]\ntest_data &lt;- original_data[-train_index,]\nThe summary of the datasets are as follows\nsummary(train_data)\n##        x1                x2           y      \n##  Min.   :0.01163   Min.   :-0.03573   0:640  \n##  1st Qu.:0.33082   1st Qu.: 0.44024   1:160  \n##  Median :0.42874   Median : 0.56336          \n##  Mean   :0.43496   Mean   : 0.56103          \n##  3rd Qu.:0.53504   3rd Qu.: 0.68092          \n##  Max.   :0.91904   Max.   : 1.02417\nsummary(test_data)\n##        x1                x2         y      \n##  Min.   :0.05498   Min.   :0.1563   0:160  \n##  1st Qu.:0.34173   1st Qu.:0.4410   1: 40  \n##  Median :0.44668   Median :0.5453          \n##  Mean   :0.43971   Mean   :0.5510          \n##  3rd Qu.:0.52208   3rd Qu.:0.6525          \n##  Max.   :0.79505   Max.   :1.0559\nA simple logistic regression model is built on the training data.\nglm1 &lt;- glm(y ~ ., data = train_data, family = 'binomial')\nsummary(glm1)\n## \n## Call:\n## glm(formula = y ~ ., family = \"binomial\", data = train_data)\n## \n## Deviance Residuals: \n##      Min        1Q    Median        3Q       Max  \n## -2.31817  -0.36147  -0.15567  -0.04157   2.72629  \n## \n## Coefficients:\n##             Estimate Std. Error z value Pr(&gt;|z|)    \n## (Intercept)  -5.0134     0.7983  -6.280 3.39e-10 ***\n## x1           15.0057     1.4026  10.698  &lt; 2e-16 ***\n## x2           -7.6975     0.9103  -8.456  &lt; 2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## (Dispersion parameter for binomial family taken to be 1)\n## \n##     Null deviance: 800.64  on 799  degrees of freedom\n## Residual deviance: 394.73  on 797  degrees of freedom\n## AIC: 400.73\n## \n## Number of Fisher Scoring iterations: 6\nBelow is a function to generate a plot of the decision boundaries and the confidence intervals.\nlibrary(ggplot2)\nplot_decision_boundary &lt;- function(model,boundary,data,title,min_=0,max_=1){\ngrid_dimension &lt;- seq(min_,max_,by =0.01)\ngrid &lt;- expand.grid(x1=grid_dimension,x2 = grid_dimension)\ngrid_preds &lt;- predict(model,newdata=grid,type='response')\n\npreds_ci &lt;-  predict(model,newdata=grid,type='link',se.fit=TRUE)\n\n#Calculate 95% prediction interval bounds\ncritval &lt;- 1.96 ## approx 95% CI\nupr &lt;- preds_ci$fit + (critval * preds_ci$se.fit)\nlwr &lt;- preds_ci$fit - (critval * preds_ci$se.fit)\n\n\n#These are logits, use inverse link function to get probabilties\nupr2 &lt;- model$family$linkinv(upr)\nlwr2 &lt;- model$family$linkinv(lwr)\n\ncontour_data &lt;- cbind(grid,lwr = lwr2,upr =upr2,pred = grid_preds) \n        \np &lt;- ggplot(data,aes(x=x1,y=x2))+geom_point(aes(colour = factor(y)),alpha =0.3)+labs(title=title) + theme (legend.title = element_blank())\n\n\np+ stat_contour(data = contour_data,aes(x = x1,y = x2,z=pred),breaks=c(boundary))+\n        stat_contour(data = contour_data,aes(x = x1,y = x2,z=lwr),breaks=c(boundary),linetype=2)+\n        stat_contour(data = contour_data,aes(x = x1,y = x2,z=upr),breaks=c(boundary),linetype=2)\n\n \n        \n}\nThe plot of the decision boundary and 95% confidence intervals from the model are shown below.The decision boundary is a contour corresponding to a probability of 0.2 given this is the proportion of the target class in the dataset.\nplot_decision_boundary(glm1,boundary=0.2,data=original_data,title= 'Original Data',min_=0,max_=1)\n\nThe overall performance of the model on the test dataset is shown below.\nlibrary(pROC)\ntest_pred_probs1 &lt;- predict(glm1,newdata = test_data,type='response')\nroc1 &lt;- roc(test_data$y,test_pred_probs1) \nplot(roc1,print.auc=TRUE)\n\n\n\nOversampled Data\nNow we oversample the minority class so that both classes are balanced.\nlibrary(ROSE)\ntrain_data_1 &lt;- sum(train_data$y==0)\noversampled_data &lt;- ovun.sample(y~.,data=train_data,method='over',N = 2*train_data_1)$data\nsummary(oversampled_data)\n##        x1                x2           y      \n##  Min.   :0.01163   Min.   :-0.03573   0:640  \n##  1st Qu.:0.38328   1st Qu.: 0.38133   1:640  \n##  Median :0.50435   Median : 0.50109          \n##  Mean   :0.49718   Mean   : 0.50539          \n##  3rd Qu.:0.60212   3rd Qu.: 0.63095          \n##  Max.   :0.91904   Max.   : 1.02417\nA second model is fitted on the oversampled data. Given we have more data, the standard error of the parameter estimates have reduced.\nglm2 &lt;- glm(y ~ ., data = oversampled_data, family = 'binomial')\nsummary(glm2)\n## \n## Call:\n## glm(formula = y ~ ., family = \"binomial\", data = oversampled_data)\n## \n## Deviance Residuals: \n##      Min        1Q    Median        3Q       Max  \n## -2.99430  -0.35839   0.00092   0.45946   2.30790  \n## \n## Coefficients:\n##             Estimate Std. Error z value Pr(&gt;|z|)    \n## (Intercept)  -4.4832     0.5698  -7.867 3.62e-15 ***\n## x1           17.1126     1.0804  15.839  &lt; 2e-16 ***\n## x2           -8.1692     0.6549 -12.474  &lt; 2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## (Dispersion parameter for binomial family taken to be 1)\n## \n##     Null deviance: 1774.46  on 1279  degrees of freedom\n## Residual deviance:  781.99  on 1277  degrees of freedom\n## AIC: 787.99\n## \n## Number of Fisher Scoring iterations: 6\nThe decision boundary corresponding to a contour of probability = 0.5 (given the data is balanced) is shown below with a 95% confidence limits .\nplot_decision_boundary(glm2,boundary=0.5,data=oversampled_data,title= 'Oversampled Data',min_=0,max_=1)\n\nAs expected the width of confidence intervals of the regression line shrink since we are now using more data.\nThe overall performance of the new model on the test dataset is shown below.\nlibrary(pROC)\ntest_pred_probs2 &lt;- predict(glm2,newdata = test_data,type='response')\nroc2 &lt;- roc(test_data$y,test_pred_probs2) \nplot(roc2,print.auc=TRUE)\n\nAs seen above, the performance of the model built on the oversampled data is only marginally better than on the imbalanced data.The difference is not statistically significant\nroc.test(roc1,roc2)\n## \n##  DeLong's test for two correlated ROC curves\n## \n## data:  roc1 and roc2\n## Z = 0.9998, p-value = 0.3174\n## alternative hypothesis: true difference in AUC is not equal to 0\n## sample estimates:\n## AUC of roc1 AUC of roc2 \n##   0.9362500   0.9348437\n\n\nUndersampled data\nNow we try undersampling the majority class. Given this amounts to throwing away useful information,this is typically done only when the data is really big and the tools/infrastructure being used does not support large data sets.\ntrain_data_0 &lt;- sum(train_data$y==1)\nundersampled_data &lt;- ovun.sample(y~.,data=train_data,method='under',N = 2*train_data_0)$data\nsummary(undersampled_data)\n##        x1                x2           y      \n##  Min.   :0.01163   Min.   :-0.03573   0:160  \n##  1st Qu.:0.37429   1st Qu.: 0.36955   1:160  \n##  Median :0.50291   Median : 0.49148          \n##  Mean   :0.48858   Mean   : 0.50101          \n##  3rd Qu.:0.59854   3rd Qu.: 0.63293          \n##  Max.   :0.91904   Max.   : 1.02417\nA third model is fitted on the undersampled data. Given undersampling reduces the amount of data we are using, the standard error of the parameter estimates increase.\nglm3 &lt;- glm(y ~ ., data = undersampled_data, family = 'binomial')\nsummary(glm3)\n## \n## Call:\n## glm(formula = y ~ ., family = \"binomial\", data = undersampled_data)\n## \n## Deviance Residuals: \n##      Min        1Q    Median        3Q       Max  \n## -2.74148  -0.31754  -0.00045   0.42250   2.28722  \n## \n## Coefficients:\n##             Estimate Std. Error z value Pr(&gt;|z|)    \n## (Intercept)   -5.352      1.177  -4.546 5.46e-06 ***\n## x1            18.465      2.362   7.818 5.35e-15 ***\n## x2            -7.597      1.220  -6.227 4.76e-10 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## (Dispersion parameter for binomial family taken to be 1)\n## \n##     Null deviance: 443.61  on 319  degrees of freedom\n## Residual deviance: 186.63  on 317  degrees of freedom\n## AIC: 192.63\n## \n## Number of Fisher Scoring iterations: 6\nThe plot is shown below with a 95% confidence limits. The confidence interval around the decision boundary has also expanded.\nplot_decision_boundary(glm3,boundary=0.5,data=undersampled_data,title= 'Undersampled Data',min_=0,max_=1)\n\nThe performance of this new model on the test dataset is shown below.\nlibrary(pROC)\ntest_pred_probs3 &lt;- predict(glm3,newdata = test_data,type='response')\nroc3 &lt;- roc(test_data$y,test_pred_probs3) \nplot(roc3,print.auc=TRUE)\n\nAs seen above, the performance of the model built on the undersampled data is again only marginally better than on the imbalanced data.The difference in performance is again not statistically significant\nroc.test(roc1,roc3)\n## \n##  DeLong's test for two correlated ROC curves\n## \n## data:  roc1 and roc3\n## Z = 1.4806, p-value = 0.1387\n## alternative hypothesis: true difference in AUC is not equal to 0\n## sample estimates:\n## AUC of roc1 AUC of roc2 \n##   0.9362500   0.9321875"
  },
  {
    "objectID": "blog/2020-06-14-should-you-rebalance-an-imbalanced-dataset.en/index.html#case-2-when-your-model-is-not-a-good-fit-for-your-data",
    "href": "blog/2020-06-14-should-you-rebalance-an-imbalanced-dataset.en/index.html#case-2-when-your-model-is-not-a-good-fit-for-your-data",
    "title": "Should you rebalance an imbalanced dataset?",
    "section": "Case 2: When your model is not a good fit for your data",
    "text": "Case 2: When your model is not a good fit for your data\nNow consider the case when the model is not a good fit for the data.This refers to the scenario when you chooses a model with high bias.\n\nOriginal Data\nThe data is again partitioned into train and test sets as earlier.\nset.seed(0)\n#class 1\nc1 &lt;- cbind('x1'=c(rnorm(100,0.2,0.1),rnorm(100,0.8,0.1)),\n            'x2'=c(rnorm(100,0.8,0.1),rnorm(100,0.2,0.1)))\n#class2\nc2 &lt;- cbind('x1'=rnorm(800,0.5,0.1),'x2'=rnorm(800,0.5,0.1))\n\noriginal_data2 &lt;- data.frame(rbind(c1,c2))\noriginal_data2$y &lt;- as.factor(c(rep(1,200),rep(0,800)))\n\n#Split to train and test sets\ntrain_index &lt;- sample(c(1:nrow(original_data2)),size = 0.8*nrow(original_data2),replace=FALSE)\ntrain_data2 &lt;- original_data2[train_index,]\ntest_data2 &lt;- original_data2[-train_index,]\nSummary of the datasets are shown below.\nsummary(train_data2)\n##        x1                 x2          y      \n##  Min.   :-0.02239   Min.   :0.02793   0:640  \n##  1st Qu.: 0.40243   1st Qu.:0.40536   1:160  \n##  Median : 0.49068   Median :0.49987          \n##  Mean   : 0.49655   Mean   :0.49993          \n##  3rd Qu.: 0.59524   3rd Qu.:0.59498          \n##  Max.   : 1.06587   Max.   :1.05071\nsummary(test_data2)\n##        x1               x2          y      \n##  Min.   :0.0460   Min.   :0.04276   0:160  \n##  1st Qu.:0.4038   1st Qu.:0.40692   1: 40  \n##  Median :0.5000   Median :0.48628          \n##  Mean   :0.4968   Mean   :0.49700          \n##  3rd Qu.:0.5847   3rd Qu.:0.59146          \n##  Max.   :0.9625   Max.   :0.96148\nA model is fitted on the original data.\nglm4 &lt;- glm(y ~ ., data = train_data2, family = 'binomial')\nsummary(glm4)\n## \n## Call:\n## glm(formula = y ~ ., family = \"binomial\", data = train_data2)\n## \n## Deviance Residuals: \n##     Min       1Q   Median       3Q      Max  \n## -0.7455  -0.6777  -0.6607  -0.6341   1.9051  \n## \n## Coefficients:\n##             Estimate Std. Error z value Pr(&gt;|z|)   \n## (Intercept)  -1.9080     0.6226  -3.065  0.00218 **\n## x1            0.6119     0.6880   0.889  0.37381   \n## x2            0.4319     0.6726   0.642  0.52077   \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## (Dispersion parameter for binomial family taken to be 1)\n## \n##     Null deviance: 800.64  on 799  degrees of freedom\n## Residual deviance: 799.84  on 797  degrees of freedom\n## AIC: 805.84\n## \n## Number of Fisher Scoring iterations: 4\nThe plot is shown below\nplot_decision_boundary(glm4,boundary=0.2,data= original_data2,title= 'Original Data',min_=0,max_=1)\n\nIt can be seen above that the model is mis-specified for the data given and the decision boundary learned is biased.\nThe performance of this new model on the test dataset has predictably deteriorated.\nlibrary(pROC)\ntest_pred_probs4 &lt;- predict(glm4,newdata = test_data2,type='response')\nroc4 &lt;- roc(test_data2$y,test_pred_probs4) \nplot(roc4,print.auc=TRUE)\n\n\n\nOversampled Data\nNow we oversample the minority class.\nset.seed(0)\ntrain_data_0 &lt;- sum(train_data2$y==0)\noversampled_data2 &lt;- ovun.sample(y~.,data=train_data2,method='over',N = 2*train_data_0)$data\nsummary(oversampled_data2)\n##        x1                 x2          y      \n##  Min.   :-0.02239   Min.   :0.02793   0:640  \n##  1st Qu.: 0.32077   1st Qu.:0.32272   1:640  \n##  Median : 0.48765   Median :0.50195          \n##  Mean   : 0.49570   Mean   :0.50277          \n##  3rd Qu.: 0.67683   3rd Qu.:0.66858          \n##  Max.   : 1.06587   Max.   :1.05071\nA second model is fitted on the oversampled data. As expected, the standard error of the parameter estimates has decreased given we are now using more data.\nglm5 &lt;- glm(y ~ ., data = oversampled_data2, family = 'binomial')\nsummary(glm5)\n## \n## Call:\n## glm(formula = y ~ ., family = \"binomial\", data = oversampled_data2)\n## \n## Deviance Residuals: \n##      Min        1Q    Median        3Q       Max  \n## -1.25580  -1.17644   0.01662   1.17775   1.24644  \n## \n## Coefficients:\n##             Estimate Std. Error z value Pr(&gt;|z|)\n## (Intercept)  -0.4073     0.3988  -1.021    0.307\n## x1            0.3969     0.4244   0.935    0.350\n## x2            0.4188     0.4049   1.034    0.301\n## \n## (Dispersion parameter for binomial family taken to be 1)\n## \n##     Null deviance: 1774.5  on 1279  degrees of freedom\n## Residual deviance: 1773.4  on 1277  degrees of freedom\n## AIC: 1779.4\n## \n## Number of Fisher Scoring iterations: 3\nThe plot is shown below. Oversampling does not make any material difference to the decision boundary.\nplot_decision_boundary(glm5,boundary=0.5,data= oversampled_data2,title= 'Oversampled Data',min_=0,max_=1)\n\nThe performance of this new model is shown below.\nlibrary(pROC)\ntest_pred_probs5 &lt;- predict(glm5,newdata = test_data2,type='response')\nroc5 &lt;- roc(test_data2$y,test_pred_probs5) \nplot(roc5,print.auc=TRUE)\n\nThe performance of the model has actually deteriorated after oversampling.\n\n\nUndersampled Data\nNow we undersample the majority class.\nset.seed(0)\ntrain_data_1 &lt;- sum(train_data2$y==1)\nundersampled_data2 &lt;- ovun.sample(y~.,data=train_data2,method='under',N = 2*train_data_1)$data\nsummary(undersampled_data2)\n##        x1                 x2          y      \n##  Min.   :-0.02239   Min.   :0.02793   0:160  \n##  1st Qu.: 0.32501   1st Qu.:0.34263   1:160  \n##  Median : 0.47918   Median :0.49461          \n##  Mean   : 0.49348   Mean   :0.49942          \n##  3rd Qu.: 0.67674   3rd Qu.:0.64952          \n##  Max.   : 1.06587   Max.   :1.05071\nA second model is fitted on the undersampled data. Given the amount of data used has shrunk, the standard error of the parameter estimates is higher than in the original data.\nglm6 &lt;- glm(y ~ ., data = undersampled_data2, family = 'binomial')\nsummary(glm6)\n## \n## Call:\n## glm(formula = y ~ ., family = \"binomial\", data = undersampled_data2)\n## \n## Deviance Residuals: \n##      Min        1Q    Median        3Q       Max  \n## -1.34987  -1.16813   0.01669   1.17236   1.42414  \n## \n## Coefficients:\n##             Estimate Std. Error z value Pr(&gt;|z|)\n## (Intercept)  -1.2904     0.8229  -1.568    0.117\n## x1            1.4033     0.8691   1.615    0.106\n## x2            1.1970     0.8504   1.408    0.159\n## \n## (Dispersion parameter for binomial family taken to be 1)\n## \n##     Null deviance: 443.61  on 319  degrees of freedom\n## Residual deviance: 440.95  on 317  degrees of freedom\n## AIC: 446.95\n## \n## Number of Fisher Scoring iterations: 4\nThe plot is shown below. Undersampling again does not make any material difference to the decision boundary.\nplot_decision_boundary(glm6,boundary=0.5,data= undersampled_data2,title= 'Undersampled Data',min_=0,max_=1)\n\nThe performance of this new model is shown below.\nlibrary(pROC)\ntest_pred_probs6 &lt;- predict(glm6,newdata = test_data2,type='response')\nroc6 &lt;- roc(test_data2$y,test_pred_probs6) \nplot(roc6,print.auc=TRUE)\n\nThe performance of the model built on undersampled data is also worse than that on the original data."
  },
  {
    "objectID": "blog/2020-06-14-should-you-rebalance-an-imbalanced-dataset.en/index.html#conclusion",
    "href": "blog/2020-06-14-should-you-rebalance-an-imbalanced-dataset.en/index.html#conclusion",
    "title": "Should you rebalance an imbalanced dataset?",
    "section": "Conclusion",
    "text": "Conclusion\n\nIf a model is correctly specified,rebalancing the data with undersampling/oversampling may marginally improve the performance of the model but the difference in performance is not statistically significant and unlikely to materially impact business outcomes\nIf the model is incorrectly specified, rebalancing the data does help such a model learn the correct decision boundary and the performance of the resulting model is likely to get worse and almost certainly will not be materially better.\nIf you have an imbalanced dataset, you should probably try to engineer new features or use a different model with lower bias before trying to rebalance the dataset.\nOversampling the dataset can be an effective strategy if your cost of false negatives is much higher than that of false positives, in this case you care more about your recall than your precision, so you would want to overweight your target class by oversampling them."
  },
  {
    "objectID": "blog/2020-06-14-should-you-rebalance-an-imbalanced-dataset.en/index.html#next-steps",
    "href": "blog/2020-06-14-should-you-rebalance-an-imbalanced-dataset.en/index.html#next-steps",
    "title": "Should you rebalance an imbalanced dataset?",
    "section": "Next Steps",
    "text": "Next Steps\nI will expand this analysis to include real world datasets. In several blog posts on imbalanced learning, I have seen that baselines without rebalancing are not presented for a fair comparison. In other cases, the thresholds for the model built on imbalanced data are chosen as 0.5 by default instead of carefully selecting the threshold to maximize an appropriate performance metric. This often results in the performance of some of these rebalancing techniques being exaggerated.\nIn a subsequent blog post or an extension to this one,I will also evaluate some of the commonly used synthetic data generation techniques and try to identify some heuristics around when they might add value."
  },
  {
    "objectID": "blog/2021-01-03-svd-is-almost-all-you-need.en/index.html",
    "href": "blog/2021-01-03-svd-is-almost-all-you-need.en/index.html",
    "title": "SVD is (almost) all you need",
    "section": "",
    "text": "Some twitter threads deserve to be full blown blog posts. A few months back Daniela Witten produced a delightfully informative twitter thread on Singular Value Decomposition while curating the WomeninStat twitter account.\nSee thread here\nAs someone who regretfully never paid much attention to my Linear Algebra lessons, this was a good opportunity to dive in and understand the practical and relevant aspects of SVD. I have taken this twitter thread, added some content and code to help bridge some gaps in my understanding."
  },
  {
    "objectID": "blog/2021-01-03-svd-is-almost-all-you-need.en/index.html#background",
    "href": "blog/2021-01-03-svd-is-almost-all-you-need.en/index.html#background",
    "title": "SVD is (almost) all you need",
    "section": "",
    "text": "Some twitter threads deserve to be full blown blog posts. A few months back Daniela Witten produced a delightfully informative twitter thread on Singular Value Decomposition while curating the WomeninStat twitter account.\nSee thread here\nAs someone who regretfully never paid much attention to my Linear Algebra lessons, this was a good opportunity to dive in and understand the practical and relevant aspects of SVD. I have taken this twitter thread, added some content and code to help bridge some gaps in my understanding."
  },
  {
    "objectID": "blog/2021-01-03-svd-is-almost-all-you-need.en/index.html#introduction",
    "href": "blog/2021-01-03-svd-is-almost-all-you-need.en/index.html#introduction",
    "title": "SVD is (almost) all you need",
    "section": "Introduction",
    "text": "Introduction\nSVD decomposes an \\(n \\times p\\) matrix \\(X\\) into three matrices, \\(U\\) (\\(n \\times p\\)), \\(V\\) (\\(p \\times p\\)) and \\(D\\) (\\(p \\times p\\))\n\\[  X = UDV^T \\]\nEach of these matrices are special\n\nV\n\nThis is the right singular matrix\n\\(V\\) is an orthogonal matrix which implies that \\(V^T V = I_p\\) and \\(VV^T = I_p\\)\nEach row and column of \\(V\\) is an orthogonal unit vector or orthonormal vector implying that the dot product of any two columns of the matrix is 0 and each column has a unit length i.e given two columns of the matrix \\(v_i\\) and \\(v_j\\)\n\n\\[ v_i.v_j = 0 \\] and\n\\[ ||v_i||_2 = ||v_j||_2 = 1 \\]\nRemember that given an n-dimensional vector, \\(v = \\begin{bmatrix} v_1 \\\\ v_2 \\\\ \\vdots \\\\ v_n \\end{bmatrix}\\)\n\\[ ||v||_{2} = \\sqrt{v_1^2 + v_2^2+...+v_n^2}  \\]\nAt the risk of jumping too far ahead, let us verify all the claims made so far\nset.seed(0)\nX &lt;- matrix(rnorm(50,10,5),ncol=5) \nprint(paste0(\"Dimensions of X: \",dim(X)[1],\"x\",dim(X)[2]))# 10x5\n## [1] \"Dimensions of X: 10x5\"\nUDV_t &lt;- svd(X)\nU &lt;- UDV_t$u\nprint(paste0(\"Dimensions of U: \",dim(U)[1],\"x\",dim(U)[2])) #10x5\n## [1] \"Dimensions of U: 10x5\"\nD &lt;- diag(UDV_t$d)\nprint(paste0(\"Dimensions of D: \",dim(D)[1],\"x\",dim(D)[2])) #5x5\n## [1] \"Dimensions of D: 5x5\"\nV &lt;- UDV_t$v\nprint(paste0(\"Dimensions of V: \",dim(V)[1],\"x\",dim(V)[2])) #5x5\n## [1] \"Dimensions of V: 5x5\"\nV\n##            [,1]       [,2]       [,3]       [,4]        [,5]\n## [1,] -0.5305928 -0.5839346  0.4900540 -0.3674478 -0.04817542\n## [2,] -0.3623817  0.2449652 -0.2651247 -0.1078315 -0.85249798\n## [3,] -0.4412964  0.3258640  0.4449880  0.7058240  0.05355521\n## [4,] -0.4608308  0.5588499 -0.1516082 -0.4858374  0.46507923\n## [5,] -0.4243149 -0.4248668 -0.6845120  0.3451506  0.22750709\nAsserting if V is an orthogonal matrix, whether the dot product of any two columns is 0 and whether each column has a unit length\n#V.V^T = I\nVdotVt = V %*% t(V)\nall.equal(VdotVt,diag(5))\n## [1] TRUE\n#V^T.V = I\nVtdotV = t(V) %*% V\nall.equal(VtdotV,diag(5))\n## [1] TRUE\n#Calculate length of the rows and columns\napply(V,1,norm,\"2\")\n## [1] 1 1 1 1 1\napply(V,2,norm,\"2\")\n## [1] 1 1 1 1 1\n\n\nU\nThis is the left singular matrix.\nU\n##             [,1]        [,2]        [,3]        [,4]         [,5]\n##  [1,] -0.4075764 -0.30451400 -0.34042098  0.10207128 -0.555209282\n##  [2,] -0.2841823 -0.04400465 -0.11911239  0.57696247  0.211675152\n##  [3,] -0.3024197 -0.19865882  0.30451706 -0.01914373  0.230178929\n##  [4,] -0.3241650 -0.07720920  0.40096627  0.17648218 -0.427589601\n##  [5,] -0.3003256  0.19558501  0.17636880 -0.38297623 -0.003307577\n##  [6,] -0.2596249  0.55588827 -0.05727637  0.11335624  0.326148076\n##  [7,] -0.2965586  0.58432033  0.15789336  0.12393566 -0.235925477\n##  [8,] -0.2715670 -0.18391382 -0.36662106  0.26229031  0.367754197\n##  [9,] -0.3405978  0.06566051 -0.51198359 -0.56492803  0.012115654\n## [10,] -0.3482504 -0.36479736  0.40070706 -0.24972577  0.337174733\n\\(U\\) is not quite an orthogonal matrix because it is not a square matrix.\ndim(U)\n## [1] 10  5\nHowever\n\\(U^T \\times U = I_n\\)\nall.equal(t(U)%*%U,diag(5))\n## [1] TRUE\nBut \\(U \\times U^T \\neq I\\)\nall.equal(U%*%t(U),diag(10))\n## [1] \"Mean relative difference: 1\"\nCheck if rows and columns are unit vectors\napply(U,1,norm,\"2\")\n##  [1] 0.8327122 0.6888946 0.5263102 0.6969914 0.5533865 0.7063424 0.7247954\n##  [8] 0.6678498 0.8376968 0.7687466\napply(U,2,norm,\"2\")\n## [1] 1 1 1 1 1\nAs you can see the columns of U are orthonormal vectors but the rows are not.\n\n\nD\n\\(D\\) is a diagonal matrix with non-negative and decreasing elements\nD\n##         [,1]     [,2]     [,3]     [,4]     [,5]\n## [1,] 72.6732  0.00000  0.00000 0.000000 0.000000\n## [2,]  0.0000 20.69451  0.00000 0.000000 0.000000\n## [3,]  0.0000  0.00000 17.38002 0.000000 0.000000\n## [4,]  0.0000  0.00000  0.00000 9.614317 0.000000\n## [5,]  0.0000  0.00000  0.00000 0.000000 6.687267\n\\[ D_{11} \\geq  D_{22}... \\geq D_{pp} \\geq 0 \\] The diagonal elements of \\(D\\) are the the singular values of the matrix\nall.equal (X, U%*%D%*%t(V))\n## [1] TRUE\nThe decomposition is unique up to sign flips of U & V.\nall.equal (X, -U%*%D%*%-t(V))\n## [1] TRUE"
  },
  {
    "objectID": "blog/2021-01-03-svd-is-almost-all-you-need.en/index.html#relation-to-pca",
    "href": "blog/2021-01-03-svd-is-almost-all-you-need.en/index.html#relation-to-pca",
    "title": "SVD is (almost) all you need",
    "section": "Relation to PCA",
    "text": "Relation to PCA\nPCA is a dimensionality reduction technique where the goal is to represent a matrix in fewer number of dimensions while preserving as much information as possible.\nLet us obtain the principle component loading vectors of X. Each column in the matrix below is a principle component loading vector.\npca &lt;- prcomp(X)\npca$rotation\n##              PC1        PC2        PC3         PC4        PC5\n## [1,] -0.70508429 -0.4819910  0.4980523  0.03090714 -0.1467294\n## [2,]  0.09849312  0.3072747  0.5821810 -0.65632479  0.3552245\n## [3,]  0.21615712 -0.4067371 -0.2131250 -0.65194755 -0.5633677\n## [4,]  0.39984007  0.2071265  0.5686941  0.33638878 -0.6005466\n## [5,] -0.53531232  0.6818581 -0.2101761 -0.17346325 -0.4174287\nNote each column above gives the weights to be used for multiplying each row of the original matrix X to get principle component scores.\nFor example the , principle component scores for the first observation(row) of X when using only the first two principle components is given by\n#1x5  x  5x2\nt(X[1,]) %*% pca$rotation[,c(1,2)]\n##            PC1      PC2\n## [1,] -14.75421 7.410003\nThe above is a 2D representation of the original 5D observation.\nLet us obtain the above principal component loading vectors using SVD\n\nCompute \\(\\bar{X}\\) by centering each column to have mean = 0\n\nn &lt;- nrow(X)\ncol_means &lt;- colMeans(X)\nX_bar &lt;- sweep(X,2,col_means)\n\nApply SVD to \\(\\bar{X}\\)\n\nsvd_X_bar &lt;- svd(X_bar)\nThe principle component loading vectors are given by V\nsvd_X_bar$v\n##             [,1]       [,2]       [,3]        [,4]       [,5]\n## [1,] -0.70508429 -0.4819910  0.4980523  0.03090714 -0.1467294\n## [2,]  0.09849312  0.3072747  0.5821810 -0.65632479  0.3552245\n## [3,]  0.21615712 -0.4067371 -0.2131250 -0.65194755 -0.5633677\n## [4,]  0.39984007  0.2071265  0.5686941  0.33638878 -0.6005466\n## [5,] -0.53531232  0.6818581 -0.2101761 -0.17346325 -0.4174287\nThe principle component score vectors are given by U upto a scaling factor.\n(u &lt;- svd_X_bar$u)\n##              [,1]        [,2]        [,3]       [,4]        [,5]\n##  [1,] -0.40654628  0.34920041  0.25956018 -0.5749824 -0.20816929\n##  [2,] -0.02935843  0.10830223 -0.55805273 -0.1681078 -0.16348723\n##  [3,] -0.18548785 -0.31514365 -0.11523416  0.2170557  0.08128469\n##  [4,] -0.09518384 -0.39695879  0.01766182 -0.3793774  0.50884622\n##  [5,]  0.17275809 -0.16211678  0.29927195  0.2484149  0.35727053\n##  [6,]  0.54750382  0.07810152 -0.13824614  0.1144150 -0.34390479\n##  [7,]  0.53884857 -0.12301901  0.13267006 -0.3368109 -0.11048795\n##  [8,] -0.15454447  0.34394972 -0.49807958  0.2415545  0.30998781\n##  [9,] -0.00521322  0.52611956  0.46298986  0.3254301  0.11384933\n## [10,] -0.38277639 -0.40843521  0.13745871  0.3124084 -0.54518930\nThe true principle component score vectors are given by\n##########10x5 X 5x5\n(pc &lt;- (X_bar)%*% svd_X_bar$v)\n##             [,1]      [,2]       [,3]       [,4]       [,5]\n##  [1,] -8.7018408  6.079596  2.7817904 -4.8650796 -0.5439899\n##  [2,] -0.6283967  1.885547 -5.9808315 -1.4224054 -0.4272263\n##  [3,] -3.9702386 -5.486666 -1.2350017  1.8365659  0.2124139\n##  [4,] -2.0373440 -6.911072  0.1892875 -3.2100133  1.3297216\n##  [5,]  3.6977669 -2.822461  3.2073942  2.1019051  0.9336226\n##  [6,] 11.7189391  1.359751 -1.4816286  0.9680956 -0.8986952\n##  [7,] 11.5336795 -2.141767  1.4218680 -2.8498470 -0.2887281\n##  [8,] -3.3079171  5.988181 -5.3380799  2.0438570  0.8100630\n##  [9,] -0.1115854  9.159767  4.9620121  2.7535512  0.2975121\n## [10,] -8.1930629 -7.110877  1.4731895  2.6433706 -1.4246937\nThe scaling factor for each column is given by\n(pc/u)[1,]\n## [1] 21.404306 17.410048 10.717323  8.461267  2.613209\nThe scaling factors are in fact given by the singular values.\nsvd_X_bar$d\n## [1] 21.404306 17.410048 10.717323  8.461267  2.613209\nThis is in fact how PCA is implemented in the prcomp function in R"
  },
  {
    "objectID": "blog/2021-01-03-svd-is-almost-all-you-need.en/index.html#relation-to-eigen-value-decomposition",
    "href": "blog/2021-01-03-svd-is-almost-all-you-need.en/index.html#relation-to-eigen-value-decomposition",
    "title": "SVD is (almost) all you need",
    "section": "Relation to Eigen Value Decomposition",
    "text": "Relation to Eigen Value Decomposition\nEigen decomposition decomposes a matrix \\(X\\) as follows\n\\[ X = P\\Sigma P^{-1} \\] Alternately\n\\[ XP = P\\Sigma \\] where \\(P\\) is a matrix of eigen vectors and \\(\\Sigma\\)is a diagonal matrix of eigen values.\nWe have already defined the SVD of X (\\(m \\times p\\)) as shown below\n\\[  X = UDV^T \\] Let the rank of X be given by \\(r\\)\n\\[ rank(X) = r \\leq min(m,p) \\]\n\\(U\\), \\(D\\) and \\(V\\) can be expressed as block matrices as shown below\n\\[ U = \\begin{bmatrix} U_1 && U_2 \\end{bmatrix} \\] where: \\(U_1\\) is an \\(m \\times r\\) matrix  \\(U_2\\) is an \\(m \\times (p-r)\\) matrix  \\(U_1^TU_1 = I_r\\)\n\\[ D = \\begin{bmatrix}  D_r &   \\textbf{0} \\\\\\\\ \\textbf{0} & \\textbf{0}    \\end{bmatrix} \\]\nwhere: \\(D_r\\) is an \\(r \\times r\\) matrix \n\\(\\textbf{0}\\) is an \\((p-r) \\times (p-r)\\) matrix\n\\[ V^T = \\begin{bmatrix} V_1^T && V_2^T \\end{bmatrix} \\]\nwhere: \\(V_1^T\\) is an \\(r \\times p\\) matrix\n\\(V_2\\) is an \\(r \\times (p-r)\\) matrix\n\\(V_1^TV_1 = I_r\\)\n\\[ X  =  \\begin{bmatrix} U_1 && U_2 \\end{bmatrix} \\begin{bmatrix}  D_r &   \\textbf{0} \\\\\\\\ \\textbf{0} & \\textbf{0}    \\end{bmatrix} \\begin{bmatrix} V_1^T && V_2^T \\end{bmatrix}   = U_1D_rV_1^T  \\] This means\n\\[ XX^T = U_1D_rV_1^T (U_1D_rV_1^T )^T \\] \\[ = U_1D_rV_1^T V1D_rU_1^T   = U_1D_r^2U_1^T \\] Multiplying both sides by \\(U_1\\) gives\n\\[ (XX^T)U_1 = U_1D_r^2U_1^TU_1 = U_1D_r^2 \\] This is the classic definition of the eigen vectors and eigen values of a matrix. \\(U_1\\) i.e. the left singular matrix gives the eigen vectors while \\(D_r^2\\) i.e. the square of the singular values gives the eigen values of \\(XX^T\\)\nSimilarly, \\[ X^TX =  (U_1D_rV_1^T )^T U_1D_rV_1^T = V_1D_rU_1^TU_1D_rV_1^T \\]\n\\[ = V_1D_r^2V_1^T \\]\nMultiplying both sides by \\(V_1\\) gives\n\\[ (X^TX)V_1 = V_1D_r^2V_1^TV_1 = V_1D_r^2 \\]\nThus, the right singular matrix \\(V_1\\) gives the eigen vectors for the matrix \\(X^TX\\),while \\(D_r^2\\) i.e. the square of the singular values gives the eigen values of \\(X^TX\\)"
  },
  {
    "objectID": "blog/2021-01-03-svd-is-almost-all-you-need.en/index.html#computing-svd-power-method",
    "href": "blog/2021-01-03-svd-is-almost-all-you-need.en/index.html#computing-svd-power-method",
    "title": "SVD is (almost) all you need",
    "section": "Computing SVD :Power Method",
    "text": "Computing SVD :Power Method\nBelow is the power method, a simple algorithm for computing SVD of a matrix \\(A \\in \\mathbb{R}^{m\\times n}\\).\nIt first computes the first singular value \\(\\sigma_1\\) and left and right singular vectors \\(u_1\\) and \\(v_1\\) of A, for which:\n\\(min_{i&lt;j} log(\\frac{\\sigma_i}{\\sigma_j}) \\geq \\lambda\\) where \\(\\lambda\\) is a tunable threshold.\n\nGenerate \\(x_0\\) such that \\(x_0(i) \\sim N(0,1)\\)\nDetermine no of of iterations (\\(s\\)) required to get \\(\\epsilon\\) precision with probability of at least \\(1 -\\delta\\)\n\n\\(s \\leftarrow \\log\\left(4\\log\\left(\\frac{2n}{\\delta}\\right)/\\epsilon\\delta\\right)/2\\lambda\\)\nNote: I did not find step 2 and 3 to be useful in practice. It gave inaccurate SVD results. In practice, as explained in the video below, your estimate gets better as \\(s \\to \\infty\\). In the below implementation, I keep iterating as long as the vector stops changing by more than \\(\\epsilon\\).\n\nFor \\(i\\) in \\([1, \\ldots, s]\\):\n\n\\(x_i \\leftarrow A^T A x_{i-1}\\)\n\\(x_i \\leftarrow \\frac{x_i}{\\|x_i\\|}\\)\n\n\\(v_1 \\leftarrow x_i\\)\n\\(\\sigma_1 \\leftarrow \\|A v_1\\|\\)\n\\(u_1 \\leftarrow \\frac{A v_1}{\\sigma_1}\\)\nReturn (\\(\\sigma_1, u_1, v_1\\))\n\nGiven we have computed (\\(\\sigma_1, u_1, v_1\\)), we can repeat the procedure for: \\[\nA - \\sigma_1 u_1 v_1^T = \\sum_{i=2}^n \\sigma_i u_i v_i^T\n\\] The top singular values and vectors of this will be (\\(\\sigma_2, u_2, v_2\\)).\nTHis method converges only if the below two assumptions hold.\n\n\\(A^TA\\) has an eigen value strictly greater in magnitude than its other eigen values\nThe starting vector \\(x_0\\) has a non-zero component in the direction of an eigen vector associated with the dominant eigen value.\n\nBelow is an implementation of the algorithm.\npower_svd &lt;- function(A,eps){\n  #Function to calculate the first singular value\n  #A: Matrix to be decomposed\n  #eps: if change between successive iterations is lesser than this threshold value, stop\nx &lt;- rnorm(dim(A)[2])\nn &lt;- dim(A)[2]\n#eps &lt;- 0.01 #precision\n#delta &lt;- 0.001 #1 - prob\n#lambda &lt;- 2\n\n#s &lt;- ceiling(log(4*log(2*n/delta)/(eps*delta))/(2*lambda))\n\nA_ &lt;- t(A)%*%A\nx &lt;- A_%*%x\nx_1 &lt;- x &lt;- x/norm(x,type=\"2\")\nwhile (TRUE){\n  x &lt;- A_%*%x \n  x_2&lt;- x &lt;- x/norm(x,type=\"2\")\n  if(max(abs(x_2-x_1))&lt;=eps){break}\n  x_1 &lt;- x_2\n  }\n\n\nv1 &lt;- x\nsigma1 &lt;- norm(A%*%v1,type=\"2\")\nu1 &lt;- A%*%v1/sigma1\nreturn(list('u'=u1,'sigma'=sigma1,'v'=v1))\n\n}\nlibrary(Matrix)\n\nSVD_fn &lt;- function(A){\n  rank &lt;- rankMatrix(A)[1]\n  udv_list &lt;- list()\n  for (i in 1:rank){\n    \n    udv &lt;- power_svd(A,eps=1e-6)\n    udv_list[[i]] &lt;- udv\n    A &lt;- A - udv$sigma*udv$u%*%t(udv$v)\n  }\n  \n  \n  u &lt;-  mapply(cbind,lapply(udv_list,function(x){x$u}))\n  d &lt;-  mapply(c,lapply(udv_list,function(x){x$sigma}))\n  v &lt;-  mapply(cbind,lapply(udv_list,function(x){x$v}))\n  return(list('u'=u,'d'=d,'v'=v))\n}\nThe results of the algorithm are consistent with expectations.\nSVD_fn(X)\n## $u\n##            [,1]        [,2]        [,3]        [,4]         [,5]\n##  [1,] 0.4075764  0.30451330 -0.34042189  0.10207090  0.555209384\n##  [2,] 0.2841823  0.04400440 -0.11911259  0.57696253 -0.211674576\n##  [3,] 0.3024197  0.19865946  0.30451647 -0.01914351 -0.230178948\n##  [4,] 0.3241650  0.07721004  0.40096602  0.17648211  0.427589777\n##  [5,] 0.3003256 -0.19558464  0.17636941 -0.38297617  0.003307195\n##  [6,] 0.2596249 -0.55588839 -0.05727474  0.11335637 -0.326147963\n##  [7,] 0.2965586 -0.58431999  0.15789507  0.12393560  0.235925601\n##  [8,] 0.2715670  0.18391306 -0.36662164  0.26229036 -0.367753935\n##  [9,] 0.3405978 -0.06566158 -0.51198335 -0.56492820 -0.012116219\n## [10,] 0.3482504  0.36479820  0.40070600 -0.24972547 -0.337174982\n## \n## $d\n## [1] 72.673196 20.694512 17.380019  9.614317  6.687267\n## \n## $v\n##           [,1]       [,2]       [,3]       [,4]        [,5]\n## [1,] 0.5305928  0.5839358  0.4900526 -0.3674478  0.04817517\n## [2,] 0.3623817 -0.2449658 -0.2651241 -0.1078322  0.85249790\n## [3,] 0.4412964 -0.3258629  0.4449887  0.7058242 -0.05355472\n## [4,] 0.4608308 -0.5588503 -0.1516067 -0.4858371 -0.46507957\n## [5,] 0.4243149  0.4248651 -0.6845131  0.3451506 -0.22750685\nThis video provides a good theoretical explanation and intuition about how this method works."
  },
  {
    "objectID": "blog/2021-01-03-svd-is-almost-all-you-need.en/index.html#applications",
    "href": "blog/2021-01-03-svd-is-almost-all-you-need.en/index.html#applications",
    "title": "SVD is (almost) all you need",
    "section": "Applications",
    "text": "Applications\n\nMatrix Compression or Approximation\nIf we want to approximate the entire matrix X with just two vectors (a rank 1 approximation), the optimal representation in terms of residual sum of squares is given by the first columns of U and V\n\\[ X \\approx D_{11}U_1V_1^T \\]\nD[1,1]*U[,1]%*%t(V[,1])\n##           [,1]      [,2]      [,3]      [,4]      [,5]\n##  [1,] 15.71609 10.733703 13.071146 13.649753 12.568156\n##  [2,] 10.95803  7.484065  9.113846  9.517278  8.763136\n##  [3,] 11.66126  7.964354  9.698726 10.128049  9.325510\n##  [4,] 12.49976  8.537027 10.396107 10.856301  9.996055\n##  [5,] 11.58052  7.909206  9.631568 10.057919  9.260936\n##  [6,] 10.01110  6.837335  8.326280  8.694850  8.005877\n##  [7,] 11.43526  7.810000  9.510759  9.931762  9.144776\n##  [8,] 10.47159  7.151837  8.709269  9.094793  8.374128\n##  [9,] 13.13341  8.969794 10.923117 11.406639 10.502785\n## [10,] 13.42849  9.171327 11.168537 11.662923 10.738761\nSimilarly a representation of the matrix using any number of vectors (1,2..p) can be computed as\n\\[  X \\approx D_{11}U_1V_1^T + D_{22}U_2V_2^T+...+D_{pp}U_pV_p^T \\]\nThe below figure shows how the quality of the compressed representation improves as we use 1,2,…p vectors\nquality &lt;- function(X,Y){sqrt(sum((X-Y)^2))} \nq &lt;- rep(NA,5)\nrepr &lt;- matrix(0,10,5)\nfor (i in 1:5){\n  repr &lt;- repr +  D[i,i]*U[,i]%*%t(V[,i])\n  q[i] &lt;- quality(X,repr)\n}\n\nplot(q,type='b',xlab = \"Rank of approximation\",ylab=\"Error\")"
  },
  {
    "objectID": "blog/2021-01-03-svd-is-almost-all-you-need.en/index.html#impute-missing-values",
    "href": "blog/2021-01-03-svd-is-almost-all-you-need.en/index.html#impute-missing-values",
    "title": "SVD is (almost) all you need",
    "section": "Impute Missing Values",
    "text": "Impute Missing Values\nAssume 1/5 of the values in the matrix are randomly missing.\n#Identify random missing elements in the matrix\nset.seed(999)\nmissing&lt;- matrix(sample(c(rep(FALSE,40),rep(TRUE,10))),nr=5)\nX_missing &lt;- X\nX_missing[missing] &lt;- NA\nX_missing\n##            [,1]      [,2]      [,3]      [,4]      [,5]\n##  [1,] 16.314771 13.817967  8.878661  8.821467 18.789515\n##  [2,]  8.368833  6.004954        NA  7.285559        NA\n##  [3,] 16.648996        NA 10.666682        NA        NA\n##  [4,]        NA  8.552692 14.020948  6.752642  5.839784\n##  [5,] 12.073207  8.503924  9.714466        NA  4.167147\n##  [6,]  2.300250        NA        NA        NA  4.672047\n##  [7,]  5.357165 11.261117 15.428847 14.960802  2.181090\n##  [8,]  8.526398  5.540394  6.545231  7.852434 15.782685\n##  [9,]  9.971164 12.178416  3.577003 16.191521 14.160236\n## [10,] 22.023267  3.812308 10.233631  8.603269  8.863357\nThe following algorithm can be used to impute the missing values\n\nFill in the missing elements with the column mean\nCompute SVD to get the rank-K approximation of the filled out matrix (Say K = 3)\nReplace missing values with values from rank-K approximation\nGo to step 2) until imputed values stop changing\n\n#Calculate column means\nX_col_means &lt;- apply(X_missing,2,mean,na.rm=TRUE)\n\n#loop through each column and replace missing values with mean of the column\n\nX_imputed &lt;- X_missing\nfor (i in c(1:ncol(X_imputed))){\n  v &lt;- X_imputed[,i]\n  v[is.na(v)] &lt;- col_means[i]\n  X_imputed[,i] &lt;- v\n}\nThe baseline will be the above matrix where the missing values were replaced by the mean. The quality of this imputation is given by\n(baseline &lt;- quality(X[missing],X_imputed[missing]))\n## [1] 9.958537\nCompute rank-3 approximation using SVD\nsvd_X &lt;- svd(X_imputed)\nU &lt;- svd_X$u\nD &lt;- diag(svd_X$d)\nV &lt;- svd_X$v\n\nrepr &lt;- matrix(0,10,5)\nfor (i in 1:3){\n  repr &lt;- repr +  D[i,i]*U[,i]%*%t(V[,i])\n}\nCompare imputed values with original values\nX[missing]\n##  [1] 16.362147  4.261715  7.942446 11.886978 12.518040  7.833448 13.633754\n##  [8] 15.759559 12.803730  7.736080\nrepr[missing]\n##  [1] 12.597238  8.272711  8.969667  8.083139  9.459792  9.914919  9.686136\n##  [8] 11.014724  7.877740 10.003734\nThe quality of applying one round of SVD is given by:\nquality(X[missing],repr[missing])\n## [1] 11.26842\nThe above algorithm have been wrapped up into a function below.\nrank_k_approx &lt;- function (X,k){\n  #X: matrix to be approximate\n  #k: rank of approximation\n  \n  svd_X &lt;- svd(X)\n  U &lt;- svd_X$u\n  D &lt;- diag(svd_X$d)\n  V &lt;- svd_X$v\n  \n  repr &lt;- matrix(0,nrow(X),ncol(X))\n  for (i in 1:k){\n    repr &lt;- repr +  D[i,i]*U[,i]%*%t(V[,i])\n  }\n  \n  return(repr)\n      }\n\n\nimpute_with_SVD &lt;- function(X_missing,k){\n  # X_missing: Matrix with missing values\n  # k: rank of approximation to be used\n  \n  #Calculate column means\n  X_col_means &lt;- apply(X_missing,2,mean,na.rm =TRUE)\n  \n  #identify missing elements\n  missing &lt;- is.na(X_missing)\n  \n  \n  #loop through each column and replace missing values with mean of the column\n  \n  for (i in c(1:ncol(X))){\n    v &lt;- X_missing[,i]\n    v[is.na(v)] &lt;- X_col_means[i]\n    X_missing[,i] &lt;- v\n  }\n  \n  \n  #Center the values on 0\n  #X_missing &lt;- sweep(X_missing,2,X_col_means)\n  \n  \n  \n  #Record newly imputed values in a vector\n  imputed &lt;- X_missing[missing]\n  \n  j&lt;- 1\n  \n  #record imputed elements in each iteration\n  imputed_list &lt;- list()\n  imputed_list[[j]] &lt;- imputed\n  \n  while(TRUE){\n  ##Compute rank-k SVD approximation\n  X_approx &lt;- rank_k_approx(X_missing,k)\n  \n  #Replace missing values with new approximation\n  X_missing[missing] &lt;- X_approx[missing]\n  \n  # break out of loop if imputed values stop changing\n  if (max(abs(imputed-X_approx[missing])) &lt; 0.001 ) {break}\n  j &lt;- j + 1\n  imputed &lt;- X_approx[missing]\n  imputed_list[[j]] &lt;- imputed\n  }\n  \n  return(list(imputed_list,X_missing))\n  \n   }\nThe missing values are now imputed using a rank 3 approximation\nans &lt;-impute_with_SVD(X_missing,3)\nThe below figure shows how the imputed values change every 100 steps\nimputed_matrix &lt;- do.call(rbind,ans[[1]])\nindex &lt;- c(1,seq(10,nrow(imputed_matrix),by=100),nrow(imputed_matrix))\nimputed_matrix &lt;- imputed_matrix[index,]\n\nmatplot(imputed_matrix,type='l')\n\nThe final imputed values and original values are shown below.\nX[missing] #Original values\n##  [1] 16.362147  4.261715  7.942446 11.886978 12.518040  7.833448 13.633754\n##  [8] 15.759559 12.803730  7.736080\nans[[2]][missing]#Imputed values\n##  [1] 22.763462  8.766332  9.275372  8.354845  7.649936  9.544315  9.144079\n##  [8] 11.624984  2.671137  9.649021\nThe quality of the imputation is given by\nquality(X[missing],ans[[2]][missing])\n## [1] 15.67569\nRank-3 approximation was used above and the quality of the imputed values were inferior to using simple column means. Let us also evaluate the quality of imputation when using approximations of rank 1 2 and 4.\nFirst,carry out the imputations with the other rank-approximations.\nimputed_list_k &lt;- list()\nimputed_list_k[[3]] &lt;- ans\nfor (i in c(1,2,4)){  \n  imputed_list_k[[i]] &lt;-impute_with_SVD(X_missing,i)\n}\nAnd evaluate their quality.\nlibrary(knitr)\nquality_df &lt;- data.frame('Method'=c('Column Means','Rank 1 Approx','Rank 2 Approx','Rank 3 Approx','Rank 4 Approx'),'RMSE' = baseline)\n\nfor( i in 1:4){\n  quality_df[i+1,'RMSE'] &lt;- quality(X[missing],imputed_list_k[[i]][[2]][missing])\n}\nkable(quality_df)\n\n\n\nMethod\nRMSE\n\n\n\n\nColumn Means\n9.958537\n\n\nRank 1 Approx\n22.739031\n\n\nRank 2 Approx\n29.282360\n\n\nRank 3 Approx\n15.675690\n\n\nRank 4 Approx\n15.632698\n\n\n\nFrom the experiments for far, it does not seem SVD is a very good approach for imputing missing values. Replacing missing values with a simple column average seems to be a more effective way to impute missing values.\nAlso note that if your original data has only positive values, SVD is probably not a good fit as it gives negative values. Non-negative matrix factorization is a better choice in this case.\n\nCheck for multi-collinearity\nTo check for multi-collinearity in an \\(m \\times p\\) matrix (\\(p \\leq m\\)), check the ratio of the first singular value to the last singular value \\(\\frac{D_{11}}{D_{pp}}\\), if this is very large then using least squares regression is a bad idea.\nFirst check this ratio for a random matrix.\nsvd_X &lt;- svd(X)\nsvd_X$d[1]/svd_X$d[5]\n## [1] 10.8674\nLet us add some multi-collinearity to X by adding a column that is equal to the sum of the first 3 columns and find the same ratio\nX2&lt;- cbind(X,X[,1]+X[,2]+X[,3])\nsvd_X2 &lt;- svd(X2)\nsvd_X2$d[1]/svd_X2$d[6]\n## [1] 1.191001e+16\n\n\nMoore-Penrose Pseudo Inverse\nThe pseudo inverse can be computed by simply using the expression below\n\\[  X^{-1} \\approx D_{11}^{-1}U_1V_1^T + D_{22}^{-1}U_2V_2^T+...+D_{pp}^{-1}U_pV_p^T \\]\ninverse &lt;- function(X){\n  # X must be a square matrix to compute the inverse\n  \n  stopifnot(\"Matrix must be square\"=nrow(X)==ncol(X))\n  \n  \n  svd_X &lt;- svd(X)\n  U &lt;- svd_X$u\n  V &lt;- svd_X$v\n  D &lt;- svd_X$d\n  nr &lt;- nc &lt;- nrow(X)\n  inv &lt;- matrix(0,nr,nc)\n  for (i in 1:nc){\n  inv &lt;- inv +  1/D[i]*(U[,i]%*%t(V[,i]))\n  }\n  \nreturn(inv)\n}\ncov_mat &lt;- cov(X)\nround(cov_mat%*%inverse(cov_mat),3)\n##      [,1] [,2] [,3] [,4] [,5]\n## [1,]    1    0    0    0    0\n## [2,]    0    1    0    0    0\n## [3,]    0    0    1    0    0\n## [4,]    0    0    0    1    0\n## [5,]    0    0    0    0    1\n\n\nRank of a Matrix\nRank of a matrix is just the number of non-zero singular values.\nThe rank of X (10 x 5 matrix) used in this blog is\nsum(D&gt;0)\n## [1] 5\nThe rank of X2 (10 x 6) matrix used above is also:\nsum(svd_X2$d&gt;0.0001) # using a value slightly greater than 0\n## [1] 5"
  },
  {
    "objectID": "blog/2021-01-03-svd-is-almost-all-you-need.en/index.html#references-and-additional-resources",
    "href": "blog/2021-01-03-svd-is-almost-all-you-need.en/index.html#references-and-additional-resources",
    "title": "SVD is (almost) all you need",
    "section": "References and Additional Resources",
    "text": "References and Additional Resources\n\nhttps://aaronschlegel.me/principal-component-analysis-r-example.html \nhttps://www.youtube.com/watch?v=3_45TkPVjpU \nhttps://www.cs.yale.edu/homes/el327/datamining2013aFiles/07_singular_value_decomposition.pdf \nhttps://www.youtube.com/playlist?list=PLtmWHNX-gukIc92m1K0P6bIOnZb-mg0hY"
  },
  {
    "objectID": "blog/2021-06-19-solving-tic-tac-toe-with-minimax.en/index.html",
    "href": "blog/2021-06-19-solving-tic-tac-toe-with-minimax.en/index.html",
    "title": "Solving Tic-Tac-Toe with Minimax",
    "section": "",
    "text": "I love Tic-Tac-Toe. Besides bringing back fond childhood memories, it offers a simple playground for a noob like me to learn more about Computer Science, Algorithms and AI. In this blog post , I will use the Mini-Max algorithm to solve the game of Tic Tac Toe.\nTo solve the game means, we will be able to discover a strategy that ties the game against an optimal opponent and wins against any non-optimal opponent."
  },
  {
    "objectID": "blog/2021-06-19-solving-tic-tac-toe-with-minimax.en/index.html#introduction",
    "href": "blog/2021-06-19-solving-tic-tac-toe-with-minimax.en/index.html#introduction",
    "title": "Solving Tic-Tac-Toe with Minimax",
    "section": "",
    "text": "I love Tic-Tac-Toe. Besides bringing back fond childhood memories, it offers a simple playground for a noob like me to learn more about Computer Science, Algorithms and AI. In this blog post , I will use the Mini-Max algorithm to solve the game of Tic Tac Toe.\nTo solve the game means, we will be able to discover a strategy that ties the game against an optimal opponent and wins against any non-optimal opponent."
  },
  {
    "objectID": "blog/2021-06-19-solving-tic-tac-toe-with-minimax.en/index.html#the-game",
    "href": "blog/2021-06-19-solving-tic-tac-toe-with-minimax.en/index.html#the-game",
    "title": "Solving Tic-Tac-Toe with Minimax",
    "section": "The Game",
    "text": "The Game\nIf you are from Mars, and do not know how the game works, here are the rules courtesy of Wikipedia\n\n“Tic-tac-toe (American English), noughts and crosses (Commonwealth English and British English), or Xs and Os/“X’y O’sies” (Ireland), is a paper-and-pencil game for two players, X and O, who take turns marking the spaces in a 3×3 grid. The player who succeeds in placing three of their marks in a diagonal, horizontal, or vertical row is the winner. ”"
  },
  {
    "objectID": "blog/2021-06-19-solving-tic-tac-toe-with-minimax.en/index.html#the-minimax-algorithm",
    "href": "blog/2021-06-19-solving-tic-tac-toe-with-minimax.en/index.html#the-minimax-algorithm",
    "title": "Solving Tic-Tac-Toe with Minimax",
    "section": "The Minimax Algorithm",
    "text": "The Minimax Algorithm\nThere are already plenty of great resources to learn about the minimax algorithm like this one which I will liberally borrow from. I only briefly explain the algorithm here.\nThe Mini-Max algorithm is perfect for 2-player (X vs Y) games like Tic-Tac-Toe where player X is trying to maximize his chances of winning while player Y is trying to minimize X’s chances of winning.\nConsider a small imaginary game where each player can make one of two moves (Left or Right). Player X moves first followed by player Y at which point the score of the game is known.\n\nThe score of the game ranges from 1-4. Player X wants to maximize this score while player Y tries to minimize it.To determine what is the optimal action player X should take , we should work backward from the end of the game.\nA,B,C,D,E,F and G are various states of the game where the latter 4 are terminal states.\n\nMove 2\nWhen making Move 2, Player Y can be in states B or C.\nIn State B, Player Y can choose L to yield a score of 4 or R to yield score of E. Given, he wants to minimize the overall score , he will choose R.\nSimilarly in State C, Player Y will choose R to minimize overall score.\nThe value of States B and C are now 3 and 1 respectively.\n\n\nMove 1\nFor the first move, Player X is in state A. He can choose L which lands him in state B with a score of 3 or choose R which lands him in state C with a score of 1.\nGiven he wants to maximize the score, he chooses L.\n\n\nAlgorithm\nThis suggests an algorithm to choose the optimal move from any given state for a maximizing player.\nAt any given state, enumerate the possible child states. Now determine the value of these child states by invoking the minimizing player i.e. Ask the minimizing player what he would do in each of these states and return the value he gets. Choose the action that leads to the highest value state. This is implemented in the maximize function below.\nSimilarly for for a minimizing player, enumerate the child states and determine the value of each state by invoking the maximizing player. Choose the action that leads to the lowest value state. This is implemented in the minimize function below.\nIf at any stage, the child state is a terminal state, the value of the terminal state is simply returned.\nLet us use code to solve the simple toy problem above.\nimport numpy as np\nchildren = {'a':['b','c'],'b':['d','e'],'c':['f','g']} #mapping from state to child states\nvalue = {'d':4,'e':3,'f':2,'g':1} # value of terminal states\naction = {'d':'L','e':'R','f':'L','g':'R','b':'L','c':'R'} # mapping from state to the action that produces #that state\nWe will also keep a cache of the optimal next state for both maximizing and minimizing players, to make the computation a little faster. This hardly matters for this toy problem but can help for larger problems.\nmax_optimal_next_states = {}\nmin_optimal_next_states = {}\nWe define three functions defined below to implement the algorithm.\ndef produce_children(state):\n    \"Function to produce children of a state\"\n    return children.get(state,None)\n\ndef is_terminal(state):\n    \"Function to check if a state is terminal and return value of the state\"\n    if state in children:\n        return False,0\n    return True,value[state]\n\ndef get_action(next_state,current_state=None): \n    \"Function to return action that moves player from current state to next state\"\n     #current state is redundant for this example as there is only one way to get to a state\n    return action[next_state]\nNow we define the key maximizing and minimizing functions that implements the core logic of the algorithm.\nimport random\ndef maximize(state):\n    \n    if state in max_optimal_next_states:\n        return max_optimal_next_states[state]\n    \n    terminal_status,reward = is_terminal(state)\n    if terminal_status:\n        return state,reward # No further state so return same state\n    \n    max_state, max_score = None,-np.Inf\n    max_states = []\n    children = produce_children(state)\n    for child in children:\n        _,score = minimize(child)\n        if score &gt; max_score:\n            max_state,max_score = child,score\n            max_states = [max_state]\n        elif score == max_score: \n            max_states.append(child)\n            \n    # If multiple actions are optimal, break ties randomly\n    max_state = random.choice(max_states)\n    max_optimal_next_states[state] = (max_state,max_score)\n    \n    return max_state,max_score\ndef minimize(state):\n    \n    if state in min_optimal_next_states:\n        return min_optimal_next_states[state]\n    \n    terminal_status,reward = is_terminal(state)\n    if terminal_status:\n        return state,reward # No further state so return same state\n    \n    min_state, min_score = None,np.Inf\n    min_states = []\n    children = produce_children(state)\n    for child in children:\n        _,score = maximize(child)\n        if score &lt; min_score:\n            min_state,min_score = child,score\n            min_states = [min_state]\n        elif score == min_score: \n            min_states.append(child)\n    \n    min_state = random.choice(min_states)\n    min_optimal_next_states[state] =  (min_state,min_score)\n    \n    return min_state,min_score\n\ndef optimal_decision(state,player = 'Maximizer'):\n    if player == 'Maximizer':\n        max_state,_ = maximize(state)\n        return get_action(max_state,state)\n    else:\n        min_state,_ = minimize(state)\n        return get_action(min_state,state)\nLet us confirm this returns the expected decisions in the image above.\noptimal_decision('a',player = 'Maximizer')\n## 'L'\noptimal_decision('b',player = 'Minimizer')\n## 'R'\noptimal_decision('c',player = 'Minimizer')\n## 'R'\nNow what if the roles were reverse with Player X being a minimizer and Player Y being a maximizer. We expect the decisions to flip.\noptimal_decision('a',player = 'Minimizer')\n## 'R'\noptimal_decision('b',player = 'Maximizer')\n## 'L'\noptimal_decision('c',player = 'Maximizer')\n## 'L'"
  },
  {
    "objectID": "blog/2021-06-19-solving-tic-tac-toe-with-minimax.en/index.html#minimax-in-limited-tic-tac-toe",
    "href": "blog/2021-06-19-solving-tic-tac-toe-with-minimax.en/index.html#minimax-in-limited-tic-tac-toe",
    "title": "Solving Tic-Tac-Toe with Minimax",
    "section": "Minimax in Limited Tic Tac Toe",
    "text": "Minimax in Limited Tic Tac Toe\nThe toy problem above can be easily mapped to a limited tic-tac-toe game as shown in the image below.\n\nHere, of the three possible moves available to player X in the starting state, the move to place ‘X’ in the center square is the only action that leads to a winning outcome. This can be figured out by recursively applying the minimax algorithm as illustrated above.\nNote that Player X makes Move 1, Player Y makes Move 2 and Player X makes Move 3. This mini-game can end as early as after Move 1."
  },
  {
    "objectID": "blog/2021-06-19-solving-tic-tac-toe-with-minimax.en/index.html#solving-tictactoe-with-minimax",
    "href": "blog/2021-06-19-solving-tic-tac-toe-with-minimax.en/index.html#solving-tictactoe-with-minimax",
    "title": "Solving Tic-Tac-Toe with Minimax",
    "section": "Solving TicTacToe with Minimax",
    "text": "Solving TicTacToe with Minimax\nLet us denote player ‘X’ who plays first using 1 and player ‘O’ who plays second using 2. An empty square will be represented with 0.\ns_to_b = {0:'__',1:'X',2:'O'} \nWe will denote the state of a game using a tuple of length 9.For example:\nstate = (1,0,0,2,0,0,0,0,0)\nThe state can be converted to a board using the below function.\ndef state_to_board(state):\n    \"Function to convert a a state(tuple) to a board(numpy array)\"\n    board = np.array([s_to_b[position] for position in state])\n    return board.reshape(3,3)\n    \nstate_to_board((1,0,0,2,0,0,0,0,0))\n## array([['X', '__', '__'],\n##        ['O', '__', '__'],\n##        ['__', '__', '__']], dtype='&lt;U2')\nA player wins if he or she gets a sequence of 3 ’X’s or 3 ’O’s.\nmax_player_wins = (1,1,1)\nmin_player_wins = (2,2,2)\nNow all we have to do is redefine the functions is_terminal , produce_children and get_action for the new tic-tac-toe problem\nA terminal state is reached if one of the players wins or if the board is fully occupied in which case the game is tied. We will set the score of the game as follows:  X wins: + 10  O wins: -10  Draw: 0\ndef is_terminal(state):\n    \n    if  state[slice(0,3)] == max_player_wins:\n        return True,10\n    elif state[slice(0,3)] == min_player_wins:\n        return True,-10\n    elif state[slice(3,6)] == max_player_wins:\n        return True,10\n    elif state[slice(3,6)] == min_player_wins:\n        return True,-10\n    elif state[slice(6,9)] == max_player_wins:\n        return True,10\n    elif state[slice(6,9)] == min_player_wins:\n        return True,-10\n    elif state[slice(0,7,3)] == max_player_wins:\n        return True,10\n    elif state[slice(0,7,3)] == min_player_wins:\n        return True,-10\n    elif state[slice(1,8,3)] == max_player_wins:\n        return True,10\n    elif state[slice(1,8,3)] == min_player_wins:\n        return True,-10\n    elif state[slice(2,9,3)] == max_player_wins:\n        return True,10\n    elif state[slice(2,9,3)] == min_player_wins:\n        return True,-10\n    elif state[slice(0,9,4)] == max_player_wins:\n        return True,10\n    elif state[slice(0,9,4)] == min_player_wins:\n        return True,-10\n    elif state[slice(2,7,2)] == max_player_wins:\n        return True,10\n    elif state[slice(2,7,2)] == min_player_wins:\n        return True,-10\n    elif state.count(0) == 0:\n        return True,0\n    else:\n        return False,0\n        \nfrom copy import deepcopy\ndef produce_children(state):\n    l = list(state)\n    children = []\n    vacant_slots = [i for i,v in enumerate(state) if v == 0]\n    if state.count(0) % 2 == 1: #If number of vacant spaces is odd , then it is max_player's turn\n        for slot in vacant_slots:\n            child = deepcopy(l)\n            child[slot] = 1\n            children.append(tuple(child))\n    else: #if number of vacant spaces is even then it is min_player's turn.\n        for slot in vacant_slots:\n            child = deepcopy(l)\n            child[slot] = 2\n            children.append(tuple(child))\n    \n    return children\nchildren = produce_children(state)\nchildren\n## [(1, 1, 0, 2, 0, 0, 0, 0, 0), (1, 0, 1, 2, 0, 0, 0, 0, 0), (1, 0, 0, 2, 1, 0, 0, 0, 0), (1, 0, 0, 2, 0, 1, 0, 0, 0), (1, 0, 0, 2, 0, 0, 1, 0, 0), (1, 0, 0, 2, 0, 0, 0, 1, 0), (1, 0, 0, 2, 0, 0, 0, 0, 1)]\nThese correspond to the following states.\nfor child in children:\n    print(state_to_board(child),'\\n')\n## [['X' 'X' '__']\n##  ['O' '__' '__']\n##  ['__' '__' '__']] \n## \n## [['X' '__' 'X']\n##  ['O' '__' '__']\n##  ['__' '__' '__']] \n## \n## [['X' '__' '__']\n##  ['O' 'X' '__']\n##  ['__' '__' '__']] \n## \n## [['X' '__' '__']\n##  ['O' '__' 'X']\n##  ['__' '__' '__']] \n## \n## [['X' '__' '__']\n##  ['O' '__' '__']\n##  ['X' '__' '__']] \n## \n## [['X' '__' '__']\n##  ['O' '__' '__']\n##  ['__' 'X' '__']] \n## \n## [['X' '__' '__']\n##  ['O' '__' '__']\n##  ['__' '__' 'X']]\nFinally we define a function to return the action that leads from the current state to the next state.\ndef difference(tuple1,tuple2):\n    \"Helper function to get the index where first difference between two tuples is observed\"\n    assert len(tuple1) == len(tuple2)\n    for i,value in enumerate(tuple1):\n        if value != tuple2[i]:\n            return i\n    return None\n    \ndef get_action(next_state,current_state):\n    \"Function to return action that moves player from current state to next state\"\n    return difference(next_state,current_state),next_state\n    \nWe can now evaluate whether we can solve the mini tic-tac-toe problem above with the tools at hand.The starting state of the game is given by\nstate = (1,2,1,0,0,2,0,2,1)\nstate_to_board(state)\n## array([['X', 'O', 'X'],\n##        ['__', '__', 'O'],\n##        ['__', 'O', 'X']], dtype='&lt;U2')\noptimal_action,optimal_next_state = optimal_decision(state,player = 'Maximizer')\noptimal_action,state_to_board(optimal_next_state)\n## (4, array([['X', 'O', 'X'],\n##        ['__', 'X', 'O'],\n##        ['__', 'O', 'X']], dtype='&lt;U2'))\nAs expected , the algorithm determines the optimal action for player ‘X’ is to occupy the central square in the board.\nLet us consider one more path in the game where player X plays a non-optimal move resulting in the following state.\nstate = (1,2,1,1,0,2,0,2,1)\nstate_to_board(state)\n## array([['X', 'O', 'X'],\n##        ['X', '__', 'O'],\n##        ['__', 'O', 'X']], dtype='&lt;U2')\nThe optimal state of the minimizing player is given by\noptimal_action,optimal_next_state = optimal_decision(state,player = 'Minimizer')\noptimal_action,state_to_board(optimal_next_state)\n## (4, array([['X', 'O', 'X'],\n##        ['X', 'O', 'O'],\n##        ['__', 'O', 'X']], dtype='&lt;U2'))\nThe minimizing player also picks the optimal action as expected."
  },
  {
    "objectID": "blog/2021-06-19-solving-tic-tac-toe-with-minimax.en/index.html#putting-it-all-together",
    "href": "blog/2021-06-19-solving-tic-tac-toe-with-minimax.en/index.html#putting-it-all-together",
    "title": "Solving Tic-Tac-Toe with Minimax",
    "section": "Putting it all together",
    "text": "Putting it all together\nNow we will analyze the following games\n\nRandom X vs Random O \nOptimal X vs Random O \nRandom X vs Optimal O \nOptimal X vs Optimal O\n\nThe following function represents a random player who chooses an available slot at random. The maximizer is player X while the minimizer is player O\nimport random\ndef random_decision(state,player = 'Maximizer'):\n    vacant_slots = [i for i,v in enumerate(state) if v == 0]\n    action = random.choice(vacant_slots)\n    state_as_list = list(state)\n    #Update state\n    if player == 'Maximizer':\n        state_as_list[action] = 1\n    else:\n        state_as_list[action] = 2\n        \n    return action,tuple(state_as_list)\n    \nNow we will define a function to play N games and record the results for analysis\nfrom collections import defaultdict\nfrom tqdm import tqdm\ndef play_games(n_games:int,X_strategy,O_strategy):\n    '''\n    n_games: Number of games to be player\n    X_strategy: function describing decision making strategy for player X\n    O_strategy: function describing decision making strategy for player Y\n    '''\n    win_stats = defaultdict(int)\n    #Dictionary for holding no of wins for games started with a particular move\n    move_wins_X = defaultdict(int)\n    move_wins_O = defaultdict(int)\n    #Dictionary for holding no of games started with a particular move\n    move_X = defaultdict(lambda:-1)\n    move_O = defaultdict(lambda:-1)\n    \n    for i in tqdm(range(n_games)):\n        random.seed(i)\n        state = (0,0,0,0,0,0,0,0,0)\n        terminal_status = False\n       \n        first_move_flag_X = True # Flag identifying first move of player X\n        first_move_flag_O = True # Flag identifying first move of player O\n        \n        \n        while not terminal_status:\n            #Player X plays;  \n            player_x_action,next_state = X_strategy(state,player='Maximizer')\n            terminal_status,score = is_terminal(next_state)\n            \n            if first_move_flag_X:\n                first_move_X = player_x_action\n                move_X[first_move_X] += 1\n                first_move_flag_X = False\n            \n            #If player X plays last move\n            if terminal_status:\n                if score == 10: #player X wins\n                    win_stats['X_win'] +=1\n                    move_wins_X[first_move_X] += 1 #record player's first move\n                else:\n                    win_stats['Draw'] += 1\n                break\n            \n            state = next_state\n            #Player O plays next\n            \n            player_o_action,next_state = O_strategy(state,player='Minimizer')\n            terminal_status,score = is_terminal(next_state)\n            \n            if first_move_flag_O:\n                first_move_O = player_o_action\n                move_O[first_move_O] += 1\n                first_move_flag_O = False\n            \n            \n            #If player O plays last move\n            if terminal_status:\n                if score == -10: #player O wins\n                    win_stats['O_win'] +=1\n                    move_wins_O[first_move_O] += 1 #record player's first move\n                else:\n                    win_stats['Draw'] += 1\n                break\n            \n            state = next_state    \n        \n    return win_stats,move_wins_X,move_wins_O,move_X,move_O\n    \nWe also create a helper function to visualize the results.\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport pandas as pd \n\ndef plot_results(results):\n    win_stats,move_wins_X,move_wins_O,move_X,move_O = results\n    win_stats_df = pd.DataFrame({'Category':list(win_stats.keys()),'Count':list(win_stats.values())})\n    move_X_win_rate = {i:move_wins_X[i]/move_X[i]  for i in range(9)}\n    move_O_win_rate = {i:move_wins_O[i]/move_O[i]  for i in range(9)}\n    \n    \n    move_X_win_rate_array = np.array([move_X_win_rate[x] for x in range(9)]).reshape(3,3)\n    move_O_win_rate_array = np.array([move_O_win_rate[x] for x in range(9)]).reshape(3,3)\n\n        \n    sns.set(font_scale=2)\n    fig, axs = plt.subplots(ncols=3,figsize=(30,10))\n    splot = sns.barplot(x=\"Category\",y=\"Count\",data=win_stats_df,ax=axs[0])\n    for p in splot.patches:\n        splot.annotate(format(p.get_height(), '.0f'), \n                       (p.get_x() + p.get_width() / 2., p.get_height()), \n                       ha = 'center', va = 'center', \n                       xytext = (0, -12), \n                       textcoords = 'offset points')\n    splot.set_title('Distribution of Wins,Losses and Ties')\n    \n    sns.heatmap(move_X_win_rate_array,annot=True,ax = axs[1]).set_title('Player X:% of wins for first move')\n    sns.heatmap(move_O_win_rate_array,annot=True,ax = axs[2]).set_title('Player Y:% of wins for first move')\n    plt.show()"
  },
  {
    "objectID": "blog/2021-06-19-solving-tic-tac-toe-with-minimax.en/index.html#random-x-vs-random-o",
    "href": "blog/2021-06-19-solving-tic-tac-toe-with-minimax.en/index.html#random-x-vs-random-o",
    "title": "Solving Tic-Tac-Toe with Minimax",
    "section": "Random X vs Random O",
    "text": "Random X vs Random O\nresults1 = play_games(1000,X_strategy=random_decision,O_strategy=random_decision)\n## \n  0%|          | 0/1000 [00:00&lt;?, ?it/s]\n100%|##########| 1000/1000 [00:00&lt;00:00, 20874.97it/s]\nWhen both players follow a random strategy, player X has a first mover advantage and wins the majority of the games.\nplot_results(results1)\n\nWe also see that given both players use random strategies: player X can play the first move in any of the squares without it affecting the win rate significantly.\nFor player O on the other hand, playing the first move in the center or corner square results in a significantly higher win rate."
  },
  {
    "objectID": "blog/2021-06-19-solving-tic-tac-toe-with-minimax.en/index.html#optimal-x-vs-random-o",
    "href": "blog/2021-06-19-solving-tic-tac-toe-with-minimax.en/index.html#optimal-x-vs-random-o",
    "title": "Solving Tic-Tac-Toe with Minimax",
    "section": "Optimal X vs Random O",
    "text": "Optimal X vs Random O\nresults2 = play_games(1000,X_strategy=optimal_decision,O_strategy=random_decision)\n## \n  0%|          | 0/1000 [00:00&lt;?, ?it/s]\n  0%|          | 1/1000 [00:00&lt;02:18,  7.21it/s]\n100%|##########| 1000/1000 [00:00&lt;00:00, 5633.05it/s]\nplot_results(results2)\n\nWhen player X uses the optimal minimax strategy,it wins almost all the games.\nIt consistently picks the bottom right hand corner in the first move in every game."
  },
  {
    "objectID": "blog/2021-06-19-solving-tic-tac-toe-with-minimax.en/index.html#random-x-vs-optimal-o",
    "href": "blog/2021-06-19-solving-tic-tac-toe-with-minimax.en/index.html#random-x-vs-optimal-o",
    "title": "Solving Tic-Tac-Toe with Minimax",
    "section": "Random X vs Optimal O",
    "text": "Random X vs Optimal O\nresults3 = play_games(1000,X_strategy=random_decision,O_strategy=optimal_decision)\n## \n  0%|          | 0/1000 [00:00&lt;?, ?it/s]\n100%|##########| 1000/1000 [00:00&lt;00:00, 22279.67it/s]\nplot_results(results3)\n In this case, O has a lower win rate given it does not have a fist mover advantage.\nThe win rate is highest when Player O gets to occupy the central square in the first move."
  },
  {
    "objectID": "blog/2021-06-19-solving-tic-tac-toe-with-minimax.en/index.html#optimal-x-vs-optimal-o",
    "href": "blog/2021-06-19-solving-tic-tac-toe-with-minimax.en/index.html#optimal-x-vs-optimal-o",
    "title": "Solving Tic-Tac-Toe with Minimax",
    "section": "Optimal X vs Optimal O",
    "text": "Optimal X vs Optimal O\nresults4 = play_games(100,X_strategy=optimal_decision,O_strategy=optimal_decision)\n## \n  0%|          | 0/100 [00:00&lt;?, ?it/s]\n100%|##########| 100/100 [00:00&lt;00:00, 20218.39it/s]\nplot_results(results4)\n\nWhen both players play the optimal strategy, all games end in ties.\nYou can find the jupyter notebook for the blog post here"
  },
  {
    "objectID": "blog/2021-03-21-obviously-awesome/index.html",
    "href": "blog/2021-03-21-obviously-awesome/index.html",
    "title": "Obviously Awesome",
    "section": "",
    "text": "Positioning is the act of deliberately defining how you are the best at something that a defined market cares a lot about.\nPositioning is “context setting” for products. When we encounter something new, we will attempt to make sense of it by gathering together all of the little clues we can quickly find to determine how we should think about this new thing. Without that context, products are very difficult to understand, and the whole company suffers—not just the marketing and sales teams.\nCustomers need to be able to easily understand what your product is, why it’s special and why it matters to them.\n\n\n\n\nYour current customers love you, but new prospects can’t figure out what you’re selling.\n\nIf you see a disconnect between how your happy customers think about your product and how prospects see it, you likely have a positioning problem.\n\nYour company has long sales cycles and low close rates, and you’re losing out to the competition.\nYou have high customer churn\nYou are under price pressure.\n\nClear positioning helps prospects understand that you are a leader in your market segment and that you offer considerable value, making it easier for you to charge a premium.\n\n\nWhen customers encounter a product they have never seen before, they will look for contextual clues to help them figure out what it is, who it’s for and why they should care. Taken together, the messaging, pricing, features, branding, partners and customers create context and set the scene for the product.\n\n\n\n\nTrap 1: You are stuck on the idea of what you intended to build, and you don’t realize that your product has become something else. For example, you might have intended to create a new dessert but it may be better positioned as a snack. Positioning the product as a dessert vs a snack can mean significant differences in\n\nTarget buyers and where you sell: Upscale restaurants vs coffee shops \nCompetitive alternatives: Cakes and ice-creams vs Donuts and bagels\nPricing and margin: Smaller volume, higher prices vs Higher Volume , lower prices\nKey product features and roadmap: Organic and gluten free vs More carmael.\n\nChoices we make in positioning can have a massive impact on our business.\nTrap 2: You carefully designed your product for a market, but that market has changed.\nFor example, say you positioned you creation as a ‘diet snack’ but the new trend of health eating means a competitor offering something very similar positions it as a ’gluten free paleo snack ;\n\n\n\n\nGreat positioning takes into account the following\n\nThe customer’s point of view on the problem you solve and the alternative ways of solving that problem.\nThe ways you are uniquely different from those alternatives and why that’s meaningful for customers.\nThe characteristics of a potential customer that really values what you can uniquely deliver.\nThe best market context for your product that makes your unique value obvious to those customers who are best suited to your product."
  },
  {
    "objectID": "blog/2021-03-21-obviously-awesome/index.html#positioning-as-context",
    "href": "blog/2021-03-21-obviously-awesome/index.html#positioning-as-context",
    "title": "Obviously Awesome",
    "section": "",
    "text": "Positioning is the act of deliberately defining how you are the best at something that a defined market cares a lot about.\nPositioning is “context setting” for products. When we encounter something new, we will attempt to make sense of it by gathering together all of the little clues we can quickly find to determine how we should think about this new thing. Without that context, products are very difficult to understand, and the whole company suffers—not just the marketing and sales teams.\nCustomers need to be able to easily understand what your product is, why it’s special and why it matters to them.\n\n\n\n\nYour current customers love you, but new prospects can’t figure out what you’re selling.\n\nIf you see a disconnect between how your happy customers think about your product and how prospects see it, you likely have a positioning problem.\n\nYour company has long sales cycles and low close rates, and you’re losing out to the competition.\nYou have high customer churn\nYou are under price pressure.\n\nClear positioning helps prospects understand that you are a leader in your market segment and that you offer considerable value, making it easier for you to charge a premium.\n\n\nWhen customers encounter a product they have never seen before, they will look for contextual clues to help them figure out what it is, who it’s for and why they should care. Taken together, the messaging, pricing, features, branding, partners and customers create context and set the scene for the product.\n\n\n\n\nTrap 1: You are stuck on the idea of what you intended to build, and you don’t realize that your product has become something else. For example, you might have intended to create a new dessert but it may be better positioned as a snack. Positioning the product as a dessert vs a snack can mean significant differences in\n\nTarget buyers and where you sell: Upscale restaurants vs coffee shops \nCompetitive alternatives: Cakes and ice-creams vs Donuts and bagels\nPricing and margin: Smaller volume, higher prices vs Higher Volume , lower prices\nKey product features and roadmap: Organic and gluten free vs More carmael.\n\nChoices we make in positioning can have a massive impact on our business.\nTrap 2: You carefully designed your product for a market, but that market has changed.\nFor example, say you positioned you creation as a ‘diet snack’ but the new trend of health eating means a competitor offering something very similar positions it as a ’gluten free paleo snack ;\n\n\n\n\nGreat positioning takes into account the following\n\nThe customer’s point of view on the problem you solve and the alternative ways of solving that problem.\nThe ways you are uniquely different from those alternatives and why that’s meaningful for customers.\nThe characteristics of a potential customer that really values what you can uniquely deliver.\nThe best market context for your product that makes your unique value obvious to those customers who are best suited to your product."
  },
  {
    "objectID": "blog/2021-03-21-obviously-awesome/index.html#components-of-effective-positioning",
    "href": "blog/2021-03-21-obviously-awesome/index.html#components-of-effective-positioning",
    "title": "Obviously Awesome",
    "section": "Components of Effective positioning",
    "text": "Components of Effective positioning\n\nTraditional Positioning\nThe traditional positioning statement looks something like this:\nFOR target buyers, your offering\nIS A market category WHICH PROVIDES competitor’s benefits UNLIKE primary competitor WHICH PROVIDES competitor’s benefits\nThis suffers from the following problems\n\nIt assumes you know the best way to fill in the blanks. It might do a good job of capturing your current thinking, but it doesn’t give you any clues about whether your positioning is good or bad.\nIt reinforces the status quo. Most offerings are not explicitly positioned because people believe there is only one possible “default” way to position their product. Rather than helping companies think creatively about what they do, the positioning statement encourages them to look at the market the way they have always looked at it. Status quo thinking will almost always put the existing market leaders at an advantage and leave you blind to potential shifts in the way your customers see your market.\nIt doesn’t give you any hints about what to do next. Positioning statements do not lead to anything useful once it is completed. Marketing does not use it create messaging. Product does not use it to inform what features they should build.Sales use it to figure out what types of customers they should sell to.\nIt’s hard to remember. The structure of the positioning statement makes it difficult to parse or memorize, so even as a way of simply documenting your current thinking, the positioning statement is a complete failure.\n\n\n\nFive Components of Effective Positioning\n\n\nCompetitive alternatives- What customers would do if your solution didn’t exist\n\nThe alternative need not be a competitor’s product but an in-house tool built using excel.\n\nUnique attributes - The features and capabilities that you have and the alternatives lack\nValue (and Proof): The benefit that those features enable for customers\n\nMust be hard data or third party opinions\nMust be provable in an objective and demonstrable way\n\nTarget market characteristics: The characteristics of a group of buyers that lead them to really care a lot about the value you deliver\n\nTarget market is customers who buy quickly, rarely ask for discounts and tell their friends about your offerings\n\nMarket category: The market you describe yourself as being part of, to help customers understand your value\n\nChoice of market category triggers a powerful set of assumptions. A poor category choice can trigger assumptions that do not apply to our product .\n\nRelevant trends : Trends that your target customers understand and/or are interested in that can help make your product relevant right now"
  },
  {
    "objectID": "blog/2021-03-21-obviously-awesome/index.html#ten-step-positioning-process",
    "href": "blog/2021-03-21-obviously-awesome/index.html#ten-step-positioning-process",
    "title": "Obviously Awesome",
    "section": "Ten Step Positioning Process",
    "text": "Ten Step Positioning Process\n\n1) Understand customers who love your product\n\nMake a short list of your best customers\nPosition your product broadly until you have enough customer experience to start seeing patterns.\nIf a company sells a group of products together, position the company first before positioning individual products\n\n\n\n2) Form a positioning team\n\nShould include representation from sales, marketing , customer success etc.\nPerson who owns business strategy should fully support the positioning\n\n\n\n3) Align your positioning vocabulary and let go of your positioning baggage\n\nMarket confusion starts with our disconnect between understanding the product as product creators, and understanding the product as customers first perceive it.\nE.g. Clearpath robotics repositioned from the robotics category (which assumed stationary robots) to self driving vehicles for industrial use\n\n\n\n4) List your true competitive alternatives\n\nThe features of our product and the value they provide are only unique, interesting and valuable when a customer perceives them in relation to alternatives.\nUnderstanding the customer’s problem isn’t enough, to really understand how they perceived our strengths and weaknesses , we need to understand the alternatives to which they compared us.\nUnderstand what a customer might replace you with in order to understand how they categorize their solution\nIn cases, where customers don’t understand that they have a problem\n1)  Remain focused on the best fit customers list and name only what those customers would see as an alternative &lt;br&gt;\n\n\nRank the list from most common to least common\n\n\nGroup the alternatives if they cluster e.g. Do it manually includes hiring an intern or using excel.\nIf the arrival of a competitor signals a change in the way customers think about a market, positioning has to be revisited.\n\n\n\n5) Isolate your unique attributes or features\n\nIsolate what makes you different and better than those alternatives\nList all capabilities you have the alternatives do not\nYour opinion of your strengths is irrelevant without proof\n\nIf ease of use is an attribute\n\nWhat makes feature easier to use and how you prove it ?\nDoes competitor product require training and yours doesn’t?\n\nStatements from an analyst or customer is better.\nConcentrate on ‘consideration’ attributes (those a customer care about when evaluating whether to buy or not) over ‘retention attributes’ (customer cares about this when renewing e.g. customer care)\n\n\n\n\n6) Map the attributes to value “themes”\n\nCapture the value each unique attribute enables for the customer.\n\n\n\nGroup attributes into 1-4 value clusters.\nPositioning is not about highlighting every little feature a customer loves, but taking the most critical things that make us special and worth considering and bringing it front and center\n\n\n\n7) Determine who cares a lot\n\nUseful segmentation needs to go beyond demographics and firmographics\nAn actionable segmentation captures a list of a person’s or company’s easily identifiable characteristics that make them really care about what you do. e.g. skills a company has or does not, other products they have invested in.\nIn general, the segment needs to meet at least two criteria to be worthy of focus:\n(1) it needs to be big enough that it’s possible to meet the goals of your business, and\n\n\nit needs to have important, specific, unmet needs that are common to the segment.\n\n\nIf you’re a tennis racquet maker and decide to market a racquet for seniors, you need to figure out first if there are enough tennis-playing seniors who might need a racquet, and second if you could fulfil an unmet need that seniors have in their racquets.\nTarget as narrowly as you can to meet your near-term sales objectives\n\n\n\n8) Find a market frame of reference that puts your strengths at the center and determine how to position it\n\nWe position our offering in a market to trigger a set of assumptions—about competitors, features and pricing—that work to our advantage. By choosing to position within a specific market, you’re giving your prospects clues about what products they should compare you with, your key features, your price and your benefits. Those comparisons help customers quickly figure out what your product is all about and whether or not they should consider purchasing it.\nUse abductive reasoning\n\nIsolate your key features, their value and ask yourself -what types of products typically have those features?\n\nExamine adjacent (growing) markets\n\nPay attention to adjacent markets that are growing quickly. Positioning yourself in a growing market has benefits of rising customer interest, media focus and buzz.\n\nAsk your customers\n\n\n\nDifferent positioning approaches\n\nHead to Head: positioning to win an existing market\n\nYou are aiming to be the leader in a market category that already exists in the minds of customers. If there is an established leader, your goal is to beat them at their own game by convincing customers that you are the best at delivering the solution.\nIf you are already the market leader, you need to continually reinforce to your buyers that the current way of thinking about the market is the best one. This work includes reinforcing the current buying criteria and reiterating why you are the best to deliver those things. You need to quickly and forcefully defend against competitors who attempt to convince buyers to pay attention to other, emerging criteria. You also need to continually demonstrate why you deliver on these criteria better than anyone else in the market.\n\nBig Fish, Small Pond: Positioning to win a subsegment of an existing market\n\nYou are aiming to dominate a piece of an existing market category. Your goal is not to take on the overall market leaders directly, but to win in a well-defined segment of the market. You do this by targeting buyers in a subsegment of the broader market who have different requirements that are not being met by the current overall market leader.\nMany startups compete in established market categories and do so successfully by first breaking up the market into smaller pieces and focusing on one piece they can win. In marketing, the process of splitting up an existing market is called subsegmenting. A market can be subsegmented by industry (manufacturing vs. retail), by geographic region (North America vs. South America), company size and a myriad of other criteria.\nThe goal of the Big Fish, Small Pond style of positioning is to carve off a piece of the market where the rules are a little bit different—just enough to give your product an edge over the category leader.\nYou are not trying to change the purchase criteria for the overall category; in fact, you will have to prove that you do a “good enough” job in those areas when compared with the category leaders. Your focus is showing that there is a subsegment of the overall category with a specific set of needs that the current category leaders are not addressing. Those needs are very important—so important that buyers may want to relax a bit on the overall category criteria to make sure that their subsegment needs are met.\nThe need of the subsegment must be clearly identifiable, but even when it is, your ability to meet it must be strong enough to convince buyers to go with you over the safer choice of the market leader. These buyers were getting along for the most part by buying a solution that didn’t do everything but was likely “good enough” for their business. Convincing them to switch will require showing them that you have deeply understood their specific pain and have fully solved the problem.\nThere should be a way to quantify the value to the customer if they choose your solution over the market leader’s more generic solution.\n\nCreate a new game : Positioning to win a market you create\n\nYou are aiming to create a new market category. Your goals are first to prove to customers that a new market category deserves to exist, then to define the parameters of that market in the minds of customers, and lastly, to position yourself as the leader within it.\n\nWhy is the problem unique?\nWhy do existing solutions in other categories fall short of solving that problem?\n\nIf your product cannot be well positioned in any existing category, this might be a good option for you. If your solution requires both a new way of thinking about the boundaries of an existing category and a new way of thinking about purchase criteria, then it probably makes more sense to create an entirely new category rather than attempt to stretch existing categories along more than one dimension.\nTo credibly create a new category, you need a product that is demonstrably, inarguably new and different from what exists in other market categories.\nTo help customers make sense of why the category hasn’t emerges sooner, there should be a very strong answer to the questions, Why now? What factors have finally made this category possible and/or necessary?\nCategory creation is about selling the market on the problem first, rather than on your solution.\n\n\n\n“The most successful efforts in category creation do not result from company executives creating an acronym at an offsite. Rather they are discovered from deeply understanding a narrow set of customers. These customers are often ‘freaks,’ extreme in their attitudes and behavior, forged by tectonic technological and societal shifts. The category then emerges when and if the freakish attitudes and behavior become mainstream. Category creation is hard, slow work, but if you are successful the rewards are huge.”\n\n\n\n9) Layer on a trend\n\nThink of your product’s strengths, your market context and a trend that is relevant to your customer base as three overlapping circles. You are aiming for the center, where all three intersect.\n\n\n\n10) Capture your positioning so it can be shared\n\nPositioning needs to have company buy-in so it can be used to inform branding, marketing campaigns, sales strategy, product decisions and customer-success strategy."
  },
  {
    "objectID": "blog/2021-03-21-obviously-awesome/index.html#translating-positioning-into-a-sales-story",
    "href": "blog/2021-03-21-obviously-awesome/index.html#translating-positioning-into-a-sales-story",
    "title": "Obviously Awesome",
    "section": "Translating Positioning into a Sales Story",
    "text": "Translating Positioning into a Sales Story\n\nStart with a definition of the problem you are trying to solve\n\ne.g. “Insurance companies today are trying to make their claims process less difficult for demanding digital-savvy customers”\n\nDescribe how customers are attempting to solve the problem today and where the current solution falls short\n\ne.g. “Insurers have added mobile claims functionality, but it still requires customers to do many steps of the process manually,”\n\n‘Perfect world’ : Describe what the features of a perfect solution would be knowing what you know about the problem and limitations of the current solution\n\ne.g. “In a perfect world, customers could complete the entire claims process seamlessly with their mobile device,”\n\nIntroduce the product or company and position it in the relevant market category\n\ne.g. “Mobileclaimsorama is a mobile claims management solution for insurers,”\n\nTalk about each of the value themes with a bit more detail on how the solution enables the value\n\n\nThe team needs to agree on how to define the problem, current solutions, the gap and the key purchase criteria that a customer should have when looking for a solution in your market."
  },
  {
    "objectID": "blog/2021-03-21-obviously-awesome/index.html#messaging-document",
    "href": "blog/2021-03-21-obviously-awesome/index.html#messaging-document",
    "title": "Obviously Awesome",
    "section": "Messaging Document",
    "text": "Messaging Document\n\nA messaging document helps you keep a record of the accepted baseline messaging, gives everyone a common starting point for building specific copy for a specific purpose and keeps the language (and the positioning) from evolving too far away from the agreed-upon starting point.\nPricing reflects positioning and might need to be adjusted. There are price expectations in each market category, so getting your pricing in line with those will help reinforce that your product belongs there. For example, we raised the pricing for our “CRM for investment banks” because our investment banking customers didn’t expect us to be the same price as a general-purpose CRM."
  },
  {
    "objectID": "blog/2021-03-21-obviously-awesome/index.html#key-takeaways",
    "href": "blog/2021-03-21-obviously-awesome/index.html#key-takeaways",
    "title": "Obviously Awesome",
    "section": "Key Takeaways",
    "text": "Key Takeaways\n\nAny product can be positioned in multiple markets. Your product is not doomed to languish in a market where nobody understands how awesome it is.\nGreat positioning rarely comes by default. If you want to succeed, you have to determine the best way to position your product. Deliberate, try, fail, test and try again.\nUnderstanding what your best customers see as true alternatives to your solution will lead you to your differentiators.\nPosition yourself in a market that makes your strengths obvious to the folks you want to sell to.\nUse trends to make your product more interesting to customers right now, but be very cautious. Don’t layer on a trend just for the sake of being trendy—it’s better to be successful and boring, rather than fashionable and bewildering."
  },
  {
    "objectID": "blog/2020-10-30-the-ancients-and-astronomy.en/index.html",
    "href": "blog/2020-10-30-the-ancients-and-astronomy.en/index.html",
    "title": "The Ancients and Astronomy",
    "section": "",
    "text": "Man figuring out his place in the cosmos represents a stunning triumph of imagination and ingenuity. What the ancient Greeks were able to accomplish over 2000 years ago with nothing more than their mental acuity puts civilization’s stunning progress into perspective. How could we, the descendants of such brilliant ancestors, not go on and accomplish the remarkable things we have done. The apple doesn’t fall far from the tree.\nIn this blog post, I lay out these triumphs and will seek to explain how using only observations perceptible to the naked eye, clever geometry and a fertile imagination, the ancient Greeks unlocked the door to understanding our place in the cosmos."
  },
  {
    "objectID": "blog/2020-10-30-the-ancients-and-astronomy.en/index.html#size-of-the-earth",
    "href": "blog/2020-10-30-the-ancients-and-astronomy.en/index.html#size-of-the-earth",
    "title": "The Ancients and Astronomy",
    "section": "Size of the Earth",
    "text": "Size of the Earth\nThe journey starts with Eratosthenes of Cyene, a Greek scholar born in 276 BC. In order to fulfill his ambition of making a map of the entire world, he realized he needed to measure the circumference of the earth.\nBased in Alexandria, he heard from travelers that the sun illuminated the entire bottom of the well in Syene(now Aswan, Egypt), without casting shadows at noon on the summer solstice,indicating that the sun was directly overhead. We moderns now know that Syene lies on the Tropic of Cancer (\\(23.5\\circ\\)), and the sun appears directly overhead on Summer Solstice that typically falls on June 21 as shown below.\n\n\n\nFigure 1: Summer Solistice\n\n\nHe then measured the angle of the shadow cast by a stick at noon on the summer solstice in Alexandria and found it made an angle of 7.2 degrees (\\(\\theta\\) in the figure below). Note that Eratosthenes deduced this angle using a form of trigonometry as modern trigonometry had not been invented yet.\nNow, all that needed to be done was measure the distance from Alexandria to Syene (\\(\\textbf{BI}\\) in the figure below)and the problem becomes one that can be solved by simple geometry as shown below.\n\n\n\nFigure 2: Measuring the circumference of the earth\n\n\nGiven the arc \\(\\textbf{BI}\\) subtends an angle \\(\\theta\\) and the circumference of the circle \\(\\textbf{C}\\) subtends an angle \\(360\\circ\\) at the center, the circumference of the circle \\(\\textbf{C}\\)\n\\[ \\textbf{C} = \\frac{360 \\times BI}{\\theta} \\]\nEratosthenes hired professional surveyors to measure the distance between the two cities by walking equal length steps. They found this distance to be 5,000 stadia which is approximately 794 km. Using the formula above he determined the circumference of the earth to be about 250,000 stadia ,approximately 39,700 km. This is remarkably close to the modern estimate of 40,000 km."
  },
  {
    "objectID": "blog/2020-10-30-the-ancients-and-astronomy.en/index.html#size-of-the-moon",
    "href": "blog/2020-10-30-the-ancients-and-astronomy.en/index.html#size-of-the-moon",
    "title": "The Ancients and Astronomy",
    "section": "Size of the Moon",
    "text": "Size of the Moon\nAristarchus of Samos measured the size of the moon by making observations about the shadow cast by the earth during the lunar eclipse (shown below)\n\n\n\nFigure 3: Lunar Eclipse\n\n\nAristarchus knew this happened on average twice a year and realized that the angular size of the shadow cast by the earth at a distance to the moon (The angle \\(\\beta\\) between the purple dashed lines) can be measured by noting the difference in the position of the moon before and after the eclipse as shown below. Note the two positions of the green circle representing the moon before and after the eclipse.\nThe angular size of the moon (The angle \\(\\alpha\\) between the red dotted lines) could be measured at any time and is approximately \\(0.5\\circ\\).\n\n\n\nFigure 4: Size of the Moon - 1\n\n\nAristarchus calculated the ratio between these two angular sizes\n\\[ \\frac{\\beta}{\\alpha} = 2.7 \\ \\ \\ \\ \\ \\ Eq(1)\\]\nBefore we proceed,it is helpful to know the Observer’s Triangle Relation\n\n\n\nFigure 5: Observer’s Traingle Relation\n\n\nIn a nutshell this relation says the Diameter of a planet \\(D\\), the distance to the planet \\(L\\) from the observation point (B) and the angle subtended by the planet at the observer’s position \\(\\alpha\\) are related as given by the following expression. You can see the derivation of this expression here.\n\\[ \\frac{\\alpha}{57.3} = \\frac{D}{L} \\]\nThis means, referring to Figure 4\n\\[ \\frac{D_{moon}}{\\alpha} = \\frac{D_{earth\\_shadow}}{\\beta} = \\frac{L}{57.3}\\ \\ \\ \\ \\ \\ Eq(2) \\] where \\(D\\_{moon}\\) is the diameter of the moon and \\(D_{earth_shadow}\\) is the diameter of the earth’s shadow at the distance to the moon (\\(L\\))\nFrom the above expression it is clear that if two objects are at the same distance (L), their diameters should have the same ratio as their angular size.\n\\[ \\frac{D_{moon}}{D_{earth\\_shadow}}  = \\frac{\\alpha}{\\beta}\\]\nThis meant that he just needed to estimate the diameter of the earth’s shadow at the distance to the moon to calculate the diameter of the moon. This again reduces to a geometry problem.\n\n\n\nFigure 6: Size of the moon - 2\n\n\nIn the figure above, let A be the center of the sun, G the center of the earth and L the center of the moon as it revolves around the earth in what is assumed to be a perfect circle, represented by the dashed circle.\nFrom the above figure, given AGF is a straight line\n\\[ \\alpha1 + \\beta1 + \\gamma1 = 180^\\circ \\]\nAlso, given DGL is a triangle,\n\\[ \\delta1 + \\beta1 + \\theta1 = 180^\\circ \\]\nThis implies that\n\\[ \\alpha1 + \\gamma1 = \\delta1 + \\theta1 \\ \\ \\ \\ \\ \\ Eq(3)\\]\nAlso note that:\n\n\\(\\alpha1\\) is half the angular size of the sun for an observer on earth\n\\(\\gamma1\\) is half the angular size of the earth’s shadow at the distance to the moon for an observer on earth\n\\(\\theta1\\) is half the angular size of the earth for an observer on the moon\n\\(\\delta1\\) is half the angular size of the earth for an observer on the sun\n\nUsing the Observer’s Triangle Relation\n\n\\(\\frac{\\gamma1}{57.3} = \\frac{Radius\\ of\\ Earth\\ shadow\\ at\\ distance\\ to\\  moon(R_{em})}{Distance\\ to\\ moon(L_{m})}\\)\n\\(\\frac{\\alpha1}{57.3} = \\frac{Radius\\ of\\ Sun(R_s)}{Distance\\ to\\ sun(L_s)}\\)\n\\(\\frac{\\theta1}{57.3} = \\frac{Radius\\ of\\ earth(R_e)}{Distance\\ to\\ moon(L_{m})}\\)\n\\(\\frac{\\delta1}{57.3} = \\frac{Radius\\ of\\ earth(R_e)}{Distance\\ to\\ sun(L_s)}\\)\n\nSubstituting these values in \\(Eq(3)\\) gives \\[ \\frac{R_s}{L_s} + \\frac{R_{em}}{L_m} = \\frac{R_e}{L_m} +\\frac{R_e}{L_s} \\ \\ \\ \\ Eq(4) \\]\nObservations showed that the angular size of the sun and the moon are very similar, so \\(\\frac{R_s}{L_s} = \\frac{R_m(Radius\\ of\\ Moon)}{L_m}\\)\nReplacing the first term in \\(Eq(4)\\) with this expression gives\n\\[\\frac{R_m}{L_m} + \\frac{R_{em}}{L_m} = \\frac{R_e}{L_m} +\\frac{R_e}{L_s} \\]\nMultiplying throughout by \\(L_m\\) gives\n\\[ R_m + R_{em} = R_e + R_e \\times \\frac{L_m}{L_s}\\ \\ \\ \\ \\ Eq(5)\\] Given the distance to the sun is much greater than the distance to the moon, Aristarchus concluded that he could ignore the last term giving the final expression\n\\[ R_{em} = R_{e} - R_{m} \\]\nRearranging terms gives\n\\[ \\frac{R_{em}}{R_m} = \\frac{R_e}{R_m} - 1\\ \\ \\ \\ Eq(6) \\]\nUsing \\(Eq(1)\\)and \\(Eq(2)\\)and the fact that radius is just half the diameter gives:\n\\[\\frac{\\beta}{\\alpha} = \\frac{D_{earth\\ shadow}}{D_{moon}} = \\frac{R_{em}}{R_{m}}= 2.7 \\]\nSubstituting this in \\(Eq(6)\\)gives:\n\\[ \\frac{R_e}{R_m} = 3.7 \\]\nThe circumference of the earth had been established as approximately 39,700 km by Eratosthenes, the radius can be easily calculated from this : 6,318 km\nThe radius of the moon is then given by \\[ \\frac{R_e}{3.7} = \\frac{6318}{3.7} = 1,707 kms \\] This is remarkably close to the modern estimate of 1,736 km"
  },
  {
    "objectID": "blog/2020-10-30-the-ancients-and-astronomy.en/index.html#distance-to-the-moon",
    "href": "blog/2020-10-30-the-ancients-and-astronomy.en/index.html#distance-to-the-moon",
    "title": "The Ancients and Astronomy",
    "section": "Distance to the Moon",
    "text": "Distance to the Moon\nHaving measured the size of the moon and the angular size of the moon, the ancient Greeks could now use the parallax method to determine the distance to the moon.\nConsider an object JQ of size \\(L\\), which has an angular size of \\(\\alpha\\)from the observation point \\(I\\). You want to measure the distance to this object.\nYou can construct a circle with center at \\(I\\)and passing through both ends of the object as shown below\n\n\n\nFigure 7: The parallax method\n\n\nIf \\(360^{\\circ}\\) corresponds to an arc length of \\(2\\pi r\\)(the circumference of the circle), and the arc length \\(L\\)corresponds to the angle \\(\\alpha\\)\n\\[ \\alpha^{\\circ}: L :: 360^{\\circ} : 2\\pi r\\]\n\\(r\\) is given by:\n\\[ r = \\frac{360 L}{2 \\pi \\alpha} \\]\nIf the object is really far away, the size of the object \\(L\\)will be roughly equal to the arc length of the circle, further the radius of the circle \\(r\\)is a rough approximation of the distance to the object.\nWe had seen earlier, that the angular size of the as observed moon from the earth (\\(\\alpha\\))is roughly \\(0.5^{\\circ}\\). Substituting these values in the above equation gives \\(r = 114.59 \\text{L}\\)\nThe distance to the moon is roughly 115 times the diameter of the moon. i.e. \\(115 \\times 2 \\times 1707 = 392,610 \\text{kms}\\)\nThis is close to the modern estimate of 384,400 km.\nNote that according to certain sources, Aristarchus measured the angle \\(\\alpha\\) as \\(2^{\\circ}\\)and hence ended up under estimating this distance by a factor of 4"
  },
  {
    "objectID": "blog/2020-10-30-the-ancients-and-astronomy.en/index.html#distance-to-the-sun",
    "href": "blog/2020-10-30-the-ancients-and-astronomy.en/index.html#distance-to-the-sun",
    "title": "The Ancients and Astronomy",
    "section": "Distance to the Sun",
    "text": "Distance to the Sun\nAristarchus again used intelligent geometry to estimate the distance to the sun,however his ingenuity would be handicapped by the lack of equipment to make accurate observations.\nConsider the sun in yellow with center at B, earth in green with center at A and the moon in red with center at G.\n\n\n\nFigure 8: Distance to the sun\n\n\nAristarchus realized that the sun , earth and moon \\(\\Delta BGA\\) formed a right triangle when the moon is half illuminated by the sun as shown above.\n\\[ \\angle BCA = 90^{\\circ} \\]\nHe estimated the angular distance between the sun and the moon as \\(87^{\\circ}\\)\n\\[ \\angle CAB = 87^{\\circ} \\]\nTrigonometry tells us that \\(\\cos(87^\\circ) = \\frac{CA}{AB} = \\frac{\\text{Distance to the moon}}{\\text{Distance to the sun}} \\approx 0.05\\)\nDistance to the sun should be 20 times the distance to the moon (which he had already estimated.)\nHowever he had estimated \\(\\angle CAB\\) incorrectly without any modern telescopic equipment. Modern measurements tell us this angle is actually \\(89.85^{\\circ}\\). Using this measurement, the distance to the sun is actually close to 382 times the distance to the moon, so he was off by a factor close to 20."
  },
  {
    "objectID": "blog/2020-10-30-the-ancients-and-astronomy.en/index.html#conclusion",
    "href": "blog/2020-10-30-the-ancients-and-astronomy.en/index.html#conclusion",
    "title": "The Ancients and Astronomy",
    "section": "Conclusion",
    "text": "Conclusion\nAfter these discoveries, more than 1500 years would go by before Copernicus and Galileo would transform our understanding of our solar system. Although we take scientific progress granted today, this shows that our civilization is fragile and progress is not a foregone conclusion.\nOur understanding of our place in the cosmos has evolved rapidly over the last two hundred years. The invention of the spectrograph by Fraunhofer in 1814 has allowed us to infer the properties,distance and mass of even distant stars by analyzing the light they emit.\nIn the space of a few hundred years, we have gone from thinking the solar system was all of the cosmos to realizing as Carl Sagan succinctly put it\n\n“We live on a hunk of rock and metal that circles a humdrum star that is one of 400 billion other stars that make up the Milky Way Galaxy, which is one of billions of other galaxies which make up a universe which may be one of a very large number, perhaps an infinite number, of other universes.”\n\nOur ancestors would be truly proud."
  },
  {
    "objectID": "blog/2020-10-30-the-ancients-and-astronomy.en/index.html#references",
    "href": "blog/2020-10-30-the-ancients-and-astronomy.en/index.html#references",
    "title": "The Ancients and Astronomy",
    "section": "References",
    "text": "References\n\nhttps://www.amazon.com/Our-Mathematical-Universe-Ultimate-Reality/dp/0307599809\nhttps://www.aps.org/publications/apsnews/200606/history.cfm\nhttps://www.youtube.com/watch?v=O6KOSvYHAmA\nhttp://www.eg.bucknell.edu/physics/astronomy/astr101/specials/aristarchus.html\nhttp://www.phy6.org/stargaze/Sparalax.htm\nhttps://pwg.gsfc.nasa.gov/stargaze/Shipprc2.htm"
  },
  {
    "objectID": "blog/2021-01-30-the-covid-vaccine-landscape-for-people-in-a-hurry.en/index.html",
    "href": "blog/2021-01-30-the-covid-vaccine-landscape-for-people-in-a-hurry.en/index.html",
    "title": "Understanding the Biology of Covid and Covid vaccines",
    "section": "",
    "text": "In this article, I provide an overview of the biology of Covid and how Covid vaccines work; atleast as best as someone with no formal training in Biology can 😊.\nAll the content in this post comes from fantastic resources I found online. All of these have been listed in the references section."
  },
  {
    "objectID": "blog/2021-01-30-the-covid-vaccine-landscape-for-people-in-a-hurry.en/index.html#introduction",
    "href": "blog/2021-01-30-the-covid-vaccine-landscape-for-people-in-a-hurry.en/index.html#introduction",
    "title": "Understanding the Biology of Covid and Covid vaccines",
    "section": "",
    "text": "In this article, I provide an overview of the biology of Covid and how Covid vaccines work; atleast as best as someone with no formal training in Biology can 😊.\nAll the content in this post comes from fantastic resources I found online. All of these have been listed in the references section."
  },
  {
    "objectID": "blog/2021-01-30-the-covid-vaccine-landscape-for-people-in-a-hurry.en/index.html#central-dogma-of-life",
    "href": "blog/2021-01-30-the-covid-vaccine-landscape-for-people-in-a-hurry.en/index.html#central-dogma-of-life",
    "title": "Understanding the Biology of Covid and Covid vaccines",
    "section": "Central Dogma of Life",
    "text": "Central Dogma of Life\nTo understand how the vaccines work, it is best to briefly understand how life works.\nOne of the major breakthroughs in molecular biology was the discovery of the following mechanism - also known as the central dogma of molecular biology,\n\\[ DNA \\rightarrow RNA \\rightarrow Amino\\ Acids \\rightarrow Protein \\]\n\nDNA\nAs you may know, DNA contains the recipe for creating and running a living organism. Just like all computer programs are ultimately sequences of bits(0 or 1). DNA is a sequence of nucleotides (A-Adenine,C-Cytosine,T-Thymine and G-Guanine) stored in the nucleus of the cell.\nDNA is equivalent to Read only Memory(ROM) that permanently stores instructions on your computer.\n\n\nRNA\nTo carry out a specific task you need to read the instructions from the ROM. This is where RNA comes in.\nOur cells have molecular machines that can read snippets of DNA and convert them into RNA molecules. The RNA molecule typically contains a subset of the information stored in the DNA. It is equivalent to reading something essential from a computer’s ROM to its RAM (Random Access Memory). The information in the RAM is meant to carry out a specific task and is quickly discarded. Similarly RNA are created for a specific task : to instruct the cell how to create amino acids and proteins. They are quickly discarded after the task is complete. RNA molecules are therefore fragile and short lived.\nJust like DNA, the RNA is also a sequence of four nucleotides - A-Adenine,C-Cytosine,U - Uracil and G-Guanine.\nThe messenger RNA or mRNA is an RNA molecule that carries information about a gene (a sequence of nucleotides in the DNA) from the nucleus into the cytoplasm (outside the nucleus) where amino acids are made. Given mRNA are generated inside the nucleus, the body is primed to quickly identify any mRNA that does not originate in the nucleus and destroy it.\n\n\nAmino Acids\nDNA encodes for Amino Acids using combinations of 3 nucleotides - called codons. There are 64 possible codons (4x4x4). AAA is a codon so is CTG.\nThere are 20 different amino acids encoded by these 64 codons, some codons encode the same amino acid. E.g. CTT and CTC encode for the same amino acid - Leucine as shown below.\n\n\n\nFigure 1: DNA/Codon Table from Wikipedia\n\n\n\n\nProteins\nAll proteins are created using some combination of amino acids.The genes in our body - through a sequence of codons - encode the amino acids that are to be used to create a protein.\nFor example the smallest protein discovered so far is TRP-Cage. It comprises 20 amino acids derived from the saliva of Gila monsters!\n\nAsn Leu Tyr Ile Gln Trp Leu Lys Asp Gly\nGly Pro Ser Ser Gly Arg Pro Pro Pro Ser\n\nProteins are created by a molecular machine in the cell called the ribosome. The ribosomes reads a strand of RNA and produces a sequence of amino acids called a polypeptide.The polypeptide is then folded into a 3D structure - the protein. The 3D structure of the protein is a key determinant of the function of a protein."
  },
  {
    "objectID": "blog/2021-01-30-the-covid-vaccine-landscape-for-people-in-a-hurry.en/index.html#sars-cov-2-virus",
    "href": "blog/2021-01-30-the-covid-vaccine-landscape-for-people-in-a-hurry.en/index.html#sars-cov-2-virus",
    "title": "Understanding the Biology of Covid and Covid vaccines",
    "section": "SARS-Cov-2 virus",
    "text": "SARS-Cov-2 virus\nThe SARS-Cov-2 virus is a single-stranded RNA enveloped virus. The image below shows two types of enveloped viruses.\n\n\n\nFigure 2: Image from amboss.com\n\n\nSARS-COV-2 resembles the one on the right. It has a Linear single strand RNA(rather than DNA as shown in the image on the left), a helical capsid (a protein shell enclosing the RNA) , a lipid bi-layer and “spike” proteins.\nThe lipid bi-layer that envelopes the virus is extremely vulnerable to organic solvents (e.g. alcohol),detergents and dry heat.\nThis is why scientists have been encouraging us to wash our hands and wipe down groceries with alcoholic wipes.\nThe spike protein is critical to the life cycle of the virus. It binds to the ACE2 protein in human cells, creating a hook to attach to human cells. The ACE2 protein in our cells thus acts as a cellular doorway or receptor for the SARS COV2 virus.\n\n\n\nFigure 3: Covid-19 coronavirus binding to ACE2 receptors: Source: Juan Gaertner/Science Photo Library\n\n\nOnce the virus binds to a target cell, activation of the spike protein allows the virus to enter the cell.\nThe ACE2(Angiotensin-converting enzyme 2) protein in our bodies is vital in modulating the activities of another protein called angiotensin II(ANG II).ANG II increases blood pressure and inflammation causing damage to blood vessel lining and other types of tissue injury. Most critically, ANG II can increase inflammation and the death of cells in the alveoli in the lungs; alveoli are critical for absorbing oxygen into the body.\nWhen the SARS-Cov2 virus binds to ACE2, it prevents ACE2 from performing its normal function of cutting up ANG-II . Without ACE2 to regulate it, ANG II can wreak havoc - injuring cells in the lungs and the heart."
  },
  {
    "objectID": "blog/2021-01-30-the-covid-vaccine-landscape-for-people-in-a-hurry.en/index.html#vaccines",
    "href": "blog/2021-01-30-the-covid-vaccine-landscape-for-people-in-a-hurry.en/index.html#vaccines",
    "title": "Understanding the Biology of Covid and Covid vaccines",
    "section": "Vaccines",
    "text": "Vaccines\nAll major vaccines attempt to trigger the human body into producing anti bodies to identify and destroy the spike protein along with the virus that carries it.\n\nTypes of Vaccines\n\nmRNA \nviral vector \nprotein subunit \nattenuated or inactivated virus \n\n\nmRNA\nThe mRNAvaccines effectively deceive the cells in the human body to produce the spike protein. This triggers our immune system into a response against the protein and any virus anchoring it.\nThis article explains how the Pfizer-BioNtech vaccine is engineered.\nHere are the key takeaways\n\nAs described in the RNA section, RNA are fragile. For this reason these vaccines have to be stored at very low temperatures.\nmRNA vaccines are synthetically generated RNA that can induce our cells to produce the spike proteins. They have been intelligently engineered in ways to deceive our immune system from destroying it.\nOur body usually identifies and destroys any foreign RNA. To evade the immune system, the Pfizer-BioNTech vaccine replaces the U nucleotide in RNA with a different molecule $ $. With this replacement, the immune system loses interest in the RNA and does not destroy it.\nA stretch of the mRNA vaccine encodes the actual spike protein with some minor modifications as shown below:\n\n\n\n\nFigure 4: Spike protein encoding in vaccine and virus\n\n\n\nIn the first codon (CUU), the U nucleotide is flipped to G, this enhances protein production in the cell \nThe spike proteins are stable when mounted on the virus, but can quickly collapse into an unrelated structure if it stands alone. Replacing two amino acids K(AAA) and V(GUU) with Proline (CCU) - a very rigid amino acid prevents this and ensures the spike proteins produced by our cells are stable.\n\nThe original virus mRNA has thus been modified in this vaccine.Further the protein being expressed has also been modified to give it more stability.\nThis mRNA has to be carried into the cell. mRNA vaccines use mRNA carriers for this purpose. All the mRNA vaccines use lipid nano particles as carriers.\nCarriers molecules should have lots of positive charges on them. Given RNA has lot of negatively charged phosphate groups,the two match up to create stable combinations. Lipid nano particles have been found to be very effective in protecting the mRNA as it travels through the bloodstream and also helps it cross through the cell membrane into the cell.\nThe Pfizer vaccine uses lipid nano particles developed by the Canadian company Acuitas.\nmRNA vaccines also have the added advantage they only need to get into the cell and not the nucleus. A DNA based vaccine on the other hand will have to get into the nucleus of the cell where it could be accidentally incorporated into the genome.\n\n\nViral Vector\nThis category of vaccines uses an actual harmless virus - typically modified adenoviruses from humans or apes as vectors - which is equipped with a strand of DNA that contains instructions to make the spike protein.\nUnlike mRNA vaccines, viral vectors have to deliver the DNA payload directly into the nucleus. Once the DNA has been delivered to the nucleus, it is read by the machinery in the human cell nucleus and translated into mRNA. The mRNA is then read by the cell and translated into spike proteins.\n\n\n\nFigure 5: How viral vector vaccines work; source- NY Times\n\n\nThe viral vectors have been modified enough not to trigger our immune system against the carrier virus itself. They also do not reproduce. However, given they are a foreign virus, they can raise an alarm and trigger the immune system into a stronger reaction to the spike proteins.\nAdeno viruses are sturdy double stranded DNA viruses and more robust than RNA. They do not need special cooling unlike the mRNA vaccines.\n\n\nProtein Sub Unit\nThis type of vaccine works by directly injecting the spike protein into the body. The spike protein is formulated to trigger the body into producing an immune response.\n\n\nAttenuated Virus\nThis type of vaccine uses a dead or harmless version of the SARS-COV-2 virus. Given the virus has been attenuated, it will not trigger an immune response on its own. Therefore, this vaccine type uses an adjuvant to trigger an immune response from the body.\nSuch adjuvanted vaccines can cause more local reactions (such as redness,pain or swelling at the injection site) as well as systematic reactions (such as fever,chills and body aches) compared to non-adjuvanted vaccines.\nOn the plus side, such vaccines do not need to express a modified protein as they are hosted on the weakened virus.\n\n\n\nProtein Expression\nA key determinant of the efficacy of vaccines is how well the proteins are expressed or produced and how stable the proteins produced are.\nResearch has indicated that modified mRNA is two orders of magnitude (100x) better at expressing proteins than unmodified mRNA. We have also seen that modified spike proteins are more stable than unmodified spike proteins.\nThe following table captures the difference between the major vaccine types.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVaccine\nCompany\nType\nDelivery Vehicle\nSpike Protein Expression\nEfficacy % (95% CI)\nDoses\nCost per dose\nStorage\n\n\n\n\nBNT162b2/Tozinemeran/Comirnaty\nPfizer-BioNTech\nmodified mRNA\nLipid-nanoparticle\nmodified\n94.8 (89.8 -97.6)\nx2\n$20\n-70 C\n\n\nmRNA-1273\nModerna\nmodified mRNA\nLipid-nanorparticle\nmodified\n94.1 (89.3 - 96.8)\n2\n$33\n-20C\n\n\nCVnCoV\nCurevac\nUnmodified mRNA\nLipid-nanoparticle\nmodified\nNot Available\n2\n$12\n4 C\n\n\nAZD1222/Covishield\nOxford-AstraZeneca\nviral vector\nNA\nun-modified\n70.4 (54.8 to 80.6)\n2\n$4\n4 C\n\n\nAd26.COV2-S\nJohnson& Johnson\nviral vector\nNA\nmodified\n66\n1\n$10\n4 C\n\n\nSputnik V\nGamaleya Research Institute of Epidemology and Microbiology\nviral vector\nNA\nNA\n91.6 (85.6-95.2)\n2\n$10\n4 C\n\n\nNVX-CoV2373\nNovavax\nprotein-subunit with adjuvant\nNA\ndoubly modified\n89.3 (75.2 - 95.4)\n2\n$ 16\n4 C\n\n\nCovaxin\nBharat Biotech\nattenuated virus with adjuvant\nNA\nNA\n81\n2\n$ 4\n4 C\n\n\nAd5 vectored vaccine\nCanSino Biological Inc and Beijing Institute of BioTechnology\nviral vector\nNA\nNA\n65.7\n1\n$ 30.62\n4 C\n\n\n\n\n\nVaccine Efficacy and Effectiveness\nEfficacy is the degree to which a vaccine prevents disease and possibly also transmission under ideal and controlled circumstances. This is measured only in Phase 3 of a vaccine’s clinical trial.The numbers in the table above measure vaccine efficacy\nPhase 3 Vaccine clinical trials are Randomized Controlled trials where a subset of the population is selected based on factors such as age . This population is divided into a test group that receives the vaccine and a placebo group that receives a placebo.\nEfficacy of 90% means a 90% reduction in cases of the diseases in the test group compared to the Vaccine group. E.g. Assume there are 1000 people in either group. After administration of the vaccine or placebo, the number of people in each group are monitored for the disease. If 900 people in the placebo group contract the disease but only 100 people in the test group contract the disease, the efficacy is given by.\n\\[ \\frac{900 -100}{900} \\times 100 =  88.9 \\%\\]\nEffectiveness is how well the vaccine performs in the real world in typical field conditions. Any factor that was not controlled for in the clinical trial such as gender, illness or quality of vaccine storage can reduce the actual effectiveness of the vaccine.\nA study in Israel where over half a million people were vaccinated with the Pfizer - BioNtech vaccine indicates this vaccine has an effectiveness of 90.3% for asymptomatic cases and 93.7% for symptomatic cases.\nIt is clear that the Pfizer-BioNTech and Moderna vaccines are the most efficacious. Even with a single shot these two vaccines have efficacy of 92.6% and 91.6% respectively.\nHowever, we need to keep in mind that most of these vaccines will be very effective in preventing severe cases of Covid that may result in hospitalizations - this is extremely critical.\nFor.e.g the J&J vaccine was 85% effective in preventing severe disease and offered complete protection against Covid related hospitalization and death 28 days after vaccination.\nMany of the other vaccines also poses fewer logistical challenges and will still be very effective in breaking the transmission of the disease and ending the pandemic."
  },
  {
    "objectID": "blog/2021-01-30-the-covid-vaccine-landscape-for-people-in-a-hurry.en/index.html#variants",
    "href": "blog/2021-01-30-the-covid-vaccine-landscape-for-people-in-a-hurry.en/index.html#variants",
    "title": "Understanding the Biology of Covid and Covid vaccines",
    "section": "Variants",
    "text": "Variants\nViruses mutate all the time due to errors in the DNA/RNA replication mechanism. Any one of the four nucleotides in the SARS-COV-2 virus RNA genome can get switched.\nAlthough the rate at which the SARS-Cov-2 virus mutates is much slower than that of others such as influenza or HIV, the scale of the pandemic means that variants of the virus that are more transmissible or resistant to vaccines may emerge due to natural selection.\nThree variants that have emerged recently include\n1)B.1.17\nThis was first detected in the UK in September 2020. This variant has incurred 23 mutations that differentiate it from it’s ancestor in the wild. Studies indicate that it is 35% - 45 % more transmissible than it’s predecessor.It has not been confirmed it is any more lethal than it’s predecessor.\n\nB.1.351\n\nThis was identified in South Africa in Dec 2020. This variant likely poses the biggest concern in the long run.\n\nP.1\n\nThis was identified in Brazil.\nAll these variants change the way the critical spike protein is expressed by the virus.The mutations in B.1.351 and P.1 seem to make it more difficult for the body’s antibodies to bind to the spike protein on the virus.\nStudies have indicated that anitbodies from individuals vaccinated with the Moderna and Pfizer-BioNTech vaccines work well against the B.1.17 variant but less so against the B.1.351 variant. In fact, no vaccine has reported greater than 60% efficacy in preventing symptoms for the B.1.351 variant of Covid\nThe Novavax and Johnson&Johnson vaccines also appear to be less effective against these variants but should still be effective in preventing severe disease and hospitalizations.\nScientists and researchers are hard at work developing vaccines that are more effective against these variants. Moderna has in fact submitted a vaccine candidate designed specifically for the B.1.351 variant.This would be administered as a booster shot."
  },
  {
    "objectID": "blog/2021-01-30-the-covid-vaccine-landscape-for-people-in-a-hurry.en/index.html#conclusion",
    "href": "blog/2021-01-30-the-covid-vaccine-landscape-for-people-in-a-hurry.en/index.html#conclusion",
    "title": "Understanding the Biology of Covid and Covid vaccines",
    "section": "Conclusion",
    "text": "Conclusion\nAt a personal level, if the pandemic has a silver lining, it is that I have developed a greater appreciation for the biological sciences. I hope this blog does the same for you.\nAlso,I want to extend my gratitude to the thousands of health care workers who have been putting their lives on the line to keep us safe and the brilliant scientists who have made pandemic busting vaccines a reality in just over a year."
  },
  {
    "objectID": "blog/2021-01-30-the-covid-vaccine-landscape-for-people-in-a-hurry.en/index.html#references",
    "href": "blog/2021-01-30-the-covid-vaccine-landscape-for-people-in-a-hurry.en/index.html#references",
    "title": "Understanding the Biology of Covid and Covid vaccines",
    "section": "References",
    "text": "References\n\nReverse Engineering the source code of the BioNTech/Pfizer Vaccine\nThe Genetic Code and Proteins of Other Covid-19 vaccines\nCentral Dogma of Molecular Biology\nhttps://www.genome.gov/genetics-glossary/messenger-rna\nTc5b\nhttps://www.amboss.com/us/knowledge/General_virology\nhttps://www.nature.com/articles/s41401-020-0485-4#Sec2\nhttps://theconversation.com/what-is-the-ace2-receptor-how-is-it-connected-to-coronavirus-and-why-might-it-be-key-to-treating-covid-19-the-experts-explain-136928\nhttps://csvoss.com/a-mechanists-guide-to-the-coronavirus-genome\nhttps://twitter.com/profshanecrotty/status/1367179841217306626\nhttps://www.quantamagazine.org/how-to-understand-covid-19-variants-and-their-effects-on-vaccines-20210225/\nhttps://www.bloomberg.com/news/articles/2021-02-08/pakistan-says-cansino-s-covid-vaccine-shows-65-7-efficacy\nhttps://www.globaltimes.cn/content/1210093.shtml\nhttps://www.nejm.org/doi/full/10.1056/NEJMc2036242\nhttps://www.nejm.org/doi/full/10.1056/NEJMoa2035389\nhttps://www.clinicaltrialsarena.com/comment/covid-19-vaccine-protection/"
  },
  {
    "objectID": "blog/2022-12-11-evaluating-and-improving-transaction-monitoring-systems-is-there-a-better-way/index.html",
    "href": "blog/2022-12-11-evaluating-and-improving-transaction-monitoring-systems-is-there-a-better-way/index.html",
    "title": "Evaluating and Improving Transaction Monitoring Systems - Is there a better way ?",
    "section": "",
    "text": "Transaction monitoring has been a critical component of AML compliance for the last twenty years. Financial institutions devote a tremendous amount of resources to maintain, optimize and enhance transaction monitoring systems in order to keep pace with the rapid changes in financial crime and the evolving nature of regulations.\nThe work horse of transaction monitoring over the past two decades has been rule based systems commonly referred to as scenarios. Scenarios are essentially simple if-else statements that trigger an alert if a specified combination of conditions are met. E.g. If X &gt; a and Y &gt; b or Z &gt; c. In industry parlance, X ,Y and Z are parameters while a,b and c are thresholds.\nFinancial institutions can deploy anywhere between a few to several dozen scenarios depending on the size and risk profile of the institution. New scenarios are also deployed in response to new products that the institution brings to market."
  },
  {
    "objectID": "blog/2022-12-11-evaluating-and-improving-transaction-monitoring-systems-is-there-a-better-way/index.html#scenario-tuning-and-its-limitations",
    "href": "blog/2022-12-11-evaluating-and-improving-transaction-monitoring-systems-is-there-a-better-way/index.html#scenario-tuning-and-its-limitations",
    "title": "Evaluating and Improving Transaction Monitoring Systems - Is there a better way ?",
    "section": "Scenario tuning, and its limitations",
    "text": "Scenario tuning, and its limitations\nThe foremost challenge in running an effective transaction monitoring system is ensuring that the system is monitoring the right activity, in other words – Is the system appropriately tuned? If the thresholds of the scenario are too low, you could end up generating too many false positives, if they are too high, you could end up with false negatives - a scary proposition for most institutions.\nFinancial institutions have traditionally used Above The Line (ATL) and Below The Line (BTL) testing to evaluate whether a scenario is appropriately tuned. There are several limitations with this approach.\nFirstly, each scenario is evaluated and tuned independently; this ignores an important property of the system – scenarios do not operate in isolation. Multiple scenarios interact and overlap to create a monitoring mesh.\nIn team sports, evaluating the players in a team independently does not tell you about the quality of a team. In transaction monitoring, evaluating each scenario independently does not necessarily tell you about the quality of the overall system.\nJust as 11 of the best players do not make the best team, using eleven conservatively tuned scenarios need not make a high-quality transaction monitoring system. There could be blind spots in the monitoring system that could still be exploited by sophisticated actors. Conversely, you can often build a great team by identifying the right players who complement each other’s strengths and weaknesses. A team can be much more than the sum of its individual parts. Could the same dynamics apply to transaction monitoring systems? Is it possible to evaluate and improve a transaction monitoring system holistically rather than reductively?\nSecondly, several institutions use the analysis of ATL data to determine if BTL testing is necessary. If the ATL data indicates an absence of effective alerts near current thresholds, BTL testing is deemed unnecessary.\nThe absence of effective alerts ATL might be a necessary but not sufficient condition for AML risk below the line. When scenarios are deployed to provide coverage rather than to detect specific activity, ATL results may not be a good predictor of BTL risk. This also doesn’t mean it is prudent to indiscriminately do BTL testing for every scenario.\nIs there a more systematic way to determine whether there is AML risk below the line for a scenario based on which a decision can be made to carry out BTL testing?\nThirdly, when a decision to do BTL testing has been made, institutions determine the threshold to be tested fairly arbitrarily using 5,000 or 10,000 dollar increments. There is simply no reason why increments should be in multiple of 5,000 or 10,000 or 2,450.\nIs there a more methodical, explainable way to determine the right threshold below the line that should be tested?\nFinally, consider the fact that ATL and BTL tuning is typically carried out on 12 -18 months of historical data. The data issues permeating this historical data are well understood by most institutions. Most of the good alerts or SARs used to tune a scenario (e.g. Rapid Movement of Funds) are not a result of activity of interest to the scenario. In fact, these alerts might have been tagged as suspicious due to entirely tangential reasons (e.g. Negative News on the focal entity). Many institutions continue to use this problematic data to tune scenarios as removing these will leave them with very little signal to tune the scenario.\nThe overwhelming majority of customers at financial institutions are law abiding citizens. Stringent KYC procedures ensure that any individual or corporation with even a whiff of suspicion are denied services or have the banking relationship terminated. This means the historical data used to evaluate transaction monitoring systems are largely from benign customers.\nUsing historical transaction data to evaluate transaction monitoring systems is akin to evaluating the strength of the financial system in 2008 using data from the boom years preceding the financial crisis. This never revealed weaknesses in the system. After the great recession of 2008, regulators mandated that institutions carry out stress tests to evaluate the robustness of the financial system.\nIn AML, is it sufficient to evaluate our transaction monitoring system with data that is equivalent to that of the pre-crisis boom years? How can we stress test the system so that its weaknesses become apparent, giving us the opportunity to fix it before a sophisticated money laundered exploits it?"
  },
  {
    "objectID": "blog/2022-12-11-evaluating-and-improving-transaction-monitoring-systems-is-there-a-better-way/index.html#a-better-way",
    "href": "blog/2022-12-11-evaluating-and-improving-transaction-monitoring-systems-is-there-a-better-way/index.html#a-better-way",
    "title": "Evaluating and Improving Transaction Monitoring Systems - Is there a better way ?",
    "section": "A Better Way",
    "text": "A Better Way\n\nA holistic approach\nI believe that a better way of evaluating transaction monitoring requires us to take a holistic perspective. Scenarios interact and overlap in ways that impact the system’s performance. Viewing transaction monitoring systems through this holistic lens will a) Reveal opportunities to retire scenarios and relax thresholds by identifying redundancies b) Alert us to gaps in the system that can be fixed by raising thresholds or deploying a new scenario.\n\n\nAn adversarial approach\nA modern approach to evaluating and improving transaction monitoring systems can also learn from ethical hacking in the cybersecurity domain.\n\nEthical hacking is a process of detecting vulnerabilities in an application, system, or organization’s infrastructure that an attacker can use to exploit an individual or organization. They use this process to prevent cyberattacks and security breaches by lawfully hacking into the systems and looking for weak points. Ethical hackers learn and perform hacking in a professional manner, based on the direction of the client, and later, present a maturity scorecard highlighting their overall risk and vulnerabilities and suggestions to improve\n\nI believe a transaction monitoring system should be evaluated by determining how effectively it can resist an adversarial money launderer who is seeking to move money through the bank. A robust transaction monitoring system will make it infeasible for the money launderer to move money through the bank in a reasonable length of time without triggering alerts."
  },
  {
    "objectID": "blog/2022-12-11-evaluating-and-improving-transaction-monitoring-systems-is-there-a-better-way/index.html#a-modern-approach-to-evaluating-transaction-monitoring-systems",
    "href": "blog/2022-12-11-evaluating-and-improving-transaction-monitoring-systems-is-there-a-better-way/index.html#a-modern-approach-to-evaluating-transaction-monitoring-systems",
    "title": "Evaluating and Improving Transaction Monitoring Systems - Is there a better way ?",
    "section": "A Modern Approach to evaluating Transaction Monitoring Systems",
    "text": "A Modern Approach to evaluating Transaction Monitoring Systems\nA system that takes such a holistic, adversarial perspective can address the limitations of scenario tuning approaches used by financial institutions today.\nThe solution is to simulate a money launderer who can test the transaction monitoring system for gaps, much like how an ethical hacker probes a cybersecurity system for vulnerabilities.\nSuch an agent can probe the entire transaction monitoring system rather than each scenario in isolation. Further, the patterns identified by such an agent can be used to identify real BTL risks for each scenario which in turn can inform which scenarios should be subjected to BTL testing and which thresholds BTL should be tested.\nBesides addressing the limitations discussed earlier, this new system will be able to highlight the pathways or gaps a potential money launderer could exploit. It can also recommend threshold changes and scenarios that can plug these gaps.\nIf an institution wants to launch a new product, the system will be able to determine what the resulting AML risk to the institution is. It will be able to mitigate this AML risk by recommending threshold changes to existing scenarios or recommend new scenarios to monitor this product.\nObviously, unlike in cybersecurity, financial institutions do not have the option to hire ethical money launderers to help evaluate the system. How can Financial Institutions implement such an adversarial, intelligent approach to evaluating transaction monitoring systems?\nAdvances in deep learning and reinforcement learning have made it possible to solve a problem that has been intractable up to this point. At Oracle, we are building Compliance Agent, a product that attempts to solve this problem using deep reinforcement learning to train such an adversarial agent.\nOFS Compliance Agent will assess the transaction monitoring system of an institution holistically, identify gaps, recommend changes to address these gaps and also provide an estimate of the operational impact of these changes.\nIf you are interested in learning more, feel fee to reach out to me. I will be happy to set up a deep dive and present some of these ideas in more detail. We are also looking for forward thinking financial institutions to enroll in the Beta program for Compliance Agent - so join us to help create the future!"
  },
  {
    "objectID": "blog/2024-02-10-base-rate-neglect-in-aml/index.html",
    "href": "blog/2024-02-10-base-rate-neglect-in-aml/index.html",
    "title": "Base Rate Neglect in AML",
    "section": "",
    "text": "For decades, false positives have plagued Anti Money Laundering (AML) detection efforts, creating an industry-wide challenge. Stories abound of Investigation teams creaking under the ever increasing burden of noisy alerts. Addressing this ever growing volume of alerts is an industry unto itself.\nIn this post, I will argue that the problem may not be with the alerts or detection system itself but how we interpret what they mean and our failure to view them them through the right lens of probability.\nOne of the classic examples that highlight our inability to assess probabilities accurately is the medical test paradox.(see footnote for 3B1B’s excellent video)"
  },
  {
    "objectID": "blog/2024-02-10-base-rate-neglect-in-aml/index.html#the-medical-test-paradox-1",
    "href": "blog/2024-02-10-base-rate-neglect-in-aml/index.html#the-medical-test-paradox-1",
    "title": "Base Rate Neglect in AML",
    "section": "The Medical Test Paradox 1",
    "text": "The Medical Test Paradox 1\nAssume that you take a medical test with a sensitivity of 0.99 and a specificity of 0.9 . If the test comes back as positive, what is the probability you have the disease ?\nSensitivity or True Positive rate is the likelihood that the medical test (T) will return a positive result (+) if a patient has the disease(D).\n\\[ P(T = + \\mid D) = 0.99 \\] Specificity or True Negative Rate is the likelihood that the test will return a negative result if a patient does not have a disease.\n\\[ P(T = - \\mid \\neg D) = 0.9 \\]\nThe complement of the True Negative Rate or Specificity is the False Positive rate.\n\\[  P(T = +  \\mid \\neg D) = 1 - P(T = - \\mid \\neg D) = 1 - 0.9 = 0.1 \\] The True Positive Rate and False Positive Rates represent Sampling probabilities. Given a hypothesis or condition (e.g. the patient has a disease), it tells us how likely we are to observe a specific outcome (e.g. medical test is positive ).\nThe value we need to compute is \\(P(\\frac{D}{T = +})\\) . This is also known as the Positive Predictive Value. This is an Inferential Probability. Given an observation (medical test is positive), it tells how likely a hypothesis (patient has the disease) is.\nThis just requires a simple application of Bayes rule.\n\\[ P(D \\mid T = +) =  \\frac {P(T = + \\mid D) \\times P(D)}{P(T = +)}  \\]\nNote the denominator in the above expression simply evaluates the total probability of the test being positive which is give by.\n\\[ P(T = +) =  P(T = + \\mid D) \\times P(D) + P(T = + \\mid \\neg D) \\times P( \\neg D) \\]\nThis simply accounts for the probability that the test can be positive when the patient has the disease or is free of the disease.\nNote also that \\(P(T = + \\mid \\neg D) = 1 - P(T = - \\mid \\neg D)\\). We already know the value of the latter term (True Negative Rate).\nWe already have most of the values we need to compute the required probability. Plugging them below, we get\n\\[ P(D \\mid T = +) =  \\frac { 0.99 \\times P(D)}{ 0.99 \\times P(D) + ( 1- 0.9) \\times P( \\neg D)}  \\]\nThis leaves us with two terms we need to determine. \\(P(D)\\) is the prior probability you will assign to someone having a disease.\nNow if this is an extremely rare disease or if you are looking at a patient who is very unlikely to have this disease based on medical history or other demographic and socio-economic factors, you would assign a low prior probability.\nFor example, the prior for Bob having Covid when he has been sheltering in place for the last 1 month is really low. Say \\(P(D) = 0.01\\).\nOn the other hand, Alice has been travelling for the last month without wearing a mask, the prior for Alice would be quite high. Say \\(P(D) = 0.25\\)\nWith this information, we can calculate the posterior probability that a patient has the disease given a positive test.\nThis simple function implements the above formula.\nposterior_prob &lt;- function(prior,sensitivity,specificity){\n  \n  post &lt;- (sensitivity * prior)/(sensitivity*prior + (1- specificity)*(1 - prior))\n  return(post)\n  \n}\ncat(\"Probability of Bob having Covid is:\",round(posterior_prob(0.01,0.99,0.9),2))\n## Probability of Bob having Covid is: 0.09\ncat(\"Probability of Alice having Covid is:\",round(posterior_prob(0.25,0.99,0.9),2))\n## Probability of Alice having Covid is: 0.77\nAs you can see, even though the medical test has an ostensibly high accuracy,the likelihood that a patient testing positive has Covid is not nearly as high as one would expect.The priors also make a significant difference to the eventual diagnosis.\nI contend that AML models and scenarios are equivalent to medical tests. Just as medical tests tell us whether a patient has a disease, an AML model is supposed to tell us whether a customer is a money launderer by creating an alert or a case. Just as with medical tests, how we interpret the alert or case is important.\n\nPosterior Odds with Bayes Factors\nAn equivalent and perhaps more intuitive way of solving this problem is with odds rather than probabilities.To do this\n\nExpress the prior as odds. Given a probability \\(p\\), the odds are given by \\(\\frac{p}{1-p}\\)\n\nprob_to_odds &lt;- function(p) p/(1-p)\nFor Bob, the prior odds would be \\(\\frac{0.01}{0.99}\\).\nprob_to_odds(0.01)\n## [1] 0.01010101\nFor Alice, this would be \\(\\frac{0.25}{0.75}\\)\nprob_to_odds(0.25)\n## [1] 0.3333333\n\nCalculate the Bayes Factor.\n\nThis is the ratio of the sensitivity(TPR) of the test to the FPR. \\[ BF = \\frac{Sensitivity}{FPR} =  \\frac{P(T = + \\mid D)}{P(T = + \\mid \\neg D)}  \\] The simple function below computes the Bayes Factor.\nbf&lt;- function(sensitivity,specificity){\n  return(sensitivity/(1 - specificity))\n}\nThe BF for the test in question is given by\nround(bf(0.99,0.9),2)\n## [1] 9.9\n\nCalculate the posterior odds by multiplying the prior odds with the Bayes Factor.\n\ncat(\" The odds of Bob having Covid is:\",round(prob_to_odds(0.01)*bf(0.99,0.9),2))\n##  The odds of Bob having Covid is: 0.1\ncat(\" The odds of Alice having Covid is:\",round(prob_to_odds(0.25)*bf(0.99,0.9),2))\n##  The odds of Alice having Covid is: 3.3\nGiven odds of \\(o\\), the probability \\(p\\) is given by \\(\\frac{o}{1+o}\\).\nWe can confirm the odds compute here are same as the probabilities computed above.\nodds_to_prob &lt;- function(o) return(o/(1+o))\ncat(\"Probability of Bob having Covid is:\",round(odds_to_prob(0.1),2))\n## Probability of Bob having Covid is: 0.09\ncat(\"Probability of Alice having Covid is:\",round(odds_to_prob(3.3),2))\n## Probability of Alice having Covid is: 0.77\nJust as the medical test paradox sheds light on common misinterpretations of probability, the Prosecutor’s Fallacy illustrates these challenges in a legal context, further emphasizing the need for probabilistic thinking in AML."
  },
  {
    "objectID": "blog/2024-02-10-base-rate-neglect-in-aml/index.html#the-prosecutors-fallacy",
    "href": "blog/2024-02-10-base-rate-neglect-in-aml/index.html#the-prosecutors-fallacy",
    "title": "Base Rate Neglect in AML",
    "section": "The Prosecutor’s Fallacy",
    "text": "The Prosecutor’s Fallacy\nIt is not just doctors, but also lawyers, juries and judges who have sometimes failed to interpret probabilities correctly.\nIn the 1968 case, People vs Collins , Malcolm and Janet Collins, an interracial couple, were arrested for robbery as their characteristics matched those given by an eye witness.\nThe prosecution made the argument that the chances of the couple just happening to be at the crime scene by pure chance was extremely low.\nThe probability of observing each of the couple’s characteristics were given as follows.\n\nTable 1: Probability of seeing a match with characteristics seen by Eye witness\n\n\nObservation\nProbability\n\n\n\n\nMan with mustache\n1/4\n\n\nWoman with blonde hair\n1/3\n\n\nWoman with pony tail\n1/10\n\n\nAfrican American man with beard\n1/10\n\n\nInterracial couple in a car\n1/1000\n\n\nPartly yellow car\n1/10\n\n\n\nConsider the hypothesis (H) that the couple is innocent. Under this hypothesis, the probability (D) that an eyewitness would see a couple with the specified characteristics is astonishingly low, calculated as 1 in 12 million—a product of the individual probabilities detailed above. Note this assumes these characteristics are independent which is almost certainly not true and actually underestimates the probability.\nThe couple were initially convicted based on this argument.If the likelihood of them being innocent is so small, then they ought to be guilty.\nThe problem is that this probability gives the probability of observing a couple with the given characteristics under the hypothesis that the couple were innocent.\nThis is P(D|H), the sampling probability.\nHowever, what we are really interested in is the probability that the hypothesis is true given we observed a couple with the aforementioned characteristics - P(H|D) or the inferential probability.\nThis can again be calculated using Bayes rule as follows\n\\[ P(H \\mid D) =  \\frac {P(D\\mid H) \\times P(H)}{P(D)}  \\] where\n\\[ P(D) = P( D \\mid H) \\times P(H) + P( D \\mid \\neg H) \\times P( \\neg H) \\]\nNow we have to determine priors - \\(P(H)\\) , the prior probability that the couple is innocent and \\(P( \\neg H)\\) , the prior probability that the couple is guilty. Note these are to determined before we observe any evidence i.e. the eye witness account.\nGiven there were some 5 million couples in the Los Angeles area where the crime was committed, and any one of these couples could have committed a crime, a reasonable prior is\n\\[ P(\\neg H) = \\frac{1}{5,000,000}  \\]\nThis implies\n\\[ P(H) = \\frac{4,999,999}{5,000,000}\\]\nThe other term we need to determine is \\(P(\\frac{D}{\\neg H})\\), This is the probability of observing the stated characteristics in table 1, if the couple were guilty. This will be 1.\nPlugging these values in, the posterior probability that the couple is innocent is given by\n\\[ P(H \\mid D) =  \\frac {\\frac{1}{12,000,000} \\times \\frac{4,999,999}{5,000,000}}{ \\frac{1}{12,000,000} \\times \\frac{4,999,999}{5,000,000} + 1 \\times \\frac{1}{5,000,000}} = 0.294 \\]\nThe posterior probability hat the couple is innocent is actually close to 30% rather 1 in 12,000,000. This certainly does not prove anything beyond a reasonable doubt.\nYou will be glad to know the Collinses’ were acquitted by the California Supreme Court on appeal where the presiding judge commented :\n\nUndoubtedly the jurors were unduly impressed by the mystique of the mathematical demonstration but were unable to assess its relevancy or value"
  },
  {
    "objectID": "blog/2024-02-10-base-rate-neglect-in-aml/index.html#why-this-matters-in-aml",
    "href": "blog/2024-02-10-base-rate-neglect-in-aml/index.html#why-this-matters-in-aml",
    "title": "Base Rate Neglect in AML",
    "section": "Why this matters in AML",
    "text": "Why this matters in AML\nMy contention is that AML practitioners also fall victim to base rate fallacy. In place of intervening medically when not needed, or convicting innocent people, AML practitioners tend to investigate cases or file SARs when there isn’t compelling evidence to do so.\nMuch like in the case against the Collinses, we tend to make decisions based on sampling probabilities rather than inferential probabilities. The chances of all these models triggering an alert on a regular customer is really low, so if they do alert, the customer must be worth investigating.\nConsider how transaction monitoring is typically done at most institutions. The transaction activity of a customer is monitored for patterns like Change in Behavior or Rapid Movement or Rapid Flow Through of Funds. If a customer triggers alerts on some combination of these scenarios, a case is created.\nSome institutions are so conservative that a case is created if an alert is triggered for just one of these scenarios. For others, a case is created only if some combination of these scenarios triggers an alert.\nLet us assume under regime 1. we create and investigate a case when any one of these scenarios triggers an alert. Under regime 2 on the other hand, we create a case only when all three scenarios trigger an alert.\nNow each of these scenarios are equivalent to a medical test. When a medical test returns a positive result, a physician can choose to intervene. Similarly, when a scenario triggers an alert, an investigator can choose to intervene and investigate the customer. The key question is how much evidence is sufficient to justify this intervention.\nLet us assume both institutions use three different scenarios or models - Scenario A (Change in Behavior) ,Scenario B (Rapid Flow Through of Funds) , Scenario C (High Risk CounterParty), with the goal of detecting Shell Accounts that might be operating through the institution.\nThe hypothesis here is that a shell company will demonstrate these underlying patterns.\nI will also assume the scenarios alert independent of each other and make the following assumptions for each scenario.\nProbability of a customer showing target pattern given customer is a shell company = 0.99  Probability of a customer showing target pattern given customer is not a shell company = 0.01  Probability a scenario alerts in the presence of the pattern = 0.9  Probability a scenario alerts in the absence of the target pattern: 0.05\nFor each of the models, the performance of the scenario is as follows\nSensitivity2 is 0.8915 (0.9 x 0.99 + 0.05x(1 - 0.99)). If the customer is truly a criminal, this is the probability that a model triggers and an alert.\nSpecificity3 is 0.9415 (1 - 0.9 x 0.01 + 0.05*(1-0.01)). If the customer is a regular citizen, this is the probability that the model does not alert.\n\nTable 2: Accuracy of scenarios\n\n\nScenario\nSensitivity\nSpecificity\n\n\n\n\nScenario A\n0.8915\n0.9415\n\n\nScenario B\n0.8915\n0.9415\n\n\nScenario C\n0.8915\n0.9415\n\n\n\nNote this is the performance you would see only with a high quality scenario.\nNow,a critical question. What is the prior probability that a customer at either Financial Institution (FI) is a bad actor or criminal.\nLet us assume FI 1 is a regional bank in the US mid west. Here the prior probability of any given customer being a money launderer is very small. Say 1/1,000,000.\nLet us assume FI 2 is an international bank offering services in tax havens and high risk jurisdictions, here the prior would be an order of magnitude bigger. Say 1/100,000\n\nFI 1 under regime 1\nHere, we have an FI with a low risk customer base which also has a very conservative approach to reviewing cases.\nThe posterior probability of an alerted customer being truly suspicious is truly minuscule.\n#Prior odds x Bayes factor for one scenario\nodds_to_prob(prob_to_odds(1/1000000)* bf(0.8915,0.9415))\n## [1] 1.52391e-05\n\n\nFI 2 under regime 1\nWhat about the FI with a higher risk customer base ?\nEven here, a conservative approach to investigating cases leads to an extremely low posterior probability of a case being truly effective, albeit three orders of magnitude higher than for FI 1.\n#Prior odds x Bayes factor for one scenario\nodds_to_prob(prob_to_odds(1/1000)* bf(0.8915,0.9415))\n## [1] 0.01502537\n\n\nFI 1 under regime 2\nWhat about FI 1 with a less conservative approach to creating cases ? It gets only marginally better.\n#Prior odds x Bayes Factor for three independent scenarios\nodds_to_prob(prob_to_odds(1/1000000)* bf(0.8915,0.9415)^3)\n## [1] 0.003526652\n\n\nFI 2 under regime 2\nWhat about FI 2 with a less conservative approach to creating cases ?\n#Prior odds x Bayes Factor for three independent scenarios\nodds_to_prob(prob_to_odds(1/1000)* bf(0.8915,0.9415)^3)\n## [1] 0.7798652\nOnly in this case does, even such a set of high quality scenarios yields cases that are truly likely to result in a SAR."
  },
  {
    "objectID": "blog/2024-02-10-base-rate-neglect-in-aml/index.html#takeaways",
    "href": "blog/2024-02-10-base-rate-neglect-in-aml/index.html#takeaways",
    "title": "Base Rate Neglect in AML",
    "section": "Takeaways",
    "text": "Takeaways\nHere is what I think we should do differently to improve the efficiency of AML Detection programs.\n\nInstead of creating cases based on binary rules or models, models or rules should output probabilities that represent the posterior probability of a case being truly effective.\nInstitutions can then choose to review cases only when the posterior probability is greater than some threshold which they can choose based on their risk tolerance. If they choose to review cases with a very low posterior probability as FI 1 chooses to do, then they should expect a lot of false positives.\nAlternately,institutions can wait until more evidence accumulates before investigating a case. This can lead to creation of higher quality cases\nCreate better segmentation. A good risk based segmentation system can create segments where the prior probability is much higher which in turn can lead to higher quality cases.\nCreate better features. This is a no brainer. Features that can more accurately detect the underlying patterns of interest will lead to improved detection. Deriving features from graph representations of underlying transaction data or time series data are promising avenues to consider.\nAssign importance to alerts based on how discriminating the underlying patterns are. If a scenario monitors for highly generic patterns that can manifest for both regular customers and bad actors, alerts will be very noisy by design. The weight given to alerts from such scenarios should be low given how noisy they can be.\nCreate richer cases by collecting as many signals as you can. Transaction data can only tell you so much. Incorporate signals from external data sources such as negative news stories or ICIJ data sets to improve the quality of cases."
  },
  {
    "objectID": "blog/2024-02-10-base-rate-neglect-in-aml/index.html#references-and-resources",
    "href": "blog/2024-02-10-base-rate-neglect-in-aml/index.html#references-and-resources",
    "title": "Base Rate Neglect in AML",
    "section": "References and Resources",
    "text": "References and Resources\n\nBernoulli’s Fallacy: Statistical Illogic and the Crisis of Modern Science"
  },
  {
    "objectID": "blog/2024-02-10-base-rate-neglect-in-aml/index.html#footnotes",
    "href": "blog/2024-02-10-base-rate-neglect-in-aml/index.html#footnotes",
    "title": "Base Rate Neglect in AML",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nhttps://www.youtube.com/watch?v=lG4VkPoG3ko↩︎\nP(S=Alert | Cust = Criminal) = P(S=Alert |(Cust shows Pattern) x P(Cust Shows Pattern |Cust = Criminal) + P(S =Alert |Cust shows no pattern) x P(Cust shows no pattern | Cust = Criminal)↩︎\n1 - P(S=Alert | Cust = Citizen) = 1 - P(S=Alert |(Cust shows Pattern) x P(Cust Shows Pattern |Cust = Citizen) + P(S =Alert |Cust shows no pattern) x P(Cust shows no pattern | Cust = Citizen)↩︎"
  },
  {
    "objectID": "blog/2023-10-15-the-german-tank-problem/index.html",
    "href": "blog/2023-10-15-the-german-tank-problem/index.html",
    "title": "The German Tank Problem",
    "section": "",
    "text": "Towards the end of World War II, the Germans introduced the Panther V, a successor to the Panther III and Panther IV models.Ahead of the Normandy invasion,the Allies wanted to estimate how many of these tanks they might encounter during battle. The only information they had to estimate this was the serial number of the parts of tanks they had captured or destroyed.\nBased on just this information, Allied statisticians provided remarkably accurate estimates of German tank production as confirmed from documents captured at the end of the war.1\n\n\n\nFigure 1: Comparison of Estimates and Actual Figures\n\n\nThis post attempts to present a solution to this problem using both frequentist and bayesian approaches."
  },
  {
    "objectID": "blog/2023-10-15-the-german-tank-problem/index.html#the-frequentist-approach",
    "href": "blog/2023-10-15-the-german-tank-problem/index.html#the-frequentist-approach",
    "title": "The German Tank Problem",
    "section": "The Frequentist Approach",
    "text": "The Frequentist Approach\n\nEstimator\nA statistic is a function that maps a sample of data to a single number, it is a quantity derived from the data. An estimator is a statistic used to estimate a population parameter. For example, if we want to estimate the mean of a population from a sample, the sample mean and the median are valid estimators.\nFor the German tank problem, a good estimator turns out to be2:\n\\[ \\hat{N} = m \\cdot \\frac{k+1}{k} - 1  \\]\nwhere \\(m = Max(n_1,\\dots,n_k)\\)\nA good way to sanity check this estimator is by evaluating the boundary conditions.\n\nIf you observe only one tank, \\(k =1\\), with a serial number \\(n_1\\), then \\(\\hat{N} = 2n_1 -1\\). i.e. the total number of tanks is double the serial number you observed,which makes sense, since you are more likely to observe a tank near the mean of the distribution.\nIf you observe all the tanks, \\(k = N\\), with the maximum serial number of \\(n_N\\) which is same as \\(N\\),then \\(\\hat{N} = N \\cdot \\frac{N+1}{N} - 1 = N\\). This means the estimate is again correct.\n\nLet the tank numbers observed be 324,167,129 and 418.Then the estimated number of tanks are:\nobs &lt;- c(324,167,129,418)\nk &lt;- length(obs)\nmax_ &lt;- max(obs)\n\nmax_*((k+1)/k) - 1\n## [1] 521.5\nThis estimator also has the nice property of being the Minimum Variance Unbiased Estimator. In the below two sections, I take a quick detour to understand what these term means. Feel free to skip over these sections.\n\nUnbiased Estimator\nIf an estimator is unbiased, the expectation or expected value of the estimator equals the true value of the parameter.\nLet \\(t_n\\) be an estimator at a given sample size \\(n\\) of the true parameter \\(\\theta\\). If the estimator is unbiased\n\\[ E[t_n] = \\theta \\] It may be easier to illustrate this idea by considering Variance. Now, Variance is a measure of dispersion that almost everyone is familiar with. It is given by :\n\\[ Var(X) = \\frac{1}{n} \\sum_{i=1}^{n}(x_i - \\mu)^2   \\]\nDespite the above definition of variance, the unbiased estimate of variance is given by.3\n\\[\\hat{Var(X)} = \\frac{1}{n - 1} \\sum_{i=1}^{n}(x_i - \\mu)^2 \\] Consider a population given by \\(N(0,1)\\). i.e. a Normal Distribution with mean = 0 and variance = 1.\nWe will evaluate both formulations of variances, the one that has an expected value equal to the true variance (=1) is the unbiased sample variance.\nset.seed(0)\n# Take 1000 draws of size 10 from the population\nsamples &lt;- replicate(1000, rnorm(10,0,1))\n\n# Function to calculate both version of variance\nvar_ &lt;- function(s){\n  mean &lt;- mean(s)\n  n &lt;- length(s)\n  var_unbiased &lt;- 1/(n-1)*sum((s - mean)^2)\n  var_biased &lt;- 1/(n)*sum((s - mean)^2)\n  return(c(\"var_unbiased\" = var_unbiased,\"var_biased\" = var_biased))\n}\n\n#Calculate sample variances\n\nvariances &lt;- apply(samples,2,var_)\n\n#Calculate expected value of both sampling distributions\nvariances_expected &lt;- apply(variances,1,mean)\nvariances_expected\n## var_unbiased   var_biased \n##    0.9952352    0.8957116\nAs you can see the unbiased sample variance is much closer to the true variance than the biased sample variance.\n\n\nMinimum Variance\nA minimum variance unbiased estimator is guaranteed to have the lowest possible variance among all unbiased estimators.\nConsider the sample mean and the sample median. Both are unbiased estimators of the population mean.\nAgain, consider a population given by \\(N(0,1)\\).\nset.seed(0)\n# Take 1000 draws of size 10 from the population\nsamples &lt;- replicate(10000, rnorm(10,0,1))\n\n# Function to calculate  mean and median\nm &lt;- function(s){\n  mean_ &lt;- mean(s)\n  median_ &lt;- median(s)\n  \n  return(c(\"mean\" = mean_,\"median\" = median_))\n}\n\n#Calculate sample means and sample medians\n\nms &lt;- apply(samples,2,m)\n\n#Calculate expected value of both sampling distributions\nms_expected &lt;- apply(ms,1,mean)\nms_expected\n##        mean      median \n## 0.001409647 0.003353574\nWe see that both are fairly close to the population mean. But if we evaluate the variance as shown below.\n#Calculate variance of  sampling distributions\napply(ms,1,var)\n##       mean     median \n## 0.09868766 0.13808784\nWe see that the variance for the sample median is much higher.This shows that the sample mean, rather than the sample median is the minimum variance estimator of the population mean.\n\n\n\nConfidence Intervals\nWe have an estimator to obtain a point estimate for the Number of tanks. We also want to construct a confidence interval around this estimate with some significance level( \\(\\alpha = 5\\)%)\nThe lower bound for the Number of tanks is given by \\(m\\), the maximum number we observe, as it is certain that the enemy has at least \\(m\\) tanks.\nWe have to establish an upper bound for the 95% ( \\(1 - \\alpha\\)) confidence interval.\nTo get to this we must answer the following question.\nGiven a collection of \\(k\\) integers, \\(n_1,n_2,\\dots,n_k\\) sampled independently and uniformly (with replacement) from the range \\(1\\) to \\(N\\), what is the probability that the maximum value we observe i.e. \\(Max(n_1,\\dots,n_k)\\) is \\(\\geq\\) m ?\nThe probability of observing a maximum value less than or equal to \\(m\\), or the cumulative probability mass function is given by \\(P [Max \\leq m ] =(\\frac{m}{N})^k\\) i.e. all tanks observed should have a number less than \\(m\\)\nThe probability of observing a maximum value greater than \\(m\\) is given by:\n\\[ P [Max \\gt m ]  = 1 - (\\frac{m}{N})^k \\]\nWe can determine the upper bound of the confidence interval by setting above value equal to \\(1 - \\alpha\\) and solving for \\(N\\) which gives:\n\\[ N_{1 - \\alpha} = \\frac{m}{(\\alpha)^\\frac{1}{k}}   \\]\nWe can conduct an experiment to validate the results as shown below. Assume there are a hundred different spies who each observe some tanks and record the serial numbers. From each spy’s observation we can estimate the total number of tanks and construct a confidence interval around each estimate.\n# True number of tanks\nN &lt;- 500\n\n# Number of tanks observed\nk &lt;- 4\n\n\n# Assume a 1000 different spies observe tanks and record the maximum number they observe.\n\nset.seed(1)\n##Function to simulate k tanks observed by a spy\nf &lt;-function(N,k){sample(c(1:N),k,replace = TRUE)}\n\n# Record the tank numbers observed by a 1000 spies\nobs &lt;- replicate(1000,f(N,k))  \n\n# Get maximum observed by each spy\nmax_ &lt;- apply(obs,2,max)\n\n# Get estimate of Number of tanks\nN_hat = max_*(k+1)/k -1\nhist(N_hat)\n\nLet us extract the maximum number observed by the first spy and the estimated number of tanks based on this observation.\n## The tank numbers observed by the first spy are : 324 167 129 418\n## The maximum tank number observed by the first spy is 418\n## The estimated number of total tanks based on this number is 521.5\nNow we can also calculate a 95% confidence interval around each of these estimates.\nalpha &lt;- 0.05\nN_95 &lt;- floor((max_)/(alpha)^(1/k))\n\nci &lt;- rbind('Lower_Bound'= max_,'Upper_Bound'=N_95)\nThe confidence interval around the estimate from the first spy’s observations is given by\nci[,1]\n## Lower_Bound Upper_Bound \n##         418         883\nBy definition, if we calculate a 95% confidence interval around an estimate, 95% of calculated confidence intervals should contain the true value.\nWe can calculate the number of confidence intervals containing the true value of 500.\nbool = ci[1,] &lt;= N & ci[2,]&gt;= N\nsum(bool)\n## [1] 951\nThis shows that approximately \\(1 - \\alpha = 0.95\\), or 95% of confidence intervals contain the true value which is consistent with the frequentist definition of confidence intervals.\nWe can plot the estimate and confidence intervals as shown below.\nlibrary(ggplot2)\nlibrary(dplyr)\n# Construct a full data frame for plotting\n\ndf &lt;- data.frame(cbind(c(1:(ncol(ci))),N_hat,t(ci)))\ncolnames(df) &lt;- c(\"N\",\"Estimate\",\"Lower_Bound\",\"Upper_Bound\")\n\ndf %&gt;% head(5) %&gt;%\nggplot(aes(x=N,y = Estimate)) + geom_point() + geom_errorbar(aes(ymin=Lower_Bound, ymax = Upper_Bound))\n\n\nP- value\nWe have defined a confidence interval above. but how does it relate to a p-value?\nConsider that there are 500(N=500) tanks the Germans had in battle. The probability mass function of the maximum serial number observed \\(m\\), for \\(k\\) observation given \\(N\\) tanks exist is given by\n\\[  P [Max = m ] =  \\begin{cases}\n    (\\frac{m}{N})^k - (\\frac{m-1}{N})^k,& \\text{if }  m\\leq N\\\\\n    0,              & \\text{otherwise}\n\\end{cases}\\]`\nThis can be plotted as shown below.\npmf &lt;- function(m,N,k){\n  return((m/N)^k - ((m-1)/N)^k)\n}\n\nm_ = c(1:N)\n\npmf_ &lt;- sapply(m_,pmf,N,k)\n\nplot(m_,pmf_,type=\"l\",xlab = \"Max\", ylab=\"probability density\")\n\nNow we have to define a rejection region. If the maximum \\(m\\) we observe is within this rejection region, we reject the hypothesized value of \\(N\\).\nThe confidence interval we calculated corresponds to defining a rejection region on the left tail of the distribution.\nAssuming a \\(\\alpha (=5 \\%)\\) significance level, this rejection region can be determined by calculating the value of \\(m\\) where the cumulative mass function has a value of \\(\\alpha\\).\n\\[ (\\frac{m'}{k})^N = \\alpha \\]\n\\[ m' = \\alpha^{\\frac{1}{k}}N  \\]\nrejection_region_max = floor(alpha^(1/k) * N)\nrejection_region_max\n## [1] 236\nThe rejection region is given by the shaded area in the figure below. \nSo if the maximum serial number we observe \\(m\\) is less than 236, we can reject the null hypothesis, that the actual number of tanks is 500.\nAs noted in the earlier section, the maximum tank number observed by the first spy is 418. So, we do not reject the null hypothesis that the actual number of tanks is 500. Now, what are the possible values of N, where we would reject the null hypothesis given we observed a maximum of 418 ?\nWell,what about N = 417 ? If there were only 417 tanks in total, we would not observe a tank with a serial number of 418, so we can rule out 417 as well as all values of N between 1 and 417\nThen, what bout N = 883. We would reject this null hypothesis if we observe a maximum tank number less than:\ncat(round(alpha^(1/k) * 883,2))\n## 417.54\nGiven the spy observe a tank number of 418, we don’t reject this null hypothesis either.\nWhat about N =884,We would reject this null hypothesis if we observe a maximum tank number less than:\ncat(round(alpha^(1/k) * 884,2))\n## 418.02\nOK - So we did observe a tank number less than 418.02, so we reject this null hypothesis. We do the same for all hypothesized values of N greater than 884.\nSo the range of hypothesized values of N for which we don’t reject the null hypothesis is [418,883]. This is exactly the confidence interval computed above!\n\n\nSignificance Level\nFor both confidence intervals and p values, I specified a significance level \\(\\alpha  = 5 \\%\\). This is simply the likelihood of rejecting a null hypothesis when it is true i.e. the probability of a false positive.\nGiven the distribution of \\(m\\) for a hypothesized value of \\(N\\) in Figure X, there is still an \\(\\alpha\\)% chance that we will observe a value of \\(m\\) in the rejection region and incorrectly reject the null hypothesis.\nSimilarly, there is an \\(\\alpha  = 5\\)% chance that the true value of \\(N\\) falls outside the \\(1 -\\alpha = 95\\)% confidence interval.\nNow you might wonder what is the probability of a false negative. This is related to the power of a statistical test which I will not explore in this blog. The references at the end of the blog will be useful if you want to explore this idea.\n\n\n\nA more conservative confidence interval\nNow let us assume we want to be a bit more risk averse about underestimating the number of tanks. This may be because the war is in its early stage when German manufacturing capabilities had not been severely depleted by Allied bombing.\nIf the maximum we observe is \\(m\\), we want the confidence interval to be conservative and be defined by a range starting at \\(m+i\\), where \\(i\\) is some whole number, rather than \\(m\\).\nIn other words, we want the 95% confidence interval to capture a range more like the upper image rather than the lower image.\n\nFirst, we want to first establish the lower bound of this new confidence interval.\nAs established earlier, the probability of observing a maximum value greater than \\(m\\) is given by:\n\\[ P \\big( Max \\gt m \\big)  = 1 - (\\frac{m}{N})^k \\]\nBy setting above value equal to \\(\\alpha\\) and solving for \\(N\\) gives:\n\\[ N_{\\alpha} = \\frac{m}{(1 -\\alpha)^\\frac{1}{k}}   \\]\nThis gives the lower bound of the confidence interval.\nNow, what is the upper bound of the confidence interval ? The upper bound turns out to be \\(\\infty\\).\nWe can again conduct an experiment to validate the results as shown below.\nWe can calculate a new 95% confidence interval around each estimate as shown below.\nalpha &lt;- 0.05\nN_05 &lt;- ceiling((max_)/(1- alpha)^(1/k))\n\nci_desired &lt;- rbind(N_05,rep(Inf,N))\nCalculate number of confidence intervals containing the true value of 10000.\nbool = ci_desired[1,] &lt;= N & ci_desired[2,]&gt;= N\nsum(bool)\n## [1] 943\nThis shows that approximately \\(1 - \\alpha = 0.95\\), or 95% of confidence intervals contain the true value which is again consistent with the frequentist definition of confidence intervals.\nThe confidence interval around estimated number of tanks observed by the first spy is:\ncat(\"[\",ci_desired[1,1],\",\",round(ci_desired[2,1],0),\"]\")\n## [ 424 , Inf ]\n\nP-value\nHow does this new confidence interval relate to p-values.\nThis new confidence interval we define simply corresponds to defining a rejection region on the right tail of the distribution.\nAssuming a \\(\\alpha (=5 \\%)\\) significance level, this new rejection region can be determined by calculating the value of \\(m\\) where the cumulative probability mass function has a value of \\(1 - \\alpha\\)\n\\[ (\\frac{m'}{N})^k =  1 -\\alpha \\] \\[ m' = (1 -\\alpha)^{\\frac{1}{k}}N  \\] Let our null hypothesis be that the number of tanks is 500, then the rejection region is given by the shaded region in the figure below.\nrejection_region_min = (1-alpha)^(1/k) * N\nrejection_region_min\n## [1] 493.6293\n\nSo if the maximum serial number we observe \\(m\\) is greater than 493, we can reject the null hypothesis \\(H0: N = 500\\)\nAs noted in earlier section, the maximum tank number observed by the first spy is 418. So, we do not reject the hypothesis that the actual number of tanks is 500. Now what are the possible values of N, where we would reject the null hypothesis given we observed a maximum of 418 ?\nWhat about N = 423. We would reject this null hypothesis if we observe a maximum tank number greater than :\ncat((1 - alpha)^(1/k) * 423)\n## 417.6104\nGiven the maximum we observed is 418, we reject this null hypothesis.\nWhat about N = 424. We would reject this null hypothesis if we observe a maximum tank number greater than :\ncat((1 - alpha)^(1/k) * 424)\n## 418.5976\nGiven the spy observed a tank number of 418, we don’t reject this null hypothesis either.\nWhat about N =10000, We would reject this null hypothesis if we observe a maximum tank number greater than :\ncat((1 - alpha)^(1/k) * 10000)\n## 9872.585\nGiven the spy observe a tank number of 418, we don’t reject this null hypothesis either.It follows that we won’t reject the null hypothesis for any value of N greater than 424.\nSo the range of hypothesized values of N for which we don’t reject the null hypothesis is \\(\\[424,\\infty\\]\\). This is exactly the confidence interval computed above.\n\n\n\nLimitations of the Frequentist Approach\nNot withstanding the fact that the definition of confidence intervals is confusing 4, we see that the two ways of computing the confidence interval can give answers of wildly varying utility. There is also no technical reason why one confidence interval is preferable compared to the other."
  },
  {
    "objectID": "blog/2023-10-15-the-german-tank-problem/index.html#the-bayesian-approach",
    "href": "blog/2023-10-15-the-german-tank-problem/index.html#the-bayesian-approach",
    "title": "The German Tank Problem",
    "section": "The Bayesian Approach",
    "text": "The Bayesian Approach\nAn alternative to the Frequentist Approach we covered above is a Bayesian approach 5. Just like we calculated confidence intervals, we can can calculate a credible interval for the number of tanks the enemy has, given the tank serial numbers observed by a spy.\nAlthough this simple enough problem to solve analytically 6, a more general approach is to solve this using a Markov Chain Monte Carlo sampler. This can be done by using the software package JAGS and the R interface to JAGS rjags.\nLet us consider the tanks observed by the first spy.\ny &lt;- obs[ ,1]\ny\n## [1] 324 167 129 418\nFo this analysis, we will use a set of utility functions available as part of the book Doing Bayesian Data Analysis by John Kruschke.\nsource(\"utilities.R\")\n## Loading required package: coda\n## Linked to JAGS 4.3.1\n## Loaded modules: basemod,bugs\nrequire(rjags)\nrequire(coda)\nFirst, the data to be analyzed is packaged into a list to be shipped to the JAGS sampler.\n# Put data into a list\n\ndataList &lt;- list(\n  y = y, # Observations\n  Ntotal = length(y), # No of observations\n  y_max = max(y) # Maximum tank number observed\n)\nTo complete a Bayesian analysis, in addition to the likelihood function we covered at the beginning of this post, we also need to define a prior.\nThe prior should be informed by our understanding of the problem and the domain.\nTo begin with, given our estimates of German production capacity towards the end of the war, we might know that the Germans have no more than 1000 tanks. Given we have no further information, we will assume a uniform prior i.e. any number of tanks between the maximum observed by the spy and 1000 is equally likely.\nWe can specify this models as shown below.\n#Define the model\nlibrary(rjags)\n\nmodel0 = \"\nmodel {\n  for ( i in 1:Ntotal ) {\n    y[i] ~ dunif(0,N) #Likelihhod\n  }\n  N ~ dunif( y_max, 1000) #Prior\n}\"\nIf this were the beginning of the war, we would think the Germans have a much higher production capacity. We would revise the top end of our prior estimate from 1000 to 10000.\nmodel1 = \"\nmodel {\n  for ( i in 1:Ntotal ) {\n    y[i] ~ dunif(0,N) #Likelihhod\n  }\n  N ~ dunif( y_max, 10000) #Prior\n}\"\nAt the end of the war, we might even have more information on the German production capacity that allows us to have a more precise prior. For e.g. we might have a prior belief think the number of tanks the enemy has is characterized by the following distribution.\nhist(rgamma(1000,25,0.05),main = \" Prior belief at end of war\")\n\nWe can define a model with this more precise prior as shown below.\n#Define the model\nlibrary(rjags)\n\nmodel2 = \"\nmodel {\n  for ( i in 1:Ntotal ) {\n    y[i] ~ dunif(0,N) #Likelihhod\n  }\n  N ~ dgamma(25,0.05) #Prior\n}\""
  },
  {
    "objectID": "blog/2023-10-15-the-german-tank-problem/index.html#run-models",
    "href": "blog/2023-10-15-the-german-tank-problem/index.html#run-models",
    "title": "The German Tank Problem",
    "section": "Run Models",
    "text": "Run Models\nFor each of these priors, we can now run MCMC samplers to infer what the posterior distribution of tanks is given the tank numbers observed by a spy.\nWhen running the chains, it is typical to specify a burn in period so as to discard the few initial values before the chain has settled on the true posterior. The number of steps to be taken to tune/adapt the samplers can also be specified.\nparameters = c( \"N\")     # The parameters to be monitored\nadaptSteps =  500        # Number of steps to adapt/tune the samplers\nburnInSteps = 500        # Number of steps to burn-in the chains\nnChains = 4              # nChains should be 2 or more for diagnostics\nThe three models can now be run and analyzed.\n\nModel 0 - End of War with limited intelligence\n# Run the chains:\njagsModel0 = jags.model(file= textConnection(model0) , data=dataList , \n                        n.chains= nChains , n.adapt= adaptSteps )\n## Compiling model graph\n##    Resolving undeclared variables\n##    Allocating nodes\n## Graph information:\n##    Observed stochastic nodes: 4\n##    Unobserved stochastic nodes: 1\n##    Total graph size: 9\n## \n## Initializing model\n# Burn in to stabilize the chains\nupdate( jagsModel0 , n.iter= burnInSteps )\n# Run chains to estimate posterior distribution\ncodaSamples0 = coda.samples( jagsModel0 , variable.names=c(\"N\") ,\n                            n.iter=2500 )\nNote that running the diagnostics on the sampled chains is an important part of the Bayesian analysis. See this post to learn more about diagnostics.\nThe posterior distribution resulting from this model can now be analyzed.\npar( mar=c(3.5,0.5,2.5,0.5) , mgp=c(2.25,0.7,0) )\nplotPost( codaSamples0[,\"N\"] , main=\"N\" , xlab=\"N\" )\n\n##        ESS     mean  median   mode hdiMass   hdiLow  hdiHigh\n## N 2036.402 556.6577 512.555 446.73    0.95 418.0032 841.3058\nWe see that the bayesian inference procedure gives us a credible interval comprising the values with the highest density. Rather than a confidence interval where the information we have is limited to whether the true value lies inside or outside the confidence interval, this credible interval gives the posterior probability of the parameter value.\nWe can also estimate the probability that the actual Number of tanks is between any two values. For example, the probability that the number of tanks is between 418 and 500 is given by.\nposterior &lt;- unlist(codaSamples0)\nprob_419_500 &lt;- sum((posterior&gt;= 419 & posterior &lt;= 500))/length(posterior)\ncat(\"The probability that the number of tanks is between 419 and 500 is:\",prob_419_500)\n## The probability that the number of tanks is between 419 and 500 is: 0.4487\n\n\nModel 1 - Beginning of war with limited intelligence\nRun chains for Model 1.\n# Run the chains:\njagsModel1 = jags.model(file= textConnection(model1) , data=dataList , \n                        n.chains= nChains , n.adapt= adaptSteps )\n## Compiling model graph\n##    Resolving undeclared variables\n##    Allocating nodes\n## Graph information:\n##    Observed stochastic nodes: 4\n##    Unobserved stochastic nodes: 1\n##    Total graph size: 9\n## \n## Initializing model\n# Burn in to stabilize the chains\nupdate( jagsModel1 , n.iter= burnInSteps )\n# Run chains to estimate posterior distribution\ncodaSamples1 = coda.samples( jagsModel1 , variable.names=c(\"N\") ,\n                            n.iter=2500 )\nThe posterior distribution resulting from this model can now be analyzed.\npar( mar=c(3.5,1.5,2.5,0.5) , mgp=c(2.25,0.7,0) )\nplotPost( codaSamples1[,\"N\"] , main=\"N\" , xlab=\"N\", xlim = c(100,2000) )\n\n##        ESS     mean   median     mode hdiMass   hdiLow  hdiHigh\n## N 241.2465 643.6212 527.8942 454.1553    0.95 418.0154 1213.298\nIn this case, we see that the posterior distribution is much wider than in Model 0.\nThis is fully expected given we are at the beginning of the war where German production capabilities are fully intact. Even though the highest tank number we observed was 418, we expect the enemy to have a much larger number of tanks then towards the end of the war.\n\n\nModel 2 - End of war with superior intelligence\nRun chains for Model 2.\n# Run the chains:\njagsModel2 = jags.model(file= textConnection(model2) , data=dataList , \n                        n.chains= nChains , n.adapt= adaptSteps )\n## Compiling model graph\n##    Resolving undeclared variables\n##    Allocating nodes\n## Graph information:\n##    Observed stochastic nodes: 4\n##    Unobserved stochastic nodes: 1\n##    Total graph size: 9\n## \n## Initializing model\n# Burn in to stabilize the chains\nupdate( jagsModel2 , n.iter= burnInSteps )\n# Run chains to estimate posterior distribution\ncodaSamples2 = coda.samples( jagsModel2 , variable.names=c(\"N\") ,\n                            n.iter=2500 )\nThe posterior distribution resulting from this model can now be analyzed.\npar( mar=c(3.5,0.5,2.5,0.5) , mgp=c(2.25,0.7,0) )\nplotPost( codaSamples2[,\"N\"] , main=\"N\" , xlab=\"N\" )\n\n##        ESS     mean   median     mode hdiMass   hdiLow  hdiHigh\n## N 2796.591 495.5214 480.9062 437.8477    0.95 418.0296 618.4802\nGiven the improved intelligence we have a the end of the war, we see that we can arrive at a much narrower posterior and hence a more confident estimate."
  },
  {
    "objectID": "blog/2023-10-15-the-german-tank-problem/index.html#conclusion",
    "href": "blog/2023-10-15-the-german-tank-problem/index.html#conclusion",
    "title": "The German Tank Problem",
    "section": "Conclusion",
    "text": "Conclusion\nWe have used frequentist and statistical inference to solve the German Tank problem. In this particular problem, the benefits of the Bayesian approach are evident. Besides being more intuitive, it also obviates the need for concepts like rejection regions and p-values.\nThe code underlying this blog post is available here."
  },
  {
    "objectID": "blog/2023-10-15-the-german-tank-problem/index.html#references-and-further-reading",
    "href": "blog/2023-10-15-the-german-tank-problem/index.html#references-and-further-reading",
    "title": "The German Tank Problem",
    "section": "References and Further Reading",
    "text": "References and Further Reading\n\nDoing Bayesian Data Analysis : A Tutorial with R, JAGS, and Stan \nBernoulli’s Fallacy: Statistical Illogic and the Crisis of Modern Science \nhttps://www.isaacslavitt.com/posts/german-tank-problem-with-pymc-and-pystan/ \nhttps://www.mjandrews.org/blog/germantank/"
  },
  {
    "objectID": "blog/2023-10-15-the-german-tank-problem/index.html#footnotes",
    "href": "blog/2023-10-15-the-german-tank-problem/index.html#footnotes",
    "title": "The German Tank Problem",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nhttps://en.wikipedia.org/wiki/German_tank_problem↩︎\nhttps://www.youtube.com/watch?v=quV-MCB8Ozs↩︎\nhttps://en.wikipedia.org/wiki/Variance#Population_variance_and_sample_variance↩︎\nhttp://jakevdp.github.io/blog/2014/06/12/frequentism-and-bayesianism-3-confidence-credibility/↩︎\nhttps://www.govindgnair.com/post/introduction-to-bayesian-methods/↩︎\nhttps://www.mjandrews.org/blog/germantank/↩︎"
  },
  {
    "objectID": "blog/2021-06-26-the-profound-implications-of-gene-editing/index.html",
    "href": "blog/2021-06-26-the-profound-implications-of-gene-editing/index.html",
    "title": "The Profound Implications of Gene Editing",
    "section": "",
    "text": "“Genetic power is the most awesome force the planet’s ever seen, but you wield it like a kid that’s found his dad’s gun” – Ian Malcolm (played by Jeff Goldblum) in the Jurassic Park\n\nIn Steven Spielberg’s 1993 blockbuster Jurassic Park, scientists resurrect dinosaurs by splicing fossilized dinosaur DNA into frog DNA. It was perhaps the first-time genetic engineering entered the popular consciousness.\nIn 2012, research teams lead by Nobel prize winning scientists Jennifer Doudna and Emmanuelle Charpentier discovered CRISPR (Clustered Regularly Interspaced Short Palindromic Repeat), an ancient defense mechanism used by bacteria to detect and destroy invading viruses. Along with Feng Zhang, they proposed using the Cas9 protein as a programmable tool to edit DNA in human beings.\nIn 2018, Chinese scientist He Jiankui used CRISPR to make germline edits to the two embryos to make them immune to the HIV virus. Germline edits unlike somatic cell edits are heritable and passed on to future generations.\nThe genie has been let out the bottle. I don’t think most people realize the full import of the developments above. For the first time in the history of life, a species has found a way to hack the code of life. Genetic destiny would no longer dictated by random chance, human beings have discovered the tools to engineer our own genetic destiny.\nWhat are the implications of this technology?\nOn one side are incontrovertible benefits, permanent cures to genetic diseases like sickle cell anemia, cystic fibrosis and Huntington disease.\nOn the other end of the spectrum – designer babies. A future where you can first eliminate the likelihood of genetic disorders and then endow your children with desirable qualities like higher intelligence or greater physical strength.\nThis could have massive ramifications on society and on our species.\nAll parents want the best for their children, so there is a valid argument to let parents choose the gene edits they want to make to their children. What would this lead to in a country like India with an obsession for fair skin? Would India become a country of fair skinned people? This is most certainly not a desirable or beneficial development.\nAll parents want their children to be smarter. Would this lead to rich people who can afford expensive gene editing bearing smarter offspring? One could argue this exacerbates social inequality, but if one were to take a utilitarian perspective, wouldn’t humanity be better off if there were more Einsteins and Mozarts?\nJust like any new technology, gene editing will be accessible only to the economic elite initially, over time as the costs decline it will become accessible to everyone. Are we willing to live with the interim social consequences of this?\nMany ethicists would consider these consequences as anathema and call for strict regulation on such gene editing research. Perhaps a strict line should be drawn preventing research on non-essential, non-disease treating genetic research – but is a global moratorium on such research possible?\nIf a country chooses to break ranks and genetically enhance their population, there is a clear path to genetic supremacy for one nation or ethnicity. The Nazis thoughts Aryan supremacy was genetic destiny, there is now the technology to make national or ethnic supremacy a reality.\nThese questions boggle my mind. How we as a society addresses these questions will determine the future of our species. It is high time we start thinking deeply about the most consequential issue the human race has ever faced."
  },
  {
    "objectID": "blog/2022-02-19-oracle-mlrg-talks/index.html",
    "href": "blog/2022-02-19-oracle-mlrg-talks/index.html",
    "title": "Oracle MLRG Talks",
    "section": "",
    "text": "Oracle invite’s ML researchers to give biweekly talks. These are my notes from these sessions."
  },
  {
    "objectID": "blog/2022-02-19-oracle-mlrg-talks/index.html#standards",
    "href": "blog/2022-02-19-oracle-mlrg-talks/index.html#standards",
    "title": "Oracle MLRG Talks",
    "section": "Standards",
    "text": "Standards\n\nOpen Provenance Model\nW3C Prov Model\n\nStadardization has not been beneficial.\nAbstraction is such a high level that it is not possible to build a semantically meaningful application on this prov model.Everybody imposes their own semantic layer on top of this prov model.\n\nOpen Lineage\nSystem provenance standards were also created e.g. CamFlow"
  },
  {
    "objectID": "blog/2022-02-19-oracle-mlrg-talks/index.html#challenges",
    "href": "blog/2022-02-19-oracle-mlrg-talks/index.html#challenges",
    "title": "Oracle MLRG Talks",
    "section": "Challenges",
    "text": "Challenges\n\nPeople don’t care about provenance\nIt is like security, people want it but won’t pay for it (in $ or performance/computation).\nGeneral provenance doesn’t help. People want provenance specific to their problem.\nProvenance is like insurance: you never really want it until it is too late.\nTakeaway: Focus on provenance applications. E.g. File system search\nWorkflow provenance: ML Model Sharing & Reproducibility.\nProvenance graphs are DAG.\nProvenance could be at OS level, database level or application level\nTribuo(Java Deep Learning library) has provenance built in."
  },
  {
    "objectID": "blog/2022-02-19-oracle-mlrg-talks/index.html#lineage-applications",
    "href": "blog/2022-02-19-oracle-mlrg-talks/index.html#lineage-applications",
    "title": "Oracle MLRG Talks",
    "section": "Lineage Applications",
    "text": "Lineage Applications\n\nUnicorn for Intrusion Detection\nLink: https://arxiv.org/pdf/2001.01525.pdf\n\nUsing provenance graphs for intrusion detection\n\n\n\nTake snapshots of provenance(streaming graph)\nCompute a unique label for a node by computing neighborhood of node(3 hop neighborhood). Create a hash value for neighborhood and plot histogram of hash values.\nCreate fixed size sketched for histograms (similarity preserving)\n\nif you measure Jaccard distance between two histograms, it should correspond to hamming distance between sketches.\n\nModels are built from sketches. Cluster sketches and keep track of transitions between clusters.\nCreate for every 1000 provenance edges.(some period of time)\nIf sketch does not fall in a cluster, it is a red flag.\n\n\n\n\nSIGL\nLink: https://arxiv.org/abs/2008.11533\n\nUses Graph LSTM for encoder. MLP for decoder."
  },
  {
    "objectID": "blog/2022-02-19-oracle-mlrg-talks/index.html#virtual-screening",
    "href": "blog/2022-02-19-oracle-mlrg-talks/index.html#virtual-screening",
    "title": "Oracle MLRG Talks",
    "section": "Virtual Screening",
    "text": "Virtual Screening\n\nCurrent Approach\nTake a compound → Get Morgan fingerprint (traditional , hand engineered molecular features) → pass through feed forward NN → Get property predictions\nMorgan Fingerprint\nEnumerate all possible substructures and group them using a hash function.\n\nEffectively a bag of sub-structures.\n\n\nNew Approach\n\nTake compound → Learn an embedding using a Graph Neural Networks → Pass through Feed Forward NN → Get prediction\nFor graph:\n\nNode feature: one hot encoding indicating type of atom\nEdge feature: single bond, double bond etc.\n\nApplication:\n\nUsed this approach to screen 2335 compounds and measure their growth inhibition against E-coli(bacteria)\nDiscovered new antibiotic.\nAchieved AUC = 0.9\n\nApproaches like domain adaptation can improve results."
  },
  {
    "objectID": "blog/2022-02-19-oracle-mlrg-talks/index.html#molecular-graph-generation",
    "href": "blog/2022-02-19-oracle-mlrg-talks/index.html#molecular-graph-generation",
    "title": "Oracle MLRG Talks",
    "section": "Molecular Graph Generation",
    "text": "Molecular Graph Generation\nGraph convolution is not invertible (unlike image convolutions)so generating graphs from embeddings does not work.\n\nApproach 1\nUse NLP like approaches to generate a molecule piece by piece.(e.g. atom by atom)\n\nEquivalent to character by character in NLP\nSimple to implement but higher computational complexity.(Need to re-encode graph after adding every new atom)\nHave multiple frontier nodes unlike Language E.g. 1 and 2 below\n\n\n\n\nApproach 2\n\nUse molecular substructures rather than atoms.\nEquivalent to word by word in NLP\nPerformance is better but needs a vocabulary of sub-structures.\n\nJunction tree algorithm can be used to reduce a graph into a tree. tree representations are easier to generate as they do not have cycles\n\nUse a NN to predict next substructure to be added.\n\n\n\nAlso need to know how to attach the two sub structures together.\n\nPredict the attaching point in new sub-structure and current partial graph and then merge them.\n\n\n\n\nHierarchical Graph Convolution\n\nRun graph convolution in the pooled-substructure level graph\nPropagate atom level embedding upwards"
  },
  {
    "objectID": "blog/2022-02-19-oracle-mlrg-talks/index.html#antibody-structure-generation",
    "href": "blog/2022-02-19-oracle-mlrg-talks/index.html#antibody-structure-generation",
    "title": "Oracle MLRG Talks",
    "section": "Antibody Structure Generation",
    "text": "Antibody Structure Generation\n\nMore complex 3D molecules.\nuse a 3D generative model to output a 3D point cloud.\nApplication: HIV antibody design\nAntibodies bind to a virus and trigger body’s immune response.\nBinding specificity/strength is largely determined by complementary determining region.\nBasic unit of antibody is a residue, whose value can be one of the 20 amino acids.\n\nEach residue is a substrucutre\nEach atom has a 3D coordinate.\n\nPerformance depends on the 3D structure of the amino acids\nCDR graphs are represented as a K-nearest-neighbor graph\n\n\n\nAuto regressive models are inadequate as a residue once added will not change . However if a new residue is generated and inserted to the CDR, distance between previous residues should naturally change.\nIterative graph refinement is used to update entire graph structure once a new reside is added."
  },
  {
    "objectID": "blog/2024-10-19-importance-of-vision-or-is-it-mission/index.html",
    "href": "blog/2024-10-19-importance-of-vision-or-is-it-mission/index.html",
    "title": "Importance of Vision - Or is it Mission ?",
    "section": "",
    "text": "When I stumbled into my first product role, like many others in the same boat, I turned to the product management classics—Marty Cagan’s books, Lenny Rachitsky’s posts, Lean Product Playbook, and the rest. One thing I’ve always found confusing in this literature is the difference between “vision” and “mission.” It feels like people go to great lengths to create a distinction that’s not always necessary.\nEven a thinker like Shreyas comes up with definitions where I still struggle to understand the clear difference between the two.\n\nFrom my perspective, if you can clearly articulate the future you’re trying to create, you don’t really need to explain why it’s worthwhile. Your mission is to create that future. To put it simply: what matters most is having an ambitious, long-term goal that can challenge and inspire people. And like any good goal, you should be able to measure whether you’re making progress.\nWhether you call this your “mission” or your “vision” doesn’t seem all that important.\nHere are a few examples of great mission/vision statements:\n\nMicrosoft’s original mission to put a computer on every desk and in every home is probably the best example. It was bold and ambitious, and incredibly, they achieved it within 20 to 30 years.\nSpaceX’s mission is “to make life multiplanetary.” Wow. When that’s your mission, it’s no wonder they’ve accomplished so much in the last decade.\nMeta’s mission is to “give people the power to build community and bring the world closer together.” You know you can only fulfill this mission if billions of people are using your platform to make meaningful connections—a goal that’s both ambitious and measurable.\n\nIn contrast, I find that many B2B companies have less compelling mission statements.\n\nSalesforce: “We bring companies and customers together.”\nOracle: “We help people see data in new ways, discover insights, and unlock endless possibilities.”\n\nIf you review the mission statements of companies like IBM and Honeywell, you might feel a little underwhelmed.\nWhy is that?  I think the challenge for most B2B companies is that after their initial product success, they grow through acquisitions and eventually become a collection of smaller businesses. Their range of products and services becomes so broad that it’s hard to have one concise, coherent mission.\nSo how do you combat this?  I believe each business unit needs its own mission statement, one that aligns with the company’s overall mission but is specific to what that team is trying to achieve. This is what I’ve aimed to create within the small business unit I’m part of at Oracle.\nFinally, nothing illustrates the importance of mission better than the famous anecdote about John F. Kennedy and NASA. During a visit in the 1960s, President Kennedy asked a janitor at the space center what he was doing. The janitor replied, “Mr. President, I’m helping put a man on the moon.”\nThat’s the power of a mission. Failing to tap into that is a missed opportunity for any company."
  }
]